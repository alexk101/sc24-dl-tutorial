+ source export_DDP_vars.sh
++ export RANK=0
++ RANK=0
++ export LOCAL_RANK=0
++ LOCAL_RANK=0
++ export WORLD_SIZE=16
++ WORLD_SIZE=16
++ export MASTER_PORT=29500
++ MASTER_PORT=29500
+++ hostname -i
++ export MASTER_ADDR=10.128.1.56
++ MASTER_ADDR=10.128.1.56
+ source export_frontier_vars.sh
++ export NCCL_SOCKET_IFNAME=hsn0
++ NCCL_SOCKET_IFNAME=hsn0
++ export NCCL_SOCKET_FAMILY=ipv4
++ NCCL_SOCKET_FAMILY=ipv4
++ export MPICH_GPU_SUPPORT_ENABLED=1
++ MPICH_GPU_SUPPORT_ENABLED=1
++ export NCCL_CROSS_NIC=1
++ NCCL_CROSS_NIC=1
+ export MASTER_PORT=3442
+ MASTER_PORT=3442
+ srun /ccs/home/kiefera/.conda/envs/pytorch/bin/python train_mp_mod.py --config=mp --tensor_parallel=8 --scale_depth=12 --scale_heads=8 --scale_dim=384 --n_train=25 --local_batch_size=16 --num_data_workers=1
2025-02-28 08:58:46,870 - root - INFO - ROCM GPU support available
2025-02-28 08:58:47,002 - root - INFO - bfloat16 support: True
[W228 08:58:47.398955482 socket.cpp:759] [c10d] The client socket cannot be initialized to connect to [frontier00073.frontier.olcf.ornl.gov]:3442 (errno: 97 - Address family not supported by protocol).
[W228 08:58:47.399510243 socket.cpp:759] [c10d] The client socket cannot be initialized to connect to [frontier00073.frontier.olcf.ornl.gov]:3442 (errno: 97 - Address family not supported by protocol).
[W228 08:58:47.400835522 socket.cpp:759] [c10d] The client socket cannot be initialized to connect to [frontier00073.frontier.olcf.ornl.gov]:3442 (errno: 97 - Address family not supported by protocol).
[W228 08:58:47.401864458 socket.cpp:759] [c10d] The client socket cannot be initialized to connect to [frontier00073.frontier.olcf.ornl.gov]:3442 (errno: 97 - Address family not supported by protocol).
[W228 08:58:47.402615088 socket.cpp:759] [c10d] The client socket cannot be initialized to connect to [frontier00073.frontier.olcf.ornl.gov]:3442 (errno: 97 - Address family not supported by protocol).
[W228 08:58:47.403797771 socket.cpp:759] [c10d] The client socket cannot be initialized to connect to [frontier00073.frontier.olcf.ornl.gov]:3442 (errno: 97 - Address family not supported by protocol).
[W228 08:58:47.403776420 socket.cpp:759] [c10d] The client socket cannot be initialized to connect to [frontier00073.frontier.olcf.ornl.gov]:3442 (errno: 97 - Address family not supported by protocol).
[W228 08:58:47.405027836 socket.cpp:759] [c10d] The client socket cannot be initialized to connect to [frontier00073.frontier.olcf.ornl.gov]:3442 (errno: 97 - Address family not supported by protocol).
[W228 08:58:47.170351027 socket.cpp:759] [c10d] The client socket cannot be initialized to connect to [frontier00073.frontier.olcf.ornl.gov]:3442 (errno: 97 - Address family not supported by protocol).
[W228 08:58:47.170340677 socket.cpp:759] [c10d] The client socket cannot be initialized to connect to [frontier00073.frontier.olcf.ornl.gov]:3442 (errno: 97 - Address family not supported by protocol).
[W228 08:58:47.170410001 socket.cpp:759] [c10d] The client socket cannot be initialized to connect to [frontier00073.frontier.olcf.ornl.gov]:3442 (errno: 97 - Address family not supported by protocol).
[W228 08:58:47.170399961 socket.cpp:759] [c10d] The client socket cannot be initialized to connect to [frontier00073.frontier.olcf.ornl.gov]:3442 (errno: 97 - Address family not supported by protocol).
[W228 08:58:47.170388579 socket.cpp:759] [c10d] The client socket cannot be initialized to connect to [frontier00073.frontier.olcf.ornl.gov]:3442 (errno: 97 - Address family not supported by protocol).
[W228 08:58:47.170396755 socket.cpp:759] [c10d] The client socket cannot be initialized to connect to [frontier00073.frontier.olcf.ornl.gov]:3442 (errno: 97 - Address family not supported by protocol).
[W228 08:58:47.170462823 socket.cpp:759] [c10d] The client socket cannot be initialized to connect to [frontier00073.frontier.olcf.ornl.gov]:3442 (errno: 97 - Address family not supported by protocol).
[W228 08:58:47.171297648 socket.cpp:759] [c10d] The client socket cannot be initialized to connect to [frontier00073.frontier.olcf.ornl.gov]:3442 (errno: 97 - Address family not supported by protocol).
2025-02-28 08:58:47,793 - root - INFO - Setting DP = 2, TP = 8, CP = 1, PP = 1
2025-02-28 08:58:47,885 - root - INFO - ------------------ Configuration ------------------
2025-02-28 08:58:47,886 - root - INFO - Configuration file: /autofs/nccs-svm1_home1/kiefera/sc24-dl-tutorial/config/ViT.yaml
2025-02-28 08:58:47,886 - root - INFO - Configuration name: mp
2025-02-28 08:58:47,886 - root - INFO - num_iters 500000
2025-02-28 08:58:47,886 - root - INFO - global_batch_size 32
2025-02-28 08:58:47,886 - root - INFO - lr 0.001
2025-02-28 08:58:47,886 - root - INFO - num_data_workers 1
2025-02-28 08:58:47,886 - root - INFO - embed_dim 1024
2025-02-28 08:58:47,886 - root - INFO - data_loader_config pytorch
2025-02-28 08:58:47,886 - root - INFO - amp_mode none
2025-02-28 08:58:47,886 - root - INFO - enable_jit False
2025-02-28 08:58:47,886 - root - INFO - enable_fused False
2025-02-28 08:58:47,886 - root - INFO - depth 12
2025-02-28 08:58:47,886 - root - INFO - dropout 0.0
2025-02-28 08:58:47,887 - root - INFO - patch_size 8
2025-02-28 08:58:47,887 - root - INFO - num_heads 8
2025-02-28 08:58:47,887 - root - INFO - img_size [360, 720]
2025-02-28 08:58:47,887 - root - INFO - dt 1
2025-02-28 08:58:47,887 - root - INFO - expdir /logs
2025-02-28 08:58:47,887 - root - INFO - lr_schedule cosine
2025-02-28 08:58:47,887 - root - INFO - warmup 0
2025-02-28 08:58:47,887 - root - INFO - optimizer Adam
2025-02-28 08:58:47,887 - root - INFO - logging_freq 100
2025-02-28 08:58:47,887 - root - INFO - n_in_channels 20
2025-02-28 08:58:47,887 - root - INFO - n_out_channels 20
2025-02-28 08:58:47,887 - root - INFO - train_data_path /data/train
2025-02-28 08:58:47,887 - root - INFO - valid_data_path /data/valid
2025-02-28 08:58:47,887 - root - INFO - inf_data_path /data/test
2025-02-28 08:58:47,887 - root - INFO - time_means_path /data/stats/time_means.npy
2025-02-28 08:58:47,887 - root - INFO - global_means_path /data/stats/global_means.npy
2025-02-28 08:58:47,887 - root - INFO - global_stds_path /data/stats/global_stds.npy
2025-02-28 08:58:47,887 - root - INFO - limit_nsamples None
2025-02-28 08:58:47,887 - root - INFO - limit_nsamples_val None
2025-02-28 08:58:47,887 - root - INFO - wireup_info env
2025-02-28 08:58:47,887 - root - INFO - wireup_store tcp
2025-02-28 08:58:47,887 - root - INFO - amp_enabled False
2025-02-28 08:58:47,887 - root - INFO - amp_dtype torch.float32
2025-02-28 08:58:47,887 - root - INFO - tp 8
2025-02-28 08:58:47,887 - root - INFO - cp 1
2025-02-28 08:58:47,887 - root - INFO - order tp-cp-dp
2025-02-28 08:58:47,887 - root - INFO - ---------------------------------------------------
2025-02-28 08:58:47,892 - root - INFO - Using AMP dtype: torch.float32
2025-02-28 08:58:47,893 - root - INFO - rank 0, begin data loader init
2025-02-28 08:58:47,956 - root - INFO - Found 25 files in /lustre/orion/geo163/scratch/kiefera/temp_train/25
2025-02-28 08:58:47,957 - root - INFO - Getting file stats from /lustre/orion/geo163/scratch/kiefera/temp_train/25/1990.h5
2025-02-28 08:58:47,957 - root - INFO - Number of samples per year: 1460
2025-02-28 08:58:47,957 - root - INFO - Found data at path /lustre/orion/geo163/scratch/kiefera/temp_train/25. Number of examples: 36500. Image Shape: 360 x 720 x 20
2025-02-28 08:58:47,959 - root - INFO - Found 2 files in /lustre/orion/geo163/scratch/kiefera/temp_val/25
2025-02-28 08:58:47,959 - root - INFO - Getting file stats from /lustre/orion/geo163/scratch/kiefera/temp_val/25/2016.h5
2025-02-28 08:58:47,960 - root - INFO - Number of samples per year: 1460
2025-02-28 08:58:47,960 - root - INFO - Found data at path /lustre/orion/geo163/scratch/kiefera/temp_val/25. Number of examples: 2920. Image Shape: 360 x 720 x 20
2025-02-28 08:58:47,960 - root - INFO - rank 0, data loader initialized
2025-02-28 08:58:47,960 - root - INFO - Rank 0: Using AMD GPU 0, Total GPU memory: 63.98 GB
2025-02-28 08:58:51,118 - root - INFO - Sanity check: Sum of ranks across nodes: 120 (Expected: 120)
2025-02-28 08:58:51,176 - root - INFO - Init shared weights
2025-02-28 08:58:52,946 - root - INFO - Init DDP
2025-02-28 08:59:01,128 - root - INFO - DistributedDataParallel(
  (module): VisionTransformer(
    (patch_embed): PatchEmbed(
      (proj): Conv2d(20, 384, kernel_size=(8, 8), stride=(8, 8))
    )
    (pos_drop): Dropout(p=0.0, inplace=False)
    (blocks): ModuleList(
      (0-11): 12 x Block(
        (drop_path): Identity()
        (attn): DistributedAttention(
          (q): DistributedMatmul()
          (k): DistributedMatmul()
          (v): DistributedMatmul()
          (attn_drop): Dropout(p=0.0, inplace=False)
          (proj): DistributedMatmul()
          (proj_drop): Dropout(p=0.0, inplace=False)
        )
        (mlp): DistributedMLP(
          (fc1): DistributedMatmul()
          (fc2): DistributedMatmul()
          (act): GELU(approximate='none')
          (drop): Dropout(p=0.0, inplace=False)
        )
        (norm1): DistributedLayerNorm(
          (norm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
        )
        (norm2): DistributedLayerNorm(
          (norm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
        )
      )
    )
    (norm): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
    (head): Linear(in_features=384, out_features=1280, bias=False)
  )
)
2025-02-28 08:59:01,129 - root - INFO - Scaffolding memory high watermark: 2.22 GB.
2025-02-28 08:59:01,131 - root - INFO - Starting Training Loop...
[rank13]:[E228 09:08:53.551535723 ProcessGroupNCCL.cpp:629] [Rank 1] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=2, OpType=BROADCAST, NumelIn=840, NumelOut=840, Timeout(ms)=600000) ran for 600025 milliseconds before timing out.
[rank13]:[E228 09:08:53.553578476 ProcessGroupNCCL.cpp:2168] [PG ID 1 PG GUID 6 Rank 1]  failure detected by watchdog at work sequence id: 2 PG status: last enqueued work: 2, last completed work: 1
[rank13]:[E228 09:08:53.553588265 ProcessGroupNCCL.cpp:667] Stack trace of the failed collective not found, potentially because FlightRecorder is disabled. You can enable it by setting TORCH_NCCL_TRACE_BUFFER_SIZE to a non-zero value.
[rank13]:[E228 09:08:53.553593014 ProcessGroupNCCL.cpp:681] [Rank 1] Some NCCL operations have failed or timed out. Due to the asynchronous nature of CUDA kernels, subsequent GPU operations might run on corrupted/incomplete data.
[rank13]:[E228 09:08:53.553595770 ProcessGroupNCCL.cpp:695] [Rank 1] To avoid data inconsistency, we are taking the entire process down.
[rank13]:[E228 09:08:53.555488041 ProcessGroupNCCL.cpp:1895] [PG ID 1 PG GUID 6 Rank 1] Process group watchdog thread terminated with exception: [Rank 1] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=2, OpType=BROADCAST, NumelIn=840, NumelOut=840, Timeout(ms)=600000) ran for 600025 milliseconds before timing out.
Exception raised from checkTimeout at /pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:632 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >) + 0x98 (0x7fff2dbba968 in /ccs/home/kiefera/.conda/envs/pytorch/lib/python3.11/site-packages/torch/lib/libc10.so)
frame #1: c10d::ProcessGroupNCCL::WorkNCCL::checkTimeout(std::optional<std::chrono::duration<long, std::ratio<1l, 1000l> > >) + 0x23d (0x7fff873fb2dd in /ccs/home/kiefera/.conda/envs/pytorch/lib/python3.11/site-packages/torch/lib/libtorch_hip.so)
frame #2: c10d::ProcessGroupNCCL::watchdogHandler() + 0x9e8 (0x7fff873fc8d8 in /ccs/home/kiefera/.conda/envs/pytorch/lib/python3.11/site-packages/torch/lib/libtorch_hip.so)
frame #3: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0x14d (0x7fff873fd57d in /ccs/home/kiefera/.conda/envs/pytorch/lib/python3.11/site-packages/torch/lib/libtorch_hip.so)
frame #4: <unknown function> + 0xe4793 (0x7fffeb42d793 in /usr/lib64/libstdc++.so.6)
frame #5: <unknown function> + 0xa761c (0x7fffed74f61c in /lib64/libc.so.6)
frame #6: <unknown function> + 0x12eaa8 (0x7fffed7d6aa8 in /lib64/libc.so.6)

terminate called after throwing an instance of 'c10::DistBackendError'
  what():  [PG ID 1 PG GUID 6 Rank 1] Process group watchdog thread terminated with exception: [Rank 1] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=2, OpType=BROADCAST, NumelIn=840, NumelOut=840, Timeout(ms)=600000) ran for 600025 milliseconds before timing out.
Exception raised from checkTimeout at /pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:632 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >) + 0x98 (0x7fff2dbba968 in /ccs/home/kiefera/.conda/envs/pytorch/lib/python3.11/site-packages/torch/lib/libc10.so)
frame #1: c10d::ProcessGroupNCCL::WorkNCCL::checkTimeout(std::optional<std::chrono::duration<long, std::ratio<1l, 1000l> > >) + 0x23d (0x7fff873fb2dd in /ccs/home/kiefera/.conda/envs/pytorch/lib/python3.11/site-packages/torch/lib/libtorch_hip.so)
frame #2: c10d::ProcessGroupNCCL::watchdogHandler() + 0x9e8 (0x7fff873fc8d8 in /ccs/home/kiefera/.conda/envs/pytorch/lib/python3.11/site-packages/torch/lib/libtorch_hip.so)
frame #3: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0x14d (0x7fff873fd57d in /ccs/home/kiefera/.conda/envs/pytorch/lib/python3.11/site-packages/torch/lib/libtorch_hip.so)
frame #4: <unknown function> + 0xe4793 (0x7fffeb42d793 in /usr/lib64/libstdc++.so.6)
frame #5: <unknown function> + 0xa761c (0x7fffed74f61c in /lib64/libc.so.6)
frame #6: <unknown function> + 0x12eaa8 (0x7fffed7d6aa8 in /lib64/libc.so.6)

Exception raised from ncclCommWatchdog at /pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1901 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >) + 0x98 (0x7fff2dbba968 in /ccs/home/kiefera/.conda/envs/pytorch/lib/python3.11/site-packages/torch/lib/libc10.so)
frame #1: <unknown function> + 0x22990fe (0x7fff873d10fe in /ccs/home/kiefera/.conda/envs/pytorch/lib/python3.11/site-packages/torch/lib/libtorch_hip.so)
frame #2: <unknown function> + 0x69cfc1 (0x7fff857d4fc1 in /ccs/home/kiefera/.conda/envs/pytorch/lib/python3.11/site-packages/torch/lib/libtorch_hip.so)
frame #3: <unknown function> + 0xe4793 (0x7fffeb42d793 in /usr/lib64/libstdc++.so.6)
frame #4: <unknown function> + 0xa761c (0x7fffed74f61c in /lib64/libc.so.6)
frame #5: <unknown function> + 0x12eaa8 (0x7fffed7d6aa8 in /lib64/libc.so.6)

[rank6]:[E228 09:08:53.497613475 ProcessGroupNCCL.cpp:629] [Rank 0] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=3, OpType=BROADCAST, NumelIn=5225280, NumelOut=5225280, Timeout(ms)=600000) ran for 600024 milliseconds before timing out.
[rank6]:[E228 09:08:53.499276205 ProcessGroupNCCL.cpp:2168] [PG ID 1 PG GUID 7 Rank 0]  failure detected by watchdog at work sequence id: 3 PG status: last enqueued work: 3, last completed work: 2
[rank6]:[E228 09:08:53.499284171 ProcessGroupNCCL.cpp:667] Stack trace of the failed collective not found, potentially because FlightRecorder is disabled. You can enable it by setting TORCH_NCCL_TRACE_BUFFER_SIZE to a non-zero value.
[rank6]:[E228 09:08:53.499288569 ProcessGroupNCCL.cpp:681] [Rank 0] Some NCCL operations have failed or timed out. Due to the asynchronous nature of CUDA kernels, subsequent GPU operations might run on corrupted/incomplete data.
[rank6]:[E228 09:08:53.499291495 ProcessGroupNCCL.cpp:695] [Rank 0] To avoid data inconsistency, we are taking the entire process down.
[rank6]:[E228 09:08:53.500978021 ProcessGroupNCCL.cpp:1895] [PG ID 1 PG GUID 7 Rank 0] Process group watchdog thread terminated with exception: [Rank 0] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=3, OpType=BROADCAST, NumelIn=5225280, NumelOut=5225280, Timeout(ms)=600000) ran for 600024 milliseconds before timing out.
Exception raised from checkTimeout at /pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:632 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >) + 0x98 (0x7fff2dbba968 in /ccs/home/kiefera/.conda/envs/pytorch/lib/python3.11/site-packages/torch/lib/libc10.so)
frame #1: c10d::ProcessGroupNCCL::WorkNCCL::checkTimeout(std::optional<std::chrono::duration<long, std::ratio<1l, 1000l> > >) + 0x23d (0x7fff873fb2dd in /ccs/home/kiefera/.conda/envs/pytorch/lib/python3.11/site-packages/torch/lib/libtorch_hip.so)
frame #2: c10d::ProcessGroupNCCL::watchdogHandler() + 0x9e8 (0x7fff873fc8d8 in /ccs/home/kiefera/.conda/envs/pytorch/lib/python3.11/site-packages/torch/lib/libtorch_hip.so)
frame #3: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0x14d (0x7fff873fd57d in /ccs/home/kiefera/.conda/envs/pytorch/lib/python3.11/site-packages/torch/lib/libtorch_hip.so)
frame #4: <unknown function> + 0xe4793 (0x7fffeb42d793 in /usr/lib64/libstdc++.so.6)
frame #5: <unknown function> + 0xa761c (0x7fffed74f61c in /lib64/libc.so.6)
frame #6: <unknown function> + 0x12eaa8 (0x7fffed7d6aa8 in /lib64/libc.so.6)

terminate called after throwing an instance of 'c10::DistBackendError'
  what():  [PG ID 1 PG GUID 7 Rank 0] Process group watchdog thread terminated with exception: [Rank 0] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=3, OpType=BROADCAST, NumelIn=5225280, NumelOut=5225280, Timeout(ms)=600000) ran for 600024 milliseconds before timing out.
Exception raised from checkTimeout at /pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:632 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >) + 0x98 (0x7fff2dbba968 in /ccs/home/kiefera/.conda/envs/pytorch/lib/python3.11/site-packages/torch/lib/libc10.so)
frame #1: c10d::ProcessGroupNCCL::WorkNCCL::checkTimeout(std::optional<std::chrono::duration<long, std::ratio<1l, 1000l> > >) + 0x23d (0x7fff873fb2dd in /ccs/home/kiefera/.conda/envs/pytorch/lib/python3.11/site-packages/torch/lib/libtorch_hip.so)
frame #2: c10d::ProcessGroupNCCL::watchdogHandler() + 0x9e8 (0x7fff873fc8d8 in /ccs/home/kiefera/.conda/envs/pytorch/lib/python3.11/site-packages/torch/lib/libtorch_hip.so)
frame #3: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0x14d (0x7fff873fd57d in /ccs/home/kiefera/.conda/envs/pytorch/lib/python3.11/site-packages/torch/lib/libtorch_hip.so)
frame #4: <unknown function> + 0xe4793 (0x7fffeb42d793 in /usr/lib64/libstdc++.so.6)
frame #5: <unknown function> + 0xa761c (0x7fffed74f61c in /lib64/libc.so.6)
frame #6: <unknown function> + 0x12eaa8 (0x7fffed7d6aa8 in /lib64/libc.so.6)

Exception raised from ncclCommWatchdog at /pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1901 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >) + 0x98 (0x7fff2dbba968 in /ccs/home/kiefera/.conda/envs/pytorch/lib/python3.11/site-packages/torch/lib/libc10.so)
frame #1: <unknown function> + 0x22990fe (0x7fff873d10fe in /ccs/home/kiefera/.conda/envs/pytorch/lib/python3.11/site-packages/torch/lib/libtorch_hip.so)
frame #2: <unknown function> + 0x69cfc1 (0x7fff857d4fc1 in /ccs/home/kiefera/.conda/envs/pytorch/lib/python3.11/site-packages/torch/lib/libtorch_hip.so)
frame #3: <unknown function> + 0xe4793 (0x7fffeb42d793 in /usr/lib64/libstdc++.so.6)
frame #4: <unknown function> + 0xa761c (0x7fffed74f61c in /lib64/libc.so.6)
frame #5: <unknown function> + 0x12eaa8 (0x7fffed7d6aa8 in /lib64/libc.so.6)

[rank0]:[E228 09:08:53.519674089 ProcessGroupNCCL.cpp:629] [Rank 0] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=3, OpType=BROADCAST, NumelIn=5225280, NumelOut=5225280, Timeout(ms)=600000) ran for 600005 milliseconds before timing out.
[rank0]:[E228 09:08:53.519756969 ProcessGroupNCCL.cpp:2168] [PG ID 1 PG GUID 1 Rank 0]  failure detected by watchdog at work sequence id: 3 PG status: last enqueued work: 3, last completed work: 2
[rank0]:[E228 09:08:53.519763912 ProcessGroupNCCL.cpp:667] Stack trace of the failed collective not found, potentially because FlightRecorder is disabled. You can enable it by setting TORCH_NCCL_TRACE_BUFFER_SIZE to a non-zero value.
[rank0]:[E228 09:08:53.519767649 ProcessGroupNCCL.cpp:681] [Rank 0] Some NCCL operations have failed or timed out. Due to the asynchronous nature of CUDA kernels, subsequent GPU operations might run on corrupted/incomplete data.
[rank0]:[E228 09:08:53.519770455 ProcessGroupNCCL.cpp:695] [Rank 0] To avoid data inconsistency, we are taking the entire process down.
[rank0]:[E228 09:08:53.520620025 ProcessGroupNCCL.cpp:1895] [PG ID 1 PG GUID 1 Rank 0] Process group watchdog thread terminated with exception: [Rank 0] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=3, OpType=BROADCAST, NumelIn=5225280, NumelOut=5225280, Timeout(ms)=600000) ran for 600005 milliseconds before timing out.
Exception raised from checkTimeout at /pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:632 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >) + 0x98 (0x7fff2dbba968 in /ccs/home/kiefera/.conda/envs/pytorch/lib/python3.11/site-packages/torch/lib/libc10.so)
frame #1: c10d::ProcessGroupNCCL::WorkNCCL::checkTimeout(std::optional<std::chrono::duration<long, std::ratio<1l, 1000l> > >) + 0x23d (0x7fff873fb2dd in /ccs/home/kiefera/.conda/envs/pytorch/lib/python3.11/site-packages/torch/lib/libtorch_hip.so)
frame #2: c10d::ProcessGroupNCCL::watchdogHandler() + 0x9e8 (0x7fff873fc8d8 in /ccs/home/kiefera/.conda/envs/pytorch/lib/python3.11/site-packages/torch/lib/libtorch_hip.so)
frame #3: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0x14d (0x7fff873fd57d in /ccs/home/kiefera/.conda/envs/pytorch/lib/python3.11/site-packages/torch/lib/libtorch_hip.so)
frame #4: <unknown function> + 0xe4793 (0x7fffeb42d793 in /usr/lib64/libstdc++.so.6)
frame #5: <unknown function> + 0xa761c (0x7fffed74f61c in /lib64/libc.so.6)
frame #6: <unknown function> + 0x12eaa8 (0x7fffed7d6aa8 in /lib64/libc.so.6)

terminate called after throwing an instance of 'c10::DistBackendError'
  what():  [PG ID 1 PG GUID 1 Rank 0] Process group watchdog thread terminated with exception: [Rank 0] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=3, OpType=BROADCAST, NumelIn=5225280, NumelOut=5225280, Timeout(ms)=600000) ran for 600005 milliseconds before timing out.
Exception raised from checkTimeout at /pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:632 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >) + 0x98 (0x7fff2dbba968 in /ccs/home/kiefera/.conda/envs/pytorch/lib/python3.11/site-packages/torch/lib/libc10.so)
frame #1: c10d::ProcessGroupNCCL::WorkNCCL::checkTimeout(std::optional<std::chrono::duration<long, std::ratio<1l, 1000l> > >) + 0x23d (0x7fff873fb2dd in /ccs/home/kiefera/.conda/envs/pytorch/lib/python3.11/site-packages/torch/lib/libtorch_hip.so)
frame #2: c10d::ProcessGroupNCCL::watchdogHandler() + 0x9e8 (0x7fff873fc8d8 in /ccs/home/kiefera/.conda/envs/pytorch/lib/python3.11/site-packages/torch/lib/libtorch_hip.so)
frame #3: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0x14d (0x7fff873fd57d in /ccs/home/kiefera/.conda/envs/pytorch/lib/python3.11/site-packages/torch/lib/libtorch_hip.so)
frame #4: <unknown function> + 0xe4793 (0x7fffeb42d793 in /usr/lib64/libstdc++.so.6)
frame #5: <unknown function> + 0xa761c (0x7fffed74f61c in /lib64/libc.so.6)
frame #6: <unknown function> + 0x12eaa8 (0x7fffed7d6aa8 in /lib64/libc.so.6)

Exception raised from ncclCommWatchdog at /pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1901 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >) + 0x98 (0x7fff2dbba968 in /ccs/home/kiefera/.conda/envs/pytorch/lib/python3.11/site-packages/torch/lib/libc10.so)
frame #1: <unknown function> + 0x22990fe (0x7fff873d10fe in /ccs/home/kiefera/.conda/envs/pytorch/lib/python3.11/site-packages/torch/lib/libtorch_hip.so)
frame #2: <unknown function> + 0x69cfc1 (0x7fff857d4fc1 in /ccs/home/kiefera/.conda/envs/pytorch/lib/python3.11/site-packages/torch/lib/libtorch_hip.so)
frame #3: <unknown function> + 0xe4793 (0x7fffeb42d793 in /usr/lib64/libstdc++.so.6)
frame #4: <unknown function> + 0xa761c (0x7fffed74f61c in /lib64/libc.so.6)
frame #5: <unknown function> + 0x12eaa8 (0x7fffed7d6aa8 in /lib64/libc.so.6)

[rank4]:[E228 09:08:53.534018074 ProcessGroupNCCL.cpp:629] [Rank 0] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=3, OpType=BROADCAST, NumelIn=5225280, NumelOut=5225280, Timeout(ms)=600000) ran for 600032 milliseconds before timing out.
[rank5]:[E228 09:08:53.534028504 ProcessGroupNCCL.cpp:629] [Rank 0] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=3, OpType=BROADCAST, NumelIn=5225280, NumelOut=5225280, Timeout(ms)=600000) ran for 600064 milliseconds before timing out.
[rank4]:[E228 09:08:53.534135871 ProcessGroupNCCL.cpp:2168] [PG ID 1 PG GUID 5 Rank 0]  failure detected by watchdog at work sequence id: 3 PG status: last enqueued work: 3, last completed work: 2
[rank4]:[E228 09:08:53.534143546 ProcessGroupNCCL.cpp:667] Stack trace of the failed collective not found, potentially because FlightRecorder is disabled. You can enable it by setting TORCH_NCCL_TRACE_BUFFER_SIZE to a non-zero value.
[rank4]:[E228 09:08:53.534147955 ProcessGroupNCCL.cpp:681] [Rank 0] Some NCCL operations have failed or timed out. Due to the asynchronous nature of CUDA kernels, subsequent GPU operations might run on corrupted/incomplete data.
[rank4]:[E228 09:08:53.534151031 ProcessGroupNCCL.cpp:695] [Rank 0] To avoid data inconsistency, we are taking the entire process down.
[rank5]:[E228 09:08:53.534141622 ProcessGroupNCCL.cpp:2168] [PG ID 1 PG GUID 6 Rank 0]  failure detected by watchdog at work sequence id: 3 PG status: last enqueued work: 3, last completed work: 2
[rank5]:[E228 09:08:53.534148746 ProcessGroupNCCL.cpp:667] Stack trace of the failed collective not found, potentially because FlightRecorder is disabled. You can enable it by setting TORCH_NCCL_TRACE_BUFFER_SIZE to a non-zero value.
[rank5]:[E228 09:08:53.534153185 ProcessGroupNCCL.cpp:681] [Rank 0] Some NCCL operations have failed or timed out. Due to the asynchronous nature of CUDA kernels, subsequent GPU operations might run on corrupted/incomplete data.
[rank5]:[E228 09:08:53.534156020 ProcessGroupNCCL.cpp:695] [Rank 0] To avoid data inconsistency, we are taking the entire process down.
[rank4]:[E228 09:08:53.534997415 ProcessGroupNCCL.cpp:1895] [PG ID 1 PG GUID 5 Rank 0] Process group watchdog thread terminated with exception: [Rank 0] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=3, OpType=BROADCAST, NumelIn=5225280, NumelOut=5225280, Timeout(ms)=600000) ran for 600032 milliseconds before timing out.
Exception raised from checkTimeout at /pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:632 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >) + 0x98 (0x7fff2dbba968 in /ccs/home/kiefera/.conda/envs/pytorch/lib/python3.11/site-packages/torch/lib/libc10.so)
frame #1: c10d::ProcessGroupNCCL::WorkNCCL::checkTimeout(std::optional<std::chrono::duration<long, std::ratio<1l, 1000l> > >) + 0x23d (0x7fff873fb2dd in /ccs/home/kiefera/.conda/envs/pytorch/lib/python3.11/site-packages/torch/lib/libtorch_hip.so)
frame #2: c10d::ProcessGroupNCCL::watchdogHandler() + 0x9e8 (0x7fff873fc8d8 in /ccs/home/kiefera/.conda/envs/pytorch/lib/python3.11/site-packages/torch/lib/libtorch_hip.so)
frame #3: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0x14d (0x7fff873fd57d in /ccs/home/kiefera/.conda/envs/pytorch/lib/python3.11/site-packages/torch/lib/libtorch_hip.so)
frame #4: <unknown function> + 0xe4793 (0x7fffeb42d793 in /usr/lib64/libstdc++.so.6)
frame #5: <unknown function> + 0xa761c (0x7fffed74f61c in /lib64/libc.so.6)
frame #6: <unknown function> + 0x12eaa8 (0x7fffed7d6aa8 in /lib64/libc.so.6)

[rank5]:[E228 09:08:53.535003737 ProcessGroupNCCL.cpp:1895] [PG ID 1 PG GUID 6 Rank 0] Process group watchdog thread terminated with exception: [Rank 0] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=3, OpType=BROADCAST, NumelIn=5225280, NumelOut=5225280, Timeout(ms)=600000) ran for 600064 milliseconds before timing out.
Exception raised from checkTimeout at /pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:632 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >) + 0x98 (0x7fff2dbba968 in /ccs/home/kiefera/.conda/envs/pytorch/lib/python3.11/site-packages/torch/lib/libc10.so)
frame #1: c10d::ProcessGroupNCCL::WorkNCCL::checkTimeout(std::optional<std::chrono::duration<long, std::ratio<1l, 1000l> > >) + 0x23d (0x7fff873fb2dd in /ccs/home/kiefera/.conda/envs/pytorch/lib/python3.11/site-packages/torch/lib/libtorch_hip.so)
frame #2: c10d::ProcessGroupNCCL::watchdogHandler() + 0x9e8 (0x7fff873fc8d8 in /ccs/home/kiefera/.conda/envs/pytorch/lib/python3.11/site-packages/torch/lib/libtorch_hip.so)
frame #3: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0x14d (0x7fff873fd57d in /ccs/home/kiefera/.conda/envs/pytorch/lib/python3.11/site-packages/torch/lib/libtorch_hip.so)
frame #4: <unknown function> + 0xe4793 (0x7fffeb42d793 in /usr/lib64/libstdc++.so.6)
frame #5: <unknown function> + 0xa761c (0x7fffed74f61c in /lib64/libc.so.6)
frame #6: <unknown function> + 0x12eaa8 (0x7fffed7d6aa8 in /lib64/libc.so.6)

terminate called after throwing an instance of 'c10::DistBackendError'
terminate called after throwing an instance of 'c10::DistBackendError'
  what():  [PG ID 1 PG GUID 5 Rank 0] Process group watchdog thread terminated with exception: [Rank 0] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=3, OpType=BROADCAST, NumelIn=5225280, NumelOut=5225280, Timeout(ms)=600000) ran for 600032 milliseconds before timing out.
Exception raised from checkTimeout at /pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:632 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >) + 0x98 (0x7fff2dbba968 in /ccs/home/kiefera/.conda/envs/pytorch/lib/python3.11/site-packages/torch/lib/libc10.so)
frame #1: c10d::ProcessGroupNCCL::WorkNCCL::checkTimeout(std::optional<std::chrono::duration<long, std::ratio<1l, 1000l> > >) + 0x23d (0x7fff873fb2dd in /ccs/home/kiefera/.conda/envs/pytorch/lib/python3.11/site-packages/torch/lib/libtorch_hip.so)
frame #2: c10d::ProcessGroupNCCL::watchdogHandler() + 0x9e8 (0x7fff873fc8d8 in /ccs/home/kiefera/.conda/envs/pytorch/lib/python3.11/site-packages/torch/lib/libtorch_hip.so)
frame #3: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0x14d (0x7fff873fd57d in /ccs/home/kiefera/.conda/envs/pytorch/lib/python3.11/site-packages/torch/lib/libtorch_hip.so)
frame #4: <unknown function> + 0xe4793 (0x7fffeb42d793 in /usr/lib64/libstdc++.so.6)
frame #5: <unknown function> + 0xa761c (0x7fffed74f61c in /lib64/libc.so.6)
frame #6: <unknown function> + 0x12eaa8 (0x7fffed7d6aa8 in /lib64/libc.so.6)

Exception raised from ncclCommWatchdog at /pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1901 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >) + 0x98 (0x7fff2dbba968 in /ccs/home/kiefera/.conda/envs/pytorch/lib/python3.11/site-packages/torch/lib/libc10.so)
frame #1: <unknown function> + 0x22990fe (0x7fff873d10fe in /ccs/home/kiefera/.conda/envs/pytorch/lib/python3.11/site-packages/torch/lib/libtorch_hip.so)
frame #2: <unknown function> + 0x69cfc1 (0x7fff857d4fc1 in /ccs/home/kiefera/.conda/envs/pytorch/lib/python3.11/site-packages/torch/lib/libtorch_hip.so)
frame #3: <unknown function> + 0xe4793 (0x7fffeb42d793 in /usr/lib64/libstdc++.so.6)
frame #4: <unknown function> + 0xa761c (0x7fffed74f61c in /lib64/libc.so.6)
frame #5: <unknown function> + 0x12eaa8 (0x7fffed7d6aa8 in /lib64/libc.so.6)

  what():  [PG ID 1 PG GUID 6 Rank 0] Process group watchdog thread terminated with exception: [Rank 0] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=3, OpType=BROADCAST, NumelIn=5225280, NumelOut=5225280, Timeout(ms)=600000) ran for 600064 milliseconds before timing out.
Exception raised from checkTimeout at /pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:632 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >) + 0x98 (0x7fff2dbba968 in /ccs/home/kiefera/.conda/envs/pytorch/lib/python3.11/site-packages/torch/lib/libc10.so)
frame #1: c10d::ProcessGroupNCCL::WorkNCCL::checkTimeout(std::optional<std::chrono::duration<long, std::ratio<1l, 1000l> > >) + 0x23d (0x7fff873fb2dd in /ccs/home/kiefera/.conda/envs/pytorch/lib/python3.11/site-packages/torch/lib/libtorch_hip.so)
frame #2: c10d::ProcessGroupNCCL::watchdogHandler() + 0x9e8 (0x7fff873fc8d8 in /ccs/home/kiefera/.conda/envs/pytorch/lib/python3.11/site-packages/torch/lib/libtorch_hip.so)
frame #3: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0x14d (0x7fff873fd57d in /ccs/home/kiefera/.conda/envs/pytorch/lib/python3.11/site-packages/torch/lib/libtorch_hip.so)
frame #4: <unknown function> + 0xe4793 (0x7fffeb42d793 in /usr/lib64/libstdc++.so.6)
frame #5: <unknown function> + 0xa761c (0x7fffed74f61c in /lib64/libc.so.6)
frame #6: <unknown function> + 0x12eaa8 (0x7fffed7d6aa8 in /lib64/libc.so.6)

Exception raised from ncclCommWatchdog at /pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1901 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >) + 0x98 (0x7fff2dbba968 in /ccs/home/kiefera/.conda/envs/pytorch/lib/python3.11/site-packages/torch/lib/libc10.so)
frame #1: <unknown function> + 0x22990fe (0x7fff873d10fe in /ccs/home/kiefera/.conda/envs/pytorch/lib/python3.11/site-packages/torch/lib/libtorch_hip.so)
frame #2: <unknown function> + 0x69cfc1 (0x7fff857d4fc1 in /ccs/home/kiefera/.conda/envs/pytorch/lib/python3.11/site-packages/torch/lib/libtorch_hip.so)
frame #3: <unknown function> + 0xe4793 (0x7fffeb42d793 in /usr/lib64/libstdc++.so.6)
frame #4: <unknown function> + 0xa761c (0x7fffed74f61c in /lib64/libc.so.6)
frame #5: <unknown function> + 0x12eaa8 (0x7fffed7d6aa8 in /lib64/libc.so.6)

[rank14]:[E228 09:08:53.624118831 ProcessGroupNCCL.cpp:629] [Rank 1] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=2, OpType=BROADCAST, NumelIn=840, NumelOut=840, Timeout(ms)=600000) ran for 600093 milliseconds before timing out.
[rank14]:[E228 09:08:53.624230417 ProcessGroupNCCL.cpp:2168] [PG ID 1 PG GUID 7 Rank 1]  failure detected by watchdog at work sequence id: 2 PG status: last enqueued work: 2, last completed work: 1
[rank14]:[E228 09:08:53.624238062 ProcessGroupNCCL.cpp:667] Stack trace of the failed collective not found, potentially because FlightRecorder is disabled. You can enable it by setting TORCH_NCCL_TRACE_BUFFER_SIZE to a non-zero value.
[rank14]:[E228 09:08:53.624241950 ProcessGroupNCCL.cpp:681] [Rank 1] Some NCCL operations have failed or timed out. Due to the asynchronous nature of CUDA kernels, subsequent GPU operations might run on corrupted/incomplete data.
[rank14]:[E228 09:08:53.624244995 ProcessGroupNCCL.cpp:695] [Rank 1] To avoid data inconsistency, we are taking the entire process down.
[rank14]:[E228 09:08:53.625088768 ProcessGroupNCCL.cpp:1895] [PG ID 1 PG GUID 7 Rank 1] Process group watchdog thread terminated with exception: [Rank 1] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=2, OpType=BROADCAST, NumelIn=840, NumelOut=840, Timeout(ms)=600000) ran for 600093 milliseconds before timing out.
Exception raised from checkTimeout at /pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:632 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >) + 0x98 (0x7fff2dbba968 in /ccs/home/kiefera/.conda/envs/pytorch/lib/python3.11/site-packages/torch/lib/libc10.so)
frame #1: c10d::ProcessGroupNCCL::WorkNCCL::checkTimeout(std::optional<std::chrono::duration<long, std::ratio<1l, 1000l> > >) + 0x23d (0x7fff873fb2dd in /ccs/home/kiefera/.conda/envs/pytorch/lib/python3.11/site-packages/torch/lib/libtorch_hip.so)
frame #2: c10d::ProcessGroupNCCL::watchdogHandler() + 0x9e8 (0x7fff873fc8d8 in /ccs/home/kiefera/.conda/envs/pytorch/lib/python3.11/site-packages/torch/lib/libtorch_hip.so)
frame #3: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0x14d (0x7fff873fd57d in /ccs/home/kiefera/.conda/envs/pytorch/lib/python3.11/site-packages/torch/lib/libtorch_hip.so)
frame #4: <unknown function> + 0xe4793 (0x7fffeb42d793 in /usr/lib64/libstdc++.so.6)
frame #5: <unknown function> + 0xa761c (0x7fffed74f61c in /lib64/libc.so.6)
frame #6: <unknown function> + 0x12eaa8 (0x7fffed7d6aa8 in /lib64/libc.so.6)

terminate called after throwing an instance of 'c10::DistBackendError'
  what():  [PG ID 1 PG GUID 7 Rank 1] Process group watchdog thread terminated with exception: [Rank 1] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=2, OpType=BROADCAST, NumelIn=840, NumelOut=840, Timeout(ms)=600000) ran for 600093 milliseconds before timing out.
Exception raised from checkTimeout at /pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:632 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >) + 0x98 (0x7fff2dbba968 in /ccs/home/kiefera/.conda/envs/pytorch/lib/python3.11/site-packages/torch/lib/libc10.so)
frame #1: c10d::ProcessGroupNCCL::WorkNCCL::checkTimeout(std::optional<std::chrono::duration<long, std::ratio<1l, 1000l> > >) + 0x23d (0x7fff873fb2dd in /ccs/home/kiefera/.conda/envs/pytorch/lib/python3.11/site-packages/torch/lib/libtorch_hip.so)
frame #2: c10d::ProcessGroupNCCL::watchdogHandler() + 0x9e8 (0x7fff873fc8d8 in /ccs/home/kiefera/.conda/envs/pytorch/lib/python3.11/site-packages/torch/lib/libtorch_hip.so)
frame #3: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0x14d (0x7fff873fd57d in /ccs/home/kiefera/.conda/envs/pytorch/lib/python3.11/site-packages/torch/lib/libtorch_hip.so)
frame #4: <unknown function> + 0xe4793 (0x7fffeb42d793 in /usr/lib64/libstdc++.so.6)
frame #5: <unknown function> + 0xa761c (0x7fffed74f61c in /lib64/libc.so.6)
frame #6: <unknown function> + 0x12eaa8 (0x7fffed7d6aa8 in /lib64/libc.so.6)

Exception raised from ncclCommWatchdog at /pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1901 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >) + 0x98 (0x7fff2dbba968 in /ccs/home/kiefera/.conda/envs/pytorch/lib/python3.11/site-packages/torch/lib/libc10.so)
frame #1: <unknown function> + 0x22990fe (0x7fff873d10fe in /ccs/home/kiefera/.conda/envs/pytorch/lib/python3.11/site-packages/torch/lib/libtorch_hip.so)
frame #2: <unknown function> + 0x69cfc1 (0x7fff857d4fc1 in /ccs/home/kiefera/.conda/envs/pytorch/lib/python3.11/site-packages/torch/lib/libtorch_hip.so)
frame #3: <unknown function> + 0xe4793 (0x7fffeb42d793 in /usr/lib64/libstdc++.so.6)
frame #4: <unknown function> + 0xa761c (0x7fffed74f61c in /lib64/libc.so.6)
frame #5: <unknown function> + 0x12eaa8 (0x7fffed7d6aa8 in /lib64/libc.so.6)

[rank8]:[E228 09:08:53.630211952 ProcessGroupNCCL.cpp:629] [Rank 1] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=2, OpType=BROADCAST, NumelIn=840, NumelOut=840, Timeout(ms)=600000) ran for 600059 milliseconds before timing out.
[rank8]:[E228 09:08:53.630316535 ProcessGroupNCCL.cpp:2168] [PG ID 1 PG GUID 1 Rank 1]  failure detected by watchdog at work sequence id: 2 PG status: last enqueued work: 2, last completed work: 1
[rank8]:[E228 09:08:53.630323779 ProcessGroupNCCL.cpp:667] Stack trace of the failed collective not found, potentially because FlightRecorder is disabled. You can enable it by setting TORCH_NCCL_TRACE_BUFFER_SIZE to a non-zero value.
[rank8]:[E228 09:08:53.630327827 ProcessGroupNCCL.cpp:681] [Rank 1] Some NCCL operations have failed or timed out. Due to the asynchronous nature of CUDA kernels, subsequent GPU operations might run on corrupted/incomplete data.
[rank8]:[E228 09:08:53.630330542 ProcessGroupNCCL.cpp:695] [Rank 1] To avoid data inconsistency, we are taking the entire process down.
[rank8]:[E228 09:08:53.631173293 ProcessGroupNCCL.cpp:1895] [PG ID 1 PG GUID 1 Rank 1] Process group watchdog thread terminated with exception: [Rank 1] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=2, OpType=BROADCAST, NumelIn=840, NumelOut=840, Timeout(ms)=600000) ran for 600059 milliseconds before timing out.
Exception raised from checkTimeout at /pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:632 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >) + 0x98 (0x7fff2dbba968 in /ccs/home/kiefera/.conda/envs/pytorch/lib/python3.11/site-packages/torch/lib/libc10.so)
frame #1: c10d::ProcessGroupNCCL::WorkNCCL::checkTimeout(std::optional<std::chrono::duration<long, std::ratio<1l, 1000l> > >) + 0x23d (0x7fff873fb2dd in /ccs/home/kiefera/.conda/envs/pytorch/lib/python3.11/site-packages/torch/lib/libtorch_hip.so)
frame #2: c10d::ProcessGroupNCCL::watchdogHandler() + 0x9e8 (0x7fff873fc8d8 in /ccs/home/kiefera/.conda/envs/pytorch/lib/python3.11/site-packages/torch/lib/libtorch_hip.so)
frame #3: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0x14d (0x7fff873fd57d in /ccs/home/kiefera/.conda/envs/pytorch/lib/python3.11/site-packages/torch/lib/libtorch_hip.so)
frame #4: <unknown function> + 0xe4793 (0x7fffeb42d793 in /usr/lib64/libstdc++.so.6)
frame #5: <unknown function> + 0xa761c (0x7fffed74f61c in /lib64/libc.so.6)
frame #6: <unknown function> + 0x12eaa8 (0x7fffed7d6aa8 in /lib64/libc.so.6)

terminate called after throwing an instance of 'c10::DistBackendError'
  what():  [PG ID 1 PG GUID 1 Rank 1] Process group watchdog thread terminated with exception: [Rank 1] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=2, OpType=BROADCAST, NumelIn=840, NumelOut=840, Timeout(ms)=600000) ran for 600059 milliseconds before timing out.
Exception raised from checkTimeout at /pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:632 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >) + 0x98 (0x7fff2dbba968 in /ccs/home/kiefera/.conda/envs/pytorch/lib/python3.11/site-packages/torch/lib/libc10.so)
frame #1: c10d::ProcessGroupNCCL::WorkNCCL::checkTimeout(std::optional<std::chrono::duration<long, std::ratio<1l, 1000l> > >) + 0x23d (0x7fff873fb2dd in /ccs/home/kiefera/.conda/envs/pytorch/lib/python3.11/site-packages/torch/lib/libtorch_hip.so)
frame #2: c10d::ProcessGroupNCCL::watchdogHandler() + 0x9e8 (0x7fff873fc8d8 in /ccs/home/kiefera/.conda/envs/pytorch/lib/python3.11/site-packages/torch/lib/libtorch_hip.so)
frame #3: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0x14d (0x7fff873fd57d in /ccs/home/kiefera/.conda/envs/pytorch/lib/python3.11/site-packages/torch/lib/libtorch_hip.so)
frame #4: <unknown function> + 0xe4793 (0x7fffeb42d793 in /usr/lib64/libstdc++.so.6)
frame #5: <unknown function> + 0xa761c (0x7fffed74f61c in /lib64/libc.so.6)
frame #6: <unknown function> + 0x12eaa8 (0x7fffed7d6aa8 in /lib64/libc.so.6)

Exception raised from ncclCommWatchdog at /pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1901 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >) + 0x98 (0x7fff2dbba968 in /ccs/home/kiefera/.conda/envs/pytorch/lib/python3.11/site-packages/torch/lib/libc10.so)
frame #1: <unknown function> + 0x22990fe (0x7fff873d10fe in /ccs/home/kiefera/.conda/envs/pytorch/lib/python3.11/site-packages/torch/lib/libtorch_hip.so)
frame #2: <unknown function> + 0x69cfc1 (0x7fff857d4fc1 in /ccs/home/kiefera/.conda/envs/pytorch/lib/python3.11/site-packages/torch/lib/libtorch_hip.so)
frame #3: <unknown function> + 0xe4793 (0x7fffeb42d793 in /usr/lib64/libstdc++.so.6)
frame #4: <unknown function> + 0xa761c (0x7fffed74f61c in /lib64/libc.so.6)
frame #5: <unknown function> + 0x12eaa8 (0x7fffed7d6aa8 in /lib64/libc.so.6)

[rank12]:[E228 09:08:53.639114520 ProcessGroupNCCL.cpp:629] [Rank 1] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=2, OpType=BROADCAST, NumelIn=840, NumelOut=840, Timeout(ms)=600000) ran for 600080 milliseconds before timing out.
[rank12]:[E228 09:08:53.639225875 ProcessGroupNCCL.cpp:2168] [PG ID 1 PG GUID 5 Rank 1]  failure detected by watchdog at work sequence id: 2 PG status: last enqueued work: 2, last completed work: 1
[rank12]:[E228 09:08:53.639233550 ProcessGroupNCCL.cpp:667] Stack trace of the failed collective not found, potentially because FlightRecorder is disabled. You can enable it by setting TORCH_NCCL_TRACE_BUFFER_SIZE to a non-zero value.
[rank12]:[E228 09:08:53.639237648 ProcessGroupNCCL.cpp:681] [Rank 1] Some NCCL operations have failed or timed out. Due to the asynchronous nature of CUDA kernels, subsequent GPU operations might run on corrupted/incomplete data.
[rank12]:[E228 09:08:53.639241436 ProcessGroupNCCL.cpp:695] [Rank 1] To avoid data inconsistency, we are taking the entire process down.
[rank12]:[E228 09:08:53.640091340 ProcessGroupNCCL.cpp:1895] [PG ID 1 PG GUID 5 Rank 1] Process group watchdog thread terminated with exception: [Rank 1] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=2, OpType=BROADCAST, NumelIn=840, NumelOut=840, Timeout(ms)=600000) ran for 600080 milliseconds before timing out.
Exception raised from checkTimeout at /pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:632 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >) + 0x98 (0x7fff2dbba968 in /ccs/home/kiefera/.conda/envs/pytorch/lib/python3.11/site-packages/torch/lib/libc10.so)
frame #1: c10d::ProcessGroupNCCL::WorkNCCL::checkTimeout(std::optional<std::chrono::duration<long, std::ratio<1l, 1000l> > >) + 0x23d (0x7fff873fb2dd in /ccs/home/kiefera/.conda/envs/pytorch/lib/python3.11/site-packages/torch/lib/libtorch_hip.so)
frame #2: c10d::ProcessGroupNCCL::watchdogHandler() + 0x9e8 (0x7fff873fc8d8 in /ccs/home/kiefera/.conda/envs/pytorch/lib/python3.11/site-packages/torch/lib/libtorch_hip.so)
frame #3: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0x14d (0x7fff873fd57d in /ccs/home/kiefera/.conda/envs/pytorch/lib/python3.11/site-packages/torch/lib/libtorch_hip.so)
frame #4: <unknown function> + 0xe4793 (0x7fffeb42d793 in /usr/lib64/libstdc++.so.6)
frame #5: <unknown function> + 0xa761c (0x7fffed74f61c in /lib64/libc.so.6)
frame #6: <unknown function> + 0x12eaa8 (0x7fffed7d6aa8 in /lib64/libc.so.6)

terminate called after throwing an instance of 'c10::DistBackendError'
  what():  [PG ID 1 PG GUID 5 Rank 1] Process group watchdog thread terminated with exception: [Rank 1] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=2, OpType=BROADCAST, NumelIn=840, NumelOut=840, Timeout(ms)=600000) ran for 600080 milliseconds before timing out.
Exception raised from checkTimeout at /pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:632 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >) + 0x98 (0x7fff2dbba968 in /ccs/home/kiefera/.conda/envs/pytorch/lib/python3.11/site-packages/torch/lib/libc10.so)
frame #1: c10d::ProcessGroupNCCL::WorkNCCL::checkTimeout(std::optional<std::chrono::duration<long, std::ratio<1l, 1000l> > >) + 0x23d (0x7fff873fb2dd in /ccs/home/kiefera/.conda/envs/pytorch/lib/python3.11/site-packages/torch/lib/libtorch_hip.so)
frame #2: c10d::ProcessGroupNCCL::watchdogHandler() + 0x9e8 (0x7fff873fc8d8 in /ccs/home/kiefera/.conda/envs/pytorch/lib/python3.11/site-packages/torch/lib/libtorch_hip.so)
frame #3: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0x14d (0x7fff873fd57d in /ccs/home/kiefera/.conda/envs/pytorch/lib/python3.11/site-packages/torch/lib/libtorch_hip.so)
frame #4: <unknown function> + 0xe4793 (0x7fffeb42d793 in /usr/lib64/libstdc++.so.6)
frame #5: <unknown function> + 0xa761c (0x7fffed74f61c in /lib64/libc.so.6)
frame #6: <unknown function> + 0x12eaa8 (0x7fffed7d6aa8 in /lib64/libc.so.6)

Exception raised from ncclCommWatchdog at /pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1901 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >) + 0x98 (0x7fff2dbba968 in /ccs/home/kiefera/.conda/envs/pytorch/lib/python3.11/site-packages/torch/lib/libc10.so)
frame #1: <unknown function> + 0x22990fe (0x7fff873d10fe in /ccs/home/kiefera/.conda/envs/pytorch/lib/python3.11/site-packages/torch/lib/libtorch_hip.so)
frame #2: <unknown function> + 0x69cfc1 (0x7fff857d4fc1 in /ccs/home/kiefera/.conda/envs/pytorch/lib/python3.11/site-packages/torch/lib/libtorch_hip.so)
frame #3: <unknown function> + 0xe4793 (0x7fffeb42d793 in /usr/lib64/libstdc++.so.6)
frame #4: <unknown function> + 0xa761c (0x7fffed74f61c in /lib64/libc.so.6)
frame #5: <unknown function> + 0x12eaa8 (0x7fffed7d6aa8 in /lib64/libc.so.6)

[rank10]:[E228 09:08:53.648884857 ProcessGroupNCCL.cpp:629] [Rank 1] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=2, OpType=BROADCAST, NumelIn=840, NumelOut=840, Timeout(ms)=600000) ran for 600070 milliseconds before timing out.
[rank10]:[E228 09:08:53.648999369 ProcessGroupNCCL.cpp:2168] [PG ID 1 PG GUID 3 Rank 1]  failure detected by watchdog at work sequence id: 2 PG status: last enqueued work: 2, last completed work: 1
[rank10]:[E228 09:08:53.649007505 ProcessGroupNCCL.cpp:667] Stack trace of the failed collective not found, potentially because FlightRecorder is disabled. You can enable it by setting TORCH_NCCL_TRACE_BUFFER_SIZE to a non-zero value.
[rank10]:[E228 09:08:53.649011763 ProcessGroupNCCL.cpp:681] [Rank 1] Some NCCL operations have failed or timed out. Due to the asynchronous nature of CUDA kernels, subsequent GPU operations might run on corrupted/incomplete data.
[rank10]:[E228 09:08:53.649014749 ProcessGroupNCCL.cpp:695] [Rank 1] To avoid data inconsistency, we are taking the entire process down.
[rank10]:[E228 09:08:53.649861417 ProcessGroupNCCL.cpp:1895] [PG ID 1 PG GUID 3 Rank 1] Process group watchdog thread terminated with exception: [Rank 1] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=2, OpType=BROADCAST, NumelIn=840, NumelOut=840, Timeout(ms)=600000) ran for 600070 milliseconds before timing out.
Exception raised from checkTimeout at /pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:632 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >) + 0x98 (0x7fff2dbba968 in /ccs/home/kiefera/.conda/envs/pytorch/lib/python3.11/site-packages/torch/lib/libc10.so)
frame #1: c10d::ProcessGroupNCCL::WorkNCCL::checkTimeout(std::optional<std::chrono::duration<long, std::ratio<1l, 1000l> > >) + 0x23d (0x7fff873fb2dd in /ccs/home/kiefera/.conda/envs/pytorch/lib/python3.11/site-packages/torch/lib/libtorch_hip.so)
frame #2: c10d::ProcessGroupNCCL::watchdogHandler() + 0x9e8 (0x7fff873fc8d8 in /ccs/home/kiefera/.conda/envs/pytorch/lib/python3.11/site-packages/torch/lib/libtorch_hip.so)
frame #3: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0x14d (0x7fff873fd57d in /ccs/home/kiefera/.conda/envs/pytorch/lib/python3.11/site-packages/torch/lib/libtorch_hip.so)
frame #4: <unknown function> + 0xe4793 (0x7fffeb42d793 in /usr/lib64/libstdc++.so.6)
frame #5: <unknown function> + 0xa761c (0x7fffed74f61c in /lib64/libc.so.6)
frame #6: <unknown function> + 0x12eaa8 (0x7fffed7d6aa8 in /lib64/libc.so.6)

terminate called after throwing an instance of 'c10::DistBackendError'
  what():  [PG ID 1 PG GUID 3 Rank 1] Process group watchdog thread terminated with exception: [Rank 1] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=2, OpType=BROADCAST, NumelIn=840, NumelOut=840, Timeout(ms)=600000) ran for 600070 milliseconds before timing out.
Exception raised from checkTimeout at /pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:632 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >) + 0x98 (0x7fff2dbba968 in /ccs/home/kiefera/.conda/envs/pytorch/lib/python3.11/site-packages/torch/lib/libc10.so)
frame #1: c10d::ProcessGroupNCCL::WorkNCCL::checkTimeout(std::optional<std::chrono::duration<long, std::ratio<1l, 1000l> > >) + 0x23d (0x7fff873fb2dd in /ccs/home/kiefera/.conda/envs/pytorch/lib/python3.11/site-packages/torch/lib/libtorch_hip.so)
frame #2: c10d::ProcessGroupNCCL::watchdogHandler() + 0x9e8 (0x7fff873fc8d8 in /ccs/home/kiefera/.conda/envs/pytorch/lib/python3.11/site-packages/torch/lib/libtorch_hip.so)
frame #3: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0x14d (0x7fff873fd57d in /ccs/home/kiefera/.conda/envs/pytorch/lib/python3.11/site-packages/torch/lib/libtorch_hip.so)
frame #4: <unknown function> + 0xe4793 (0x7fffeb42d793 in /usr/lib64/libstdc++.so.6)
frame #5: <unknown function> + 0xa761c (0x7fffed74f61c in /lib64/libc.so.6)
frame #6: <unknown function> + 0x12eaa8 (0x7fffed7d6aa8 in /lib64/libc.so.6)

Exception raised from ncclCommWatchdog at /pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1901 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >) + 0x98 (0x7fff2dbba968 in /ccs/home/kiefera/.conda/envs/pytorch/lib/python3.11/site-packages/torch/lib/libc10.so)
frame #1: <unknown function> + 0x22990fe (0x7fff873d10fe in /ccs/home/kiefera/.conda/envs/pytorch/lib/python3.11/site-packages/torch/lib/libtorch_hip.so)
frame #2: <unknown function> + 0x69cfc1 (0x7fff857d4fc1 in /ccs/home/kiefera/.conda/envs/pytorch/lib/python3.11/site-packages/torch/lib/libtorch_hip.so)
frame #3: <unknown function> + 0xe4793 (0x7fffeb42d793 in /usr/lib64/libstdc++.so.6)
frame #4: <unknown function> + 0xa761c (0x7fffed74f61c in /lib64/libc.so.6)
frame #5: <unknown function> + 0x12eaa8 (0x7fffed7d6aa8 in /lib64/libc.so.6)

[rank2]:[E228 09:08:53.602512250 ProcessGroupNCCL.cpp:629] [Rank 0] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=3, OpType=BROADCAST, NumelIn=5225280, NumelOut=5225280, Timeout(ms)=600000) ran for 600080 milliseconds before timing out.
[rank2]:[E228 09:08:53.602624837 ProcessGroupNCCL.cpp:2168] [PG ID 1 PG GUID 3 Rank 0]  failure detected by watchdog at work sequence id: 3 PG status: last enqueued work: 3, last completed work: 2
[rank2]:[E228 09:08:53.602632632 ProcessGroupNCCL.cpp:667] Stack trace of the failed collective not found, potentially because FlightRecorder is disabled. You can enable it by setting TORCH_NCCL_TRACE_BUFFER_SIZE to a non-zero value.
[rank2]:[E228 09:08:53.602636861 ProcessGroupNCCL.cpp:681] [Rank 0] Some NCCL operations have failed or timed out. Due to the asynchronous nature of CUDA kernels, subsequent GPU operations might run on corrupted/incomplete data.
[rank2]:[E228 09:08:53.602639766 ProcessGroupNCCL.cpp:695] [Rank 0] To avoid data inconsistency, we are taking the entire process down.
[rank2]:[E228 09:08:53.603482654 ProcessGroupNCCL.cpp:1895] [PG ID 1 PG GUID 3 Rank 0] Process group watchdog thread terminated with exception: [Rank 0] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=3, OpType=BROADCAST, NumelIn=5225280, NumelOut=5225280, Timeout(ms)=600000) ran for 600080 milliseconds before timing out.
Exception raised from checkTimeout at /pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:632 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >) + 0x98 (0x7fff2dbba968 in /ccs/home/kiefera/.conda/envs/pytorch/lib/python3.11/site-packages/torch/lib/libc10.so)
frame #1: c10d::ProcessGroupNCCL::WorkNCCL::checkTimeout(std::optional<std::chrono::duration<long, std::ratio<1l, 1000l> > >) + 0x23d (0x7fff873fb2dd in /ccs/home/kiefera/.conda/envs/pytorch/lib/python3.11/site-packages/torch/lib/libtorch_hip.so)
frame #2: c10d::ProcessGroupNCCL::watchdogHandler() + 0x9e8 (0x7fff873fc8d8 in /ccs/home/kiefera/.conda/envs/pytorch/lib/python3.11/site-packages/torch/lib/libtorch_hip.so)
frame #3: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0x14d (0x7fff873fd57d in /ccs/home/kiefera/.conda/envs/pytorch/lib/python3.11/site-packages/torch/lib/libtorch_hip.so)
frame #4: <unknown function> + 0xe4793 (0x7fffeb42d793 in /usr/lib64/libstdc++.so.6)
frame #5: <unknown function> + 0xa761c (0x7fffed74f61c in /lib64/libc.so.6)
frame #6: <unknown function> + 0x12eaa8 (0x7fffed7d6aa8 in /lib64/libc.so.6)

terminate called after throwing an instance of 'c10::DistBackendError'
  what():  [PG ID 1 PG GUID 3 Rank 0] Process group watchdog thread terminated with exception: [Rank 0] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=3, OpType=BROADCAST, NumelIn=5225280, NumelOut=5225280, Timeout(ms)=600000) ran for 600080 milliseconds before timing out.
Exception raised from checkTimeout at /pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:632 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >) + 0x98 (0x7fff2dbba968 in /ccs/home/kiefera/.conda/envs/pytorch/lib/python3.11/site-packages/torch/lib/libc10.so)
frame #1: c10d::ProcessGroupNCCL::WorkNCCL::checkTimeout(std::optional<std::chrono::duration<long, std::ratio<1l, 1000l> > >) + 0x23d (0x7fff873fb2dd in /ccs/home/kiefera/.conda/envs/pytorch/lib/python3.11/site-packages/torch/lib/libtorch_hip.so)
frame #2: c10d::ProcessGroupNCCL::watchdogHandler() + 0x9e8 (0x7fff873fc8d8 in /ccs/home/kiefera/.conda/envs/pytorch/lib/python3.11/site-packages/torch/lib/libtorch_hip.so)
frame #3: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0x14d (0x7fff873fd57d in /ccs/home/kiefera/.conda/envs/pytorch/lib/python3.11/site-packages/torch/lib/libtorch_hip.so)
frame #4: <unknown function> + 0xe4793 (0x7fffeb42d793 in /usr/lib64/libstdc++.so.6)
frame #5: <unknown function> + 0xa761c (0x7fffed74f61c in /lib64/libc.so.6)
frame #6: <unknown function> + 0x12eaa8 (0x7fffed7d6aa8 in /lib64/libc.so.6)

Exception raised from ncclCommWatchdog at /pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1901 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >) + 0x98 (0x7fff2dbba968 in /ccs/home/kiefera/.conda/envs/pytorch/lib/python3.11/site-packages/torch/lib/libc10.so)
frame #1: <unknown function> + 0x22990fe (0x7fff873d10fe in /ccs/home/kiefera/.conda/envs/pytorch/lib/python3.11/site-packages/torch/lib/libtorch_hip.so)
frame #2: <unknown function> + 0x69cfc1 (0x7fff857d4fc1 in /ccs/home/kiefera/.conda/envs/pytorch/lib/python3.11/site-packages/torch/lib/libtorch_hip.so)
frame #3: <unknown function> + 0xe4793 (0x7fffeb42d793 in /usr/lib64/libstdc++.so.6)
frame #4: <unknown function> + 0xa761c (0x7fffed74f61c in /lib64/libc.so.6)
frame #5: <unknown function> + 0x12eaa8 (0x7fffed7d6aa8 in /lib64/libc.so.6)

[rank15]:[E228 09:09:20.379156933 ProcessGroupNCCL.cpp:629] [Rank 7] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=73, OpType=ALLREDUCE, NumelIn=24883200, NumelOut=24883200, Timeout(ms)=600000) ran for 600018 milliseconds before timing out.
[rank15]:[E228 09:09:20.379273358 ProcessGroupNCCL.cpp:2168] [PG ID 2 PG GUID 10 Rank 7]  failure detected by watchdog at work sequence id: 73 PG status: last enqueued work: 73, last completed work: 72
[rank15]:[E228 09:09:20.379281163 ProcessGroupNCCL.cpp:667] Stack trace of the failed collective not found, potentially because FlightRecorder is disabled. You can enable it by setting TORCH_NCCL_TRACE_BUFFER_SIZE to a non-zero value.
[rank15]:[E228 09:09:20.379285391 ProcessGroupNCCL.cpp:681] [Rank 7] Some NCCL operations have failed or timed out. Due to the asynchronous nature of CUDA kernels, subsequent GPU operations might run on corrupted/incomplete data.
[rank15]:[E228 09:09:20.379289579 ProcessGroupNCCL.cpp:695] [Rank 7] To avoid data inconsistency, we are taking the entire process down.
[rank15]:[E228 09:09:20.380145155 ProcessGroupNCCL.cpp:1895] [PG ID 2 PG GUID 10 Rank 7] Process group watchdog thread terminated with exception: [Rank 7] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=73, OpType=ALLREDUCE, NumelIn=24883200, NumelOut=24883200, Timeout(ms)=600000) ran for 600018 milliseconds before timing out.
Exception raised from checkTimeout at /pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:632 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >) + 0x98 (0x7fff2dbba968 in /ccs/home/kiefera/.conda/envs/pytorch/lib/python3.11/site-packages/torch/lib/libc10.so)
frame #1: c10d::ProcessGroupNCCL::WorkNCCL::checkTimeout(std::optional<std::chrono::duration<long, std::ratio<1l, 1000l> > >) + 0x23d (0x7fff873fb2dd in /ccs/home/kiefera/.conda/envs/pytorch/lib/python3.11/site-packages/torch/lib/libtorch_hip.so)
frame #2: c10d::ProcessGroupNCCL::watchdogHandler() + 0x9e8 (0x7fff873fc8d8 in /ccs/home/kiefera/.conda/envs/pytorch/lib/python3.11/site-packages/torch/lib/libtorch_hip.so)
frame #3: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0x14d (0x7fff873fd57d in /ccs/home/kiefera/.conda/envs/pytorch/lib/python3.11/site-packages/torch/lib/libtorch_hip.so)
frame #4: <unknown function> + 0xe4793 (0x7fffeb42d793 in /usr/lib64/libstdc++.so.6)
frame #5: <unknown function> + 0xa761c (0x7fffed74f61c in /lib64/libc.so.6)
frame #6: <unknown function> + 0x12eaa8 (0x7fffed7d6aa8 in /lib64/libc.so.6)

terminate called after throwing an instance of 'c10::DistBackendError'
  what():  [PG ID 2 PG GUID 10 Rank 7] Process group watchdog thread terminated with exception: [Rank 7] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=73, OpType=ALLREDUCE, NumelIn=24883200, NumelOut=24883200, Timeout(ms)=600000) ran for 600018 milliseconds before timing out.
Exception raised from checkTimeout at /pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:632 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >) + 0x98 (0x7fff2dbba968 in /ccs/home/kiefera/.conda/envs/pytorch/lib/python3.11/site-packages/torch/lib/libc10.so)
frame #1: c10d::ProcessGroupNCCL::WorkNCCL::checkTimeout(std::optional<std::chrono::duration<long, std::ratio<1l, 1000l> > >) + 0x23d (0x7fff873fb2dd in /ccs/home/kiefera/.conda/envs/pytorch/lib/python3.11/site-packages/torch/lib/libtorch_hip.so)
frame #2: c10d::ProcessGroupNCCL::watchdogHandler() + 0x9e8 (0x7fff873fc8d8 in /ccs/home/kiefera/.conda/envs/pytorch/lib/python3.11/site-packages/torch/lib/libtorch_hip.so)
frame #3: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0x14d (0x7fff873fd57d in /ccs/home/kiefera/.conda/envs/pytorch/lib/python3.11/site-packages/torch/lib/libtorch_hip.so)
frame #4: <unknown function> + 0xe4793 (0x7fffeb42d793 in /usr/lib64/libstdc++.so.6)
frame #5: <unknown function> + 0xa761c (0x7fffed74f61c in /lib64/libc.so.6)
frame #6: <unknown function> + 0x12eaa8 (0x7fffed7d6aa8 in /lib64/libc.so.6)

Exception raised from ncclCommWatchdog at /pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1901 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >) + 0x98 (0x7fff2dbba968 in /ccs/home/kiefera/.conda/envs/pytorch/lib/python3.11/site-packages/torch/lib/libc10.so)
frame #1: <unknown function> + 0x22990fe (0x7fff873d10fe in /ccs/home/kiefera/.conda/envs/pytorch/lib/python3.11/site-packages/torch/lib/libtorch_hip.so)
frame #2: <unknown function> + 0x69cfc1 (0x7fff857d4fc1 in /ccs/home/kiefera/.conda/envs/pytorch/lib/python3.11/site-packages/torch/lib/libtorch_hip.so)
frame #3: <unknown function> + 0xe4793 (0x7fffeb42d793 in /usr/lib64/libstdc++.so.6)
frame #4: <unknown function> + 0xa761c (0x7fffed74f61c in /lib64/libc.so.6)
frame #5: <unknown function> + 0x12eaa8 (0x7fffed7d6aa8 in /lib64/libc.so.6)

[rank9]:[E228 09:09:20.416031910 ProcessGroupNCCL.cpp:629] [Rank 1] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=73, OpType=ALLREDUCE, NumelIn=24883200, NumelOut=24883200, Timeout(ms)=600000) ran for 600055 milliseconds before timing out.
[rank9]:[E228 09:09:20.416151562 ProcessGroupNCCL.cpp:2168] [PG ID 2 PG GUID 10 Rank 1]  failure detected by watchdog at work sequence id: 73 PG status: last enqueued work: 73, last completed work: 72
[rank9]:[E228 09:09:20.416159978 ProcessGroupNCCL.cpp:667] Stack trace of the failed collective not found, potentially because FlightRecorder is disabled. You can enable it by setting TORCH_NCCL_TRACE_BUFFER_SIZE to a non-zero value.
[rank9]:[E228 09:09:20.416165789 ProcessGroupNCCL.cpp:681] [Rank 1] Some NCCL operations have failed or timed out. Due to the asynchronous nature of CUDA kernels, subsequent GPU operations might run on corrupted/incomplete data.
[rank9]:[E228 09:09:20.416169847 ProcessGroupNCCL.cpp:695] [Rank 1] To avoid data inconsistency, we are taking the entire process down.
[rank9]:[E228 09:09:20.417642507 ProcessGroupNCCL.cpp:1895] [PG ID 2 PG GUID 10 Rank 1] Process group watchdog thread terminated with exception: [Rank 1] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=73, OpType=ALLREDUCE, NumelIn=24883200, NumelOut=24883200, Timeout(ms)=600000) ran for 600055 milliseconds before timing out.
Exception raised from checkTimeout at /pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:632 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >) + 0x98 (0x7fff2dbba968 in /ccs/home/kiefera/.conda/envs/pytorch/lib/python3.11/site-packages/torch/lib/libc10.so)
frame #1: c10d::ProcessGroupNCCL::WorkNCCL::checkTimeout(std::optional<std::chrono::duration<long, std::ratio<1l, 1000l> > >) + 0x23d (0x7fff873fb2dd in /ccs/home/kiefera/.conda/envs/pytorch/lib/python3.11/site-packages/torch/lib/libtorch_hip.so)
frame #2: c10d::ProcessGroupNCCL::watchdogHandler() + 0x9e8 (0x7fff873fc8d8 in /ccs/home/kiefera/.conda/envs/pytorch/lib/python3.11/site-packages/torch/lib/libtorch_hip.so)
frame #3: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0x14d (0x7fff873fd57d in /ccs/home/kiefera/.conda/envs/pytorch/lib/python3.11/site-packages/torch/lib/libtorch_hip.so)
frame #4: <unknown function> + 0xe4793 (0x7fffeb42d793 in /usr/lib64/libstdc++.so.6)
frame #5: <unknown function> + 0xa761c (0x7fffed74f61c in /lib64/libc.so.6)
frame #6: <unknown function> + 0x12eaa8 (0x7fffed7d6aa8 in /lib64/libc.so.6)

terminate called after throwing an instance of 'c10::DistBackendError'
  what():  [PG ID 2 PG GUID 10 Rank 1] Process group watchdog thread terminated with exception: [Rank 1] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=73, OpType=ALLREDUCE, NumelIn=24883200, NumelOut=24883200, Timeout(ms)=600000) ran for 600055 milliseconds before timing out.
Exception raised from checkTimeout at /pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:632 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >) + 0x98 (0x7fff2dbba968 in /ccs/home/kiefera/.conda/envs/pytorch/lib/python3.11/site-packages/torch/lib/libc10.so)
frame #1: c10d::ProcessGroupNCCL::WorkNCCL::checkTimeout(std::optional<std::chrono::duration<long, std::ratio<1l, 1000l> > >) + 0x23d (0x7fff873fb2dd in /ccs/home/kiefera/.conda/envs/pytorch/lib/python3.11/site-packages/torch/lib/libtorch_hip.so)
frame #2: c10d::ProcessGroupNCCL::watchdogHandler() + 0x9e8 (0x7fff873fc8d8 in /ccs/home/kiefera/.conda/envs/pytorch/lib/python3.11/site-packages/torch/lib/libtorch_hip.so)
frame #3: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0x14d (0x7fff873fd57d in /ccs/home/kiefera/.conda/envs/pytorch/lib/python3.11/site-packages/torch/lib/libtorch_hip.so)
frame #4: <unknown function> + 0xe4793 (0x7fffeb42d793 in /usr/lib64/libstdc++.so.6)
frame #5: <unknown function> + 0xa761c (0x7fffed74f61c in /lib64/libc.so.6)
frame #6: <unknown function> + 0x12eaa8 (0x7fffed7d6aa8 in /lib64/libc.so.6)

Exception raised from ncclCommWatchdog at /pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1901 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >) + 0x98 (0x7fff2dbba968 in /ccs/home/kiefera/.conda/envs/pytorch/lib/python3.11/site-packages/torch/lib/libc10.so)
frame #1: <unknown function> + 0x22990fe (0x7fff873d10fe in /ccs/home/kiefera/.conda/envs/pytorch/lib/python3.11/site-packages/torch/lib/libtorch_hip.so)
frame #2: <unknown function> + 0x69cfc1 (0x7fff857d4fc1 in /ccs/home/kiefera/.conda/envs/pytorch/lib/python3.11/site-packages/torch/lib/libtorch_hip.so)
frame #3: <unknown function> + 0xe4793 (0x7fffeb42d793 in /usr/lib64/libstdc++.so.6)
frame #4: <unknown function> + 0xa761c (0x7fffed74f61c in /lib64/libc.so.6)
frame #5: <unknown function> + 0x12eaa8 (0x7fffed7d6aa8 in /lib64/libc.so.6)

[rank11]:[E228 09:09:21.446273873 ProcessGroupNCCL.cpp:629] [Rank 3] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=73, OpType=ALLREDUCE, NumelIn=24883200, NumelOut=24883200, Timeout(ms)=600000) ran for 600085 milliseconds before timing out.
[rank11]:[E228 09:09:21.446407031 ProcessGroupNCCL.cpp:2168] [PG ID 2 PG GUID 10 Rank 3]  failure detected by watchdog at work sequence id: 73 PG status: last enqueued work: 73, last completed work: 72
[rank11]:[E228 09:09:21.446417632 ProcessGroupNCCL.cpp:667] Stack trace of the failed collective not found, potentially because FlightRecorder is disabled. You can enable it by setting TORCH_NCCL_TRACE_BUFFER_SIZE to a non-zero value.
[rank11]:[E228 09:09:21.446424805 ProcessGroupNCCL.cpp:681] [Rank 3] Some NCCL operations have failed or timed out. Due to the asynchronous nature of CUDA kernels, subsequent GPU operations might run on corrupted/incomplete data.
[rank11]:[E228 09:09:21.446429354 ProcessGroupNCCL.cpp:695] [Rank 3] To avoid data inconsistency, we are taking the entire process down.
[rank11]:[E228 09:09:21.447924177 ProcessGroupNCCL.cpp:1895] [PG ID 2 PG GUID 10 Rank 3] Process group watchdog thread terminated with exception: [Rank 3] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=73, OpType=ALLREDUCE, NumelIn=24883200, NumelOut=24883200, Timeout(ms)=600000) ran for 600085 milliseconds before timing out.
Exception raised from checkTimeout at /pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:632 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >) + 0x98 (0x7fff2dbba968 in /ccs/home/kiefera/.conda/envs/pytorch/lib/python3.11/site-packages/torch/lib/libc10.so)
frame #1: c10d::ProcessGroupNCCL::WorkNCCL::checkTimeout(std::optional<std::chrono::duration<long, std::ratio<1l, 1000l> > >) + 0x23d (0x7fff873fb2dd in /ccs/home/kiefera/.conda/envs/pytorch/lib/python3.11/site-packages/torch/lib/libtorch_hip.so)
frame #2: c10d::ProcessGroupNCCL::watchdogHandler() + 0x9e8 (0x7fff873fc8d8 in /ccs/home/kiefera/.conda/envs/pytorch/lib/python3.11/site-packages/torch/lib/libtorch_hip.so)
frame #3: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0x14d (0x7fff873fd57d in /ccs/home/kiefera/.conda/envs/pytorch/lib/python3.11/site-packages/torch/lib/libtorch_hip.so)
frame #4: <unknown function> + 0xe4793 (0x7fffeb42d793 in /usr/lib64/libstdc++.so.6)
frame #5: <unknown function> + 0xa761c (0x7fffed74f61c in /lib64/libc.so.6)
frame #6: <unknown function> + 0x12eaa8 (0x7fffed7d6aa8 in /lib64/libc.so.6)

terminate called after throwing an instance of 'c10::DistBackendError'
  what():  [PG ID 2 PG GUID 10 Rank 3] Process group watchdog thread terminated with exception: [Rank 3] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=73, OpType=ALLREDUCE, NumelIn=24883200, NumelOut=24883200, Timeout(ms)=600000) ran for 600085 milliseconds before timing out.
Exception raised from checkTimeout at /pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:632 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >) + 0x98 (0x7fff2dbba968 in /ccs/home/kiefera/.conda/envs/pytorch/lib/python3.11/site-packages/torch/lib/libc10.so)
frame #1: c10d::ProcessGroupNCCL::WorkNCCL::checkTimeout(std::optional<std::chrono::duration<long, std::ratio<1l, 1000l> > >) + 0x23d (0x7fff873fb2dd in /ccs/home/kiefera/.conda/envs/pytorch/lib/python3.11/site-packages/torch/lib/libtorch_hip.so)
frame #2: c10d::ProcessGroupNCCL::watchdogHandler() + 0x9e8 (0x7fff873fc8d8 in /ccs/home/kiefera/.conda/envs/pytorch/lib/python3.11/site-packages/torch/lib/libtorch_hip.so)
frame #3: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0x14d (0x7fff873fd57d in /ccs/home/kiefera/.conda/envs/pytorch/lib/python3.11/site-packages/torch/lib/libtorch_hip.so)
frame #4: <unknown function> + 0xe4793 (0x7fffeb42d793 in /usr/lib64/libstdc++.so.6)
frame #5: <unknown function> + 0xa761c (0x7fffed74f61c in /lib64/libc.so.6)
frame #6: <unknown function> + 0x12eaa8 (0x7fffed7d6aa8 in /lib64/libc.so.6)

Exception raised from ncclCommWatchdog at /pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1901 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >) + 0x98 (0x7fff2dbba968 in /ccs/home/kiefera/.conda/envs/pytorch/lib/python3.11/site-packages/torch/lib/libc10.so)
frame #1: <unknown function> + 0x22990fe (0x7fff873d10fe in /ccs/home/kiefera/.conda/envs/pytorch/lib/python3.11/site-packages/torch/lib/libtorch_hip.so)
frame #2: <unknown function> + 0x69cfc1 (0x7fff857d4fc1 in /ccs/home/kiefera/.conda/envs/pytorch/lib/python3.11/site-packages/torch/lib/libtorch_hip.so)
frame #3: <unknown function> + 0xe4793 (0x7fffeb42d793 in /usr/lib64/libstdc++.so.6)
frame #4: <unknown function> + 0xa761c (0x7fffed74f61c in /lib64/libc.so.6)
frame #5: <unknown function> + 0x12eaa8 (0x7fffed7d6aa8 in /lib64/libc.so.6)

[rank1]:[E228 09:09:22.386202728 ProcessGroupNCCL.cpp:629] [Rank 1] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=73, OpType=ALLREDUCE, NumelIn=24883200, NumelOut=24883200, Timeout(ms)=600000) ran for 600071 milliseconds before timing out.
[rank1]:[E228 09:09:22.386312961 ProcessGroupNCCL.cpp:2168] [PG ID 2 PG GUID 9 Rank 1]  failure detected by watchdog at work sequence id: 73 PG status: last enqueued work: 73, last completed work: 72
[rank1]:[E228 09:09:22.386319383 ProcessGroupNCCL.cpp:667] Stack trace of the failed collective not found, potentially because FlightRecorder is disabled. You can enable it by setting TORCH_NCCL_TRACE_BUFFER_SIZE to a non-zero value.
[rank1]:[E228 09:09:22.386323101 ProcessGroupNCCL.cpp:681] [Rank 1] Some NCCL operations have failed or timed out. Due to the asynchronous nature of CUDA kernels, subsequent GPU operations might run on corrupted/incomplete data.
[rank1]:[E228 09:09:22.386325645 ProcessGroupNCCL.cpp:695] [Rank 1] To avoid data inconsistency, we are taking the entire process down.
[rank1]:[E228 09:09:22.387170206 ProcessGroupNCCL.cpp:1895] [PG ID 2 PG GUID 9 Rank 1] Process group watchdog thread terminated with exception: [Rank 1] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=73, OpType=ALLREDUCE, NumelIn=24883200, NumelOut=24883200, Timeout(ms)=600000) ran for 600071 milliseconds before timing out.
Exception raised from checkTimeout at /pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:632 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >) + 0x98 (0x7fff2dbba968 in /ccs/home/kiefera/.conda/envs/pytorch/lib/python3.11/site-packages/torch/lib/libc10.so)
frame #1: c10d::ProcessGroupNCCL::WorkNCCL::checkTimeout(std::optional<std::chrono::duration<long, std::ratio<1l, 1000l> > >) + 0x23d (0x7fff873fb2dd in /ccs/home/kiefera/.conda/envs/pytorch/lib/python3.11/site-packages/torch/lib/libtorch_hip.so)
frame #2: c10d::ProcessGroupNCCL::watchdogHandler() + 0x9e8 (0x7fff873fc8d8 in /ccs/home/kiefera/.conda/envs/pytorch/lib/python3.11/site-packages/torch/lib/libtorch_hip.so)
frame #3: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0x14d (0x7fff873fd57d in /ccs/home/kiefera/.conda/envs/pytorch/lib/python3.11/site-packages/torch/lib/libtorch_hip.so)
frame #4: <unknown function> + 0xe4793 (0x7fffeb42d793 in /usr/lib64/libstdc++.so.6)
frame #5: <unknown function> + 0xa761c (0x7fffed74f61c in /lib64/libc.so.6)
frame #6: <unknown function> + 0x12eaa8 (0x7fffed7d6aa8 in /lib64/libc.so.6)

terminate called after throwing an instance of 'c10::DistBackendError'
  what():  [PG ID 2 PG GUID 9 Rank 1] Process group watchdog thread terminated with exception: [Rank 1] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=73, OpType=ALLREDUCE, NumelIn=24883200, NumelOut=24883200, Timeout(ms)=600000) ran for 600071 milliseconds before timing out.
Exception raised from checkTimeout at /pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:632 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >) + 0x98 (0x7fff2dbba968 in /ccs/home/kiefera/.conda/envs/pytorch/lib/python3.11/site-packages/torch/lib/libc10.so)
frame #1: c10d::ProcessGroupNCCL::WorkNCCL::checkTimeout(std::optional<std::chrono::duration<long, std::ratio<1l, 1000l> > >) + 0x23d (0x7fff873fb2dd in /ccs/home/kiefera/.conda/envs/pytorch/lib/python3.11/site-packages/torch/lib/libtorch_hip.so)
frame #2: c10d::ProcessGroupNCCL::watchdogHandler() + 0x9e8 (0x7fff873fc8d8 in /ccs/home/kiefera/.conda/envs/pytorch/lib/python3.11/site-packages/torch/lib/libtorch_hip.so)
frame #3: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0x14d (0x7fff873fd57d in /ccs/home/kiefera/.conda/envs/pytorch/lib/python3.11/site-packages/torch/lib/libtorch_hip.so)
frame #4: <unknown function> + 0xe4793 (0x7fffeb42d793 in /usr/lib64/libstdc++.so.6)
frame #5: <unknown function> + 0xa761c (0x7fffed74f61c in /lib64/libc.so.6)
frame #6: <unknown function> + 0x12eaa8 (0x7fffed7d6aa8 in /lib64/libc.so.6)

Exception raised from ncclCommWatchdog at /pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1901 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >) + 0x98 (0x7fff2dbba968 in /ccs/home/kiefera/.conda/envs/pytorch/lib/python3.11/site-packages/torch/lib/libc10.so)
frame #1: <unknown function> + 0x22990fe (0x7fff873d10fe in /ccs/home/kiefera/.conda/envs/pytorch/lib/python3.11/site-packages/torch/lib/libtorch_hip.so)
frame #2: <unknown function> + 0x69cfc1 (0x7fff857d4fc1 in /ccs/home/kiefera/.conda/envs/pytorch/lib/python3.11/site-packages/torch/lib/libtorch_hip.so)
frame #3: <unknown function> + 0xe4793 (0x7fffeb42d793 in /usr/lib64/libstdc++.so.6)
frame #4: <unknown function> + 0xa761c (0x7fffed74f61c in /lib64/libc.so.6)
frame #5: <unknown function> + 0x12eaa8 (0x7fffed7d6aa8 in /lib64/libc.so.6)

[rank7]:[E228 09:09:22.395094255 ProcessGroupNCCL.cpp:629] [Rank 7] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=73, OpType=ALLREDUCE, NumelIn=24883200, NumelOut=24883200, Timeout(ms)=600000) ran for 600080 milliseconds before timing out.
[rank7]:[E228 09:09:22.395204468 ProcessGroupNCCL.cpp:2168] [PG ID 2 PG GUID 9 Rank 7]  failure detected by watchdog at work sequence id: 73 PG status: last enqueued work: 73, last completed work: 72
[rank7]:[E228 09:09:22.395212764 ProcessGroupNCCL.cpp:667] Stack trace of the failed collective not found, potentially because FlightRecorder is disabled. You can enable it by setting TORCH_NCCL_TRACE_BUFFER_SIZE to a non-zero value.
[rank7]:[E228 09:09:22.395216982 ProcessGroupNCCL.cpp:681] [Rank 7] Some NCCL operations have failed or timed out. Due to the asynchronous nature of CUDA kernels, subsequent GPU operations might run on corrupted/incomplete data.
[rank7]:[E228 09:09:22.395220579 ProcessGroupNCCL.cpp:695] [Rank 7] To avoid data inconsistency, we are taking the entire process down.
[rank7]:[E228 09:09:22.396074769 ProcessGroupNCCL.cpp:1895] [PG ID 2 PG GUID 9 Rank 7] Process group watchdog thread terminated with exception: [Rank 7] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=73, OpType=ALLREDUCE, NumelIn=24883200, NumelOut=24883200, Timeout(ms)=600000) ran for 600080 milliseconds before timing out.
Exception raised from checkTimeout at /pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:632 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >) + 0x98 (0x7fff2dbba968 in /ccs/home/kiefera/.conda/envs/pytorch/lib/python3.11/site-packages/torch/lib/libc10.so)
frame #1: c10d::ProcessGroupNCCL::WorkNCCL::checkTimeout(std::optional<std::chrono::duration<long, std::ratio<1l, 1000l> > >) + 0x23d (0x7fff873fb2dd in /ccs/home/kiefera/.conda/envs/pytorch/lib/python3.11/site-packages/torch/lib/libtorch_hip.so)
frame #2: c10d::ProcessGroupNCCL::watchdogHandler() + 0x9e8 (0x7fff873fc8d8 in /ccs/home/kiefera/.conda/envs/pytorch/lib/python3.11/site-packages/torch/lib/libtorch_hip.so)
frame #3: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0x14d (0x7fff873fd57d in /ccs/home/kiefera/.conda/envs/pytorch/lib/python3.11/site-packages/torch/lib/libtorch_hip.so)
frame #4: <unknown function> + 0xe4793 (0x7fffeb42d793 in /usr/lib64/libstdc++.so.6)
frame #5: <unknown function> + 0xa761c (0x7fffed74f61c in /lib64/libc.so.6)
frame #6: <unknown function> + 0x12eaa8 (0x7fffed7d6aa8 in /lib64/libc.so.6)

terminate called after throwing an instance of 'c10::DistBackendError'
  what():  [PG ID 2 PG GUID 9 Rank 7] Process group watchdog thread terminated with exception: [Rank 7] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=73, OpType=ALLREDUCE, NumelIn=24883200, NumelOut=24883200, Timeout(ms)=600000) ran for 600080 milliseconds before timing out.
Exception raised from checkTimeout at /pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:632 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >) + 0x98 (0x7fff2dbba968 in /ccs/home/kiefera/.conda/envs/pytorch/lib/python3.11/site-packages/torch/lib/libc10.so)
frame #1: c10d::ProcessGroupNCCL::WorkNCCL::checkTimeout(std::optional<std::chrono::duration<long, std::ratio<1l, 1000l> > >) + 0x23d (0x7fff873fb2dd in /ccs/home/kiefera/.conda/envs/pytorch/lib/python3.11/site-packages/torch/lib/libtorch_hip.so)
frame #2: c10d::ProcessGroupNCCL::watchdogHandler() + 0x9e8 (0x7fff873fc8d8 in /ccs/home/kiefera/.conda/envs/pytorch/lib/python3.11/site-packages/torch/lib/libtorch_hip.so)
frame #3: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0x14d (0x7fff873fd57d in /ccs/home/kiefera/.conda/envs/pytorch/lib/python3.11/site-packages/torch/lib/libtorch_hip.so)
frame #4: <unknown function> + 0xe4793 (0x7fffeb42d793 in /usr/lib64/libstdc++.so.6)
frame #5: <unknown function> + 0xa761c (0x7fffed74f61c in /lib64/libc.so.6)
frame #6: <unknown function> + 0x12eaa8 (0x7fffed7d6aa8 in /lib64/libc.so.6)

Exception raised from ncclCommWatchdog at /pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1901 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >) + 0x98 (0x7fff2dbba968 in /ccs/home/kiefera/.conda/envs/pytorch/lib/python3.11/site-packages/torch/lib/libc10.so)
frame #1: <unknown function> + 0x22990fe (0x7fff873d10fe in /ccs/home/kiefera/.conda/envs/pytorch/lib/python3.11/site-packages/torch/lib/libtorch_hip.so)
frame #2: <unknown function> + 0x69cfc1 (0x7fff857d4fc1 in /ccs/home/kiefera/.conda/envs/pytorch/lib/python3.11/site-packages/torch/lib/libtorch_hip.so)
frame #3: <unknown function> + 0xe4793 (0x7fffeb42d793 in /usr/lib64/libstdc++.so.6)
frame #4: <unknown function> + 0xa761c (0x7fffed74f61c in /lib64/libc.so.6)
frame #5: <unknown function> + 0x12eaa8 (0x7fffed7d6aa8 in /lib64/libc.so.6)

[rank3]:[E228 09:09:22.397569034 ProcessGroupNCCL.cpp:629] [Rank 3] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=73, OpType=ALLREDUCE, NumelIn=24883200, NumelOut=24883200, Timeout(ms)=600000) ran for 600083 milliseconds before timing out.
[rank3]:[E228 09:09:22.397681902 ProcessGroupNCCL.cpp:2168] [PG ID 2 PG GUID 9 Rank 3]  failure detected by watchdog at work sequence id: 73 PG status: last enqueued work: 73, last completed work: 72
[rank3]:[E228 09:09:22.397690629 ProcessGroupNCCL.cpp:667] Stack trace of the failed collective not found, potentially because FlightRecorder is disabled. You can enable it by setting TORCH_NCCL_TRACE_BUFFER_SIZE to a non-zero value.
[rank3]:[E228 09:09:22.397695217 ProcessGroupNCCL.cpp:681] [Rank 3] Some NCCL operations have failed or timed out. Due to the asynchronous nature of CUDA kernels, subsequent GPU operations might run on corrupted/incomplete data.
[rank3]:[E228 09:09:22.397698073 ProcessGroupNCCL.cpp:695] [Rank 3] To avoid data inconsistency, we are taking the entire process down.
[rank3]:[E228 09:09:22.398548755 ProcessGroupNCCL.cpp:1895] [PG ID 2 PG GUID 9 Rank 3] Process group watchdog thread terminated with exception: [Rank 3] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=73, OpType=ALLREDUCE, NumelIn=24883200, NumelOut=24883200, Timeout(ms)=600000) ran for 600083 milliseconds before timing out.
Exception raised from checkTimeout at /pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:632 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >) + 0x98 (0x7fff2dbba968 in /ccs/home/kiefera/.conda/envs/pytorch/lib/python3.11/site-packages/torch/lib/libc10.so)
frame #1: c10d::ProcessGroupNCCL::WorkNCCL::checkTimeout(std::optional<std::chrono::duration<long, std::ratio<1l, 1000l> > >) + 0x23d (0x7fff873fb2dd in /ccs/home/kiefera/.conda/envs/pytorch/lib/python3.11/site-packages/torch/lib/libtorch_hip.so)
frame #2: c10d::ProcessGroupNCCL::watchdogHandler() + 0x9e8 (0x7fff873fc8d8 in /ccs/home/kiefera/.conda/envs/pytorch/lib/python3.11/site-packages/torch/lib/libtorch_hip.so)
frame #3: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0x14d (0x7fff873fd57d in /ccs/home/kiefera/.conda/envs/pytorch/lib/python3.11/site-packages/torch/lib/libtorch_hip.so)
frame #4: <unknown function> + 0xe4793 (0x7fffeb42d793 in /usr/lib64/libstdc++.so.6)
frame #5: <unknown function> + 0xa761c (0x7fffed74f61c in /lib64/libc.so.6)
frame #6: <unknown function> + 0x12eaa8 (0x7fffed7d6aa8 in /lib64/libc.so.6)

terminate called after throwing an instance of 'c10::DistBackendError'
  what():  [PG ID 2 PG GUID 9 Rank 3] Process group watchdog thread terminated with exception: [Rank 3] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=73, OpType=ALLREDUCE, NumelIn=24883200, NumelOut=24883200, Timeout(ms)=600000) ran for 600083 milliseconds before timing out.
Exception raised from checkTimeout at /pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:632 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >) + 0x98 (0x7fff2dbba968 in /ccs/home/kiefera/.conda/envs/pytorch/lib/python3.11/site-packages/torch/lib/libc10.so)
frame #1: c10d::ProcessGroupNCCL::WorkNCCL::checkTimeout(std::optional<std::chrono::duration<long, std::ratio<1l, 1000l> > >) + 0x23d (0x7fff873fb2dd in /ccs/home/kiefera/.conda/envs/pytorch/lib/python3.11/site-packages/torch/lib/libtorch_hip.so)
frame #2: c10d::ProcessGroupNCCL::watchdogHandler() + 0x9e8 (0x7fff873fc8d8 in /ccs/home/kiefera/.conda/envs/pytorch/lib/python3.11/site-packages/torch/lib/libtorch_hip.so)
frame #3: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0x14d (0x7fff873fd57d in /ccs/home/kiefera/.conda/envs/pytorch/lib/python3.11/site-packages/torch/lib/libtorch_hip.so)
frame #4: <unknown function> + 0xe4793 (0x7fffeb42d793 in /usr/lib64/libstdc++.so.6)
frame #5: <unknown function> + 0xa761c (0x7fffed74f61c in /lib64/libc.so.6)
frame #6: <unknown function> + 0x12eaa8 (0x7fffed7d6aa8 in /lib64/libc.so.6)

Exception raised from ncclCommWatchdog at /pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1901 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >) + 0x98 (0x7fff2dbba968 in /ccs/home/kiefera/.conda/envs/pytorch/lib/python3.11/site-packages/torch/lib/libc10.so)
frame #1: <unknown function> + 0x22990fe (0x7fff873d10fe in /ccs/home/kiefera/.conda/envs/pytorch/lib/python3.11/site-packages/torch/lib/libtorch_hip.so)
frame #2: <unknown function> + 0x69cfc1 (0x7fff857d4fc1 in /ccs/home/kiefera/.conda/envs/pytorch/lib/python3.11/site-packages/torch/lib/libtorch_hip.so)
frame #3: <unknown function> + 0xe4793 (0x7fffeb42d793 in /usr/lib64/libstdc++.so.6)
frame #4: <unknown function> + 0xa761c (0x7fffed74f61c in /lib64/libc.so.6)
frame #5: <unknown function> + 0x12eaa8 (0x7fffed7d6aa8 in /lib64/libc.so.6)

srun: error: frontier00074: task 8: Aborted (core dumped)
srun: Terminating StepId=3109073.0
slurmstepd: error: *** STEP 3109073.0 ON frontier00073 CANCELLED AT 2025-02-28T09:10:12 ***
srun: error: frontier00074: task 13: Aborted (core dumped)
srun: error: frontier00074: task 12: Aborted (core dumped)
srun: error: frontier00074: task 10: Aborted (core dumped)
srun: error: frontier00074: task 14: Aborted (core dumped)
srun: error: frontier00074: task 9: Killed
srun: error: frontier00074: tasks 11,15: Killed
srun: error: frontier00073: tasks 0,5-6: Killed
srun: error: frontier00073: tasks 2,4: Killed
srun: error: frontier00073: tasks 1,7: Killed
srun: error: frontier00073: task 3: Killed
srun: Force Terminated StepId=3109073.0
