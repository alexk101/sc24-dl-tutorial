cat vit-era5-mp-35918163.out
+ srun -u shifter -V /pscratch/sd/s/shas1693/data/sc24_tutorial_data:/data -V /pscratch/sd/a/akiefer/sc24-dl-tutorial/logs:/logs bash -c '
    source export_DDP_vars.sh
     python train_mp_mod.py --config=mp --tensor_parallel=4 --scale_depth=12 --scale_heads=8 --scale_dim=384 --n_train=25 --local_batch_size=4 --amp_mode=none --patch_size=4 --parallel_order=tp-dp
    '
2025-02-16 21:49:44,191 - root - INFO - CUDA GPU support available
2025-02-16 21:49:44,353 - root - INFO - bfloat16 support: True
2025-02-16 21:49:48,424 - root - INFO - Setting DP = 4, TP = 4, CP = 1, PP = 1
2025-02-16 21:49:48,450 - root - INFO - ------------------ Configuration ------------------
2025-02-16 21:49:48,451 - root - INFO - Configuration file: /global/u2/a/akiefer/sc24-dl-tutorial/config/ViT.yaml
2025-02-16 21:49:48,451 - root - INFO - Configuration name: mp
2025-02-16 21:49:48,451 - root - INFO - num_iters 500000
2025-02-16 21:49:48,451 - root - INFO - global_batch_size 16
2025-02-16 21:49:48,451 - root - INFO - lr 0.001
2025-02-16 21:49:48,451 - root - INFO - num_data_workers 8
2025-02-16 21:49:48,451 - root - INFO - embed_dim 1024
2025-02-16 21:49:48,451 - root - INFO - data_loader_config dali
2025-02-16 21:49:48,451 - root - INFO - amp_mode none
2025-02-16 21:49:48,451 - root - INFO - enable_jit False
2025-02-16 21:49:48,451 - root - INFO - enable_fused False
2025-02-16 21:49:48,452 - root - INFO - depth 12
2025-02-16 21:49:48,452 - root - INFO - dropout 0.0
2025-02-16 21:49:48,452 - root - INFO - patch_size 8
2025-02-16 21:49:48,452 - root - INFO - num_heads 8
2025-02-16 21:49:48,452 - root - INFO - img_size [360, 720]
2025-02-16 21:49:48,452 - root - INFO - dt 1
2025-02-16 21:49:48,452 - root - INFO - expdir /logs
2025-02-16 21:49:48,452 - root - INFO - lr_schedule cosine
2025-02-16 21:49:48,452 - root - INFO - warmup 0
2025-02-16 21:49:48,452 - root - INFO - optimizer Adam
2025-02-16 21:49:48,452 - root - INFO - logging_freq 100
2025-02-16 21:49:48,452 - root - INFO - n_in_channels 20
2025-02-16 21:49:48,452 - root - INFO - n_out_channels 20
2025-02-16 21:49:48,452 - root - INFO - train_data_path /data/train
2025-02-16 21:49:48,452 - root - INFO - valid_data_path /data/valid
2025-02-16 21:49:48,452 - root - INFO - inf_data_path /data/test
2025-02-16 21:49:48,452 - root - INFO - time_means_path /data/stats/time_means.npy
2025-02-16 21:49:48,452 - root - INFO - global_means_path /data/stats/global_means.npy
2025-02-16 21:49:48,453 - root - INFO - global_stds_path /data/stats/global_stds.npy
2025-02-16 21:49:48,453 - root - INFO - limit_nsamples None
2025-02-16 21:49:48,453 - root - INFO - limit_nsamples_val None
2025-02-16 21:49:48,453 - root - INFO - wireup_info env
2025-02-16 21:49:48,453 - root - INFO - wireup_store tcp
2025-02-16 21:49:48,453 - root - INFO - amp_enabled False
2025-02-16 21:49:48,453 - root - INFO - amp_dtype torch.float32
2025-02-16 21:49:48,453 - root - INFO - tp 4
2025-02-16 21:49:48,453 - root - INFO - cp 1
2025-02-16 21:49:48,453 - root - INFO - order tp-dp
2025-02-16 21:49:48,453 - root - INFO - ---------------------------------------------------
2025-02-16 21:49:48,457 - root - INFO - Using AMP dtype: torch.float32
2025-02-16 21:49:48,457 - root - INFO - rank 0, begin data loader init
2025-02-16 21:49:53,075 - root - INFO - Getting files stats from /pscratch/sd/a/akiefer/temp_train/25
2025-02-16 21:49:53,076 - root - INFO - Getting file stats from /pscratch/sd/a/akiefer/temp_train/25/1990.h5
2025-02-16 21:49:53,076 - root - INFO - Image shape: 360 x 720
2025-02-16 21:49:53,076 - root - INFO - Dataset shape: (1460, 20, 360, 720)
2025-02-16 21:49:53,077 - root - INFO - Number of samples per year: 1460
2025-02-16 21:49:53,077 - root - INFO - Found data at path /pscratch/sd/a/akiefer/temp_train/25. Number of examples: 36500. Image Shape: 360 x 720 x 20
2025-02-16 21:49:53,077 - root - INFO - Using shards of size 9125 per rank
2025-02-16 21:49:53,405 - root - INFO - Getting files stats from /pscratch/sd/a/akiefer/temp_train/25
2025-02-16 21:49:53,406 - root - INFO - Getting file stats from /pscratch/sd/a/akiefer/temp_train/25/1990.h5
2025-02-16 21:49:53,406 - root - INFO - Image shape: 360 x 720
2025-02-16 21:49:53,406 - root - INFO - Dataset shape: (1460, 20, 360, 720)
2025-02-16 21:49:55,391 - root - INFO - CUDA GPU support available
2025-02-16 21:49:55,454 - root - INFO - bfloat16 support: True
2025-02-16 21:49:57,630 - root - INFO - CUDA GPU support available
2025-02-16 21:49:57,675 - root - INFO - bfloat16 support: True
2025-02-16 21:49:59,929 - root - INFO - CUDA GPU support available
2025-02-16 21:50:00,009 - root - INFO - bfloat16 support: True
2025-02-16 21:50:02,171 - root - INFO - CUDA GPU support available
2025-02-16 21:50:02,254 - root - INFO - bfloat16 support: True
2025-02-16 21:50:04,516 - root - INFO - CUDA GPU support available
2025-02-16 21:50:04,576 - root - INFO - bfloat16 support: True
2025-02-16 21:50:06,664 - root - INFO - CUDA GPU support available
2025-02-16 21:50:06,712 - root - INFO - bfloat16 support: True
2025-02-16 21:50:08,948 - root - INFO - CUDA GPU support available
2025-02-16 21:50:08,993 - root - INFO - bfloat16 support: True
2025-02-16 21:50:11,249 - root - INFO - CUDA GPU support available
2025-02-16 21:50:11,318 - root - INFO - bfloat16 support: True
2025-02-16 21:50:13,144 - root - INFO - Getting files stats from /pscratch/sd/a/akiefer/temp_val/25
2025-02-16 21:50:13,145 - root - INFO - Getting file stats from /pscratch/sd/a/akiefer/temp_val/25/2016.h5
2025-02-16 21:50:13,146 - root - INFO - Image shape: 360 x 720
2025-02-16 21:50:13,146 - root - INFO - Dataset shape: (1460, 20, 360, 720)
2025-02-16 21:50:13,147 - root - INFO - Number of samples per year: 1460
2025-02-16 21:50:13,147 - root - INFO - Found data at path /pscratch/sd/a/akiefer/temp_val/25. Number of examples: 2920. Image Shape: 360 x 720 x 20
2025-02-16 21:50:13,147 - root - INFO - Using shards of size 730 per rank
2025-02-16 21:50:13,234 - root - INFO - Getting files stats from /pscratch/sd/a/akiefer/temp_val/25
2025-02-16 21:50:13,234 - root - INFO - Getting file stats from /pscratch/sd/a/akiefer/temp_val/25/2016.h5
2025-02-16 21:50:13,235 - root - INFO - Image shape: 360 x 720
2025-02-16 21:50:13,235 - root - INFO - Dataset shape: (1460, 20, 360, 720)
2025-02-16 21:50:15,038 - root - INFO - CUDA GPU support available
2025-02-16 21:50:15,137 - root - INFO - bfloat16 support: True
2025-02-16 21:50:17,577 - root - INFO - CUDA GPU support available
2025-02-16 21:50:17,675 - root - INFO - bfloat16 support: True
2025-02-16 21:50:20,085 - root - INFO - CUDA GPU support available
2025-02-16 21:50:20,134 - root - INFO - bfloat16 support: True
2025-02-16 21:50:22,311 - root - INFO - CUDA GPU support available
2025-02-16 21:50:22,438 - root - INFO - bfloat16 support: True
2025-02-16 21:50:24,928 - root - INFO - CUDA GPU support available
2025-02-16 21:50:25,131 - root - INFO - bfloat16 support: True
2025-02-16 21:50:27,484 - root - INFO - CUDA GPU support available
2025-02-16 21:50:27,540 - root - INFO - bfloat16 support: True
2025-02-16 21:50:29,768 - root - INFO - CUDA GPU support available
2025-02-16 21:50:29,847 - root - INFO - bfloat16 support: True
2025-02-16 21:50:32,039 - root - INFO - CUDA GPU support available
2025-02-16 21:50:32,113 - root - INFO - bfloat16 support: True
2025-02-16 21:50:33,400 - root - INFO - rank 0, data loader initialized
2025-02-16 21:50:33,431 - root - INFO - Rank 0: Using NVIDIA A100-SXM4-40GB, Total GPU memory: 40.00 GB
2025-02-16 21:50:35,453 - root - INFO - Sanity check: Sum of ranks across nodes: 120 (Expected: 120)
2025-02-16 21:50:39,239 - root - INFO - DistributedDataParallel(
  (module): VisionTransformer(
    (patch_embed): PatchEmbed(
      (proj): Conv2d(20, 384, kernel_size=(4, 4), stride=(4, 4))
    )
    (pos_drop): Dropout(p=0.0, inplace=False)
    (blocks): ModuleList(
      (0-11): 12 x Block(
        (drop_path): Identity()
        (attn): DistributedAttention(
          (q): DistributedMatmul()
          (k): DistributedMatmul()
          (v): DistributedMatmul()
          (attn_drop): Dropout(p=0.0, inplace=False)
          (proj): DistributedMatmul()
          (proj_drop): Dropout(p=0.0, inplace=False)
        )
        (mlp): DistributedMLP(
          (fc1): DistributedMatmul()
          (fc2): DistributedMatmul()
          (act): GELU(approximate='none')
          (drop): Dropout(p=0.0, inplace=False)
        )
        (norm1): DistributedLayerNorm(
          (norm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
        )
        (norm2): DistributedLayerNorm(
          (norm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
        )
      )
    )
    (norm): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
    (head): Linear(in_features=384, out_features=320, bias=False)
  )
)
2025-02-16 21:50:39,241 - root - INFO - Scaffolding memory high watermark: 5.4158935546875 GB.
2025-02-16 21:50:39,243 - root - INFO - Starting Training Loop...
2025-02-16 21:51:50,156 - root - INFO - Will check remaining time every 100 iterations
The module hierarchy tracking seems to be messed up.Please file a bug to PyTorch.
The module hierarchy tracking seems to be messed up.Please file a bug to PyTorch.
The module hierarchy tracking seems to be messed up.Please file a bug to PyTorch.
The module hierarchy tracking seems to be messed up.Please file a bug to PyTorch.
The module hierarchy tracking seems to be messed up.Please file a bug to PyTorch.
The module hierarchy tracking seems to be messed up.Please file a bug to PyTorch.
The module hierarchy tracking seems to be messed up.Please file a bug to PyTorch.
The module hierarchy tracking seems to be messed up.Please file a bug to PyTorch.
The module hierarchy tracking seems to be messed up.Please file a bug to PyTorch.
The module hierarchy tracking seems to be messed up.Please file a bug to PyTorch.
The module hierarchy tracking seems to be messed up.Please file a bug to PyTorch.
The module hierarchy tracking seems to be messed up.Please file a bug to PyTorch.
The module hierarchy tracking seems to be messed up.Please file a bug to PyTorch.
The module hierarchy tracking seems to be messed up.Please file a bug to PyTorch.
The module hierarchy tracking seems to be messed up.Please file a bug to PyTorch.
The module hierarchy tracking seems to be messed up.Please file a bug to PyTorch.
Module                                                        FLOP    % Total
---------------------------------------------------------  -------  ---------
Block                                                       6.648T     34.85%
 - aten.mm                                                  0.803T      4.21%
 - aten._scaled_dot_product_efficient_attention             4.837T     25.36%
 - aten._scaled_dot_product_efficient_attention_backward    1.008T      5.28%
 Block.attn                                                17.618T     92.37%
  - aten.mm                                                 0.688T      3.61%
  - aten._scaled_dot_product_efficient_attention            4.837T     25.36%
  - aten._scaled_dot_product_efficient_attention_backward  12.093T     63.40%
 Block.drop_path                                           13.485T     70.70%
  - aten.mm                                                 1.376T      7.21%
  - aten._scaled_dot_product_efficient_attention_backward  12.093T     63.40%
  - aten.convolution_backward                               0.016T      0.08%
 Block.mlp                                                  1.376T      7.21%
  - aten.mm                                                 1.376T      7.21%
DistributedDataParallel                                    19.074T    100.00%
 - aten.convolution                                         0.016T      0.08%
 - aten.mm                                                  2.112T     11.07%
 - aten._scaled_dot_product_efficient_attention             4.837T     25.36%
 - aten._scaled_dot_product_efficient_attention_backward   12.093T     63.40%
 - aten.convolution_backward                                0.016T      0.08%
 DistributedDataParallel.module                            19.074T    100.00%
  - aten.convolution                                        0.016T      0.08%
  - aten.mm                                                 2.112T     11.07%
  - aten._scaled_dot_product_efficient_attention            4.837T     25.36%
  - aten._scaled_dot_product_efficient_attention_backward  12.093T     63.40%
  - aten.convolution_backward                               0.016T      0.08%
Module                                                        FLOP    % Total
---------------------------------------------------------  -------  ---------
Block                                                       6.648T     34.85%
 - aten.mm                                                  0.803T      4.21%
 - aten._scaled_dot_product_efficient_attention             4.837T     25.36%
 - aten._scaled_dot_product_efficient_attention_backward    1.008T      5.28%
 Block.attn                                                17.618T     92.37%
  - aten.mm                                                 0.688T      3.61%
  - aten._scaled_dot_product_efficient_attention            4.837T     25.36%
  - aten._scaled_dot_product_efficient_attention_backward  12.093T     63.40%
 Block.drop_path                                           13.485T     70.70%
  - aten.mm                                                 1.376T      7.21%
  - aten._scaled_dot_product_efficient_attention_backward  12.093T     63.40%
  - aten.convolution_backward                               0.016T      0.08%
 Block.mlp                                                  1.376T      7.21%
  - aten.mm                                                 1.376T      7.21%
DistributedDataParallel                                    19.074T    100.00%
 - aten.convolution                                         0.016T      0.08%
 - aten.mm                                                  2.112T     11.07%
 - aten._scaled_dot_product_efficient_attention             4.837T     25.36%
 - aten._scaled_dot_product_efficient_attention_backward   12.093T     63.40%
 - aten.convolution_backward                                0.016T      0.08%
 DistributedDataParallel.module                            19.074T    100.00%
  - aten.convolution                                        0.016T      0.08%
  - aten.mm                                                 2.112T     11.07%
  - aten._scaled_dot_product_efficient_attention            4.837T     25.36%
  - aten._scaled_dot_product_efficient_attention_backward  12.093T     63.40%
  - aten.convolution_backward                               0.016T      0.08%
Module                                                        FLOP    % Total
---------------------------------------------------------  -------  ---------
Block                                                       6.648T     34.85%
 - aten.mm                                                  0.803T      4.21%
 - aten._scaled_dot_product_efficient_attention             4.837T     25.36%
 - aten._scaled_dot_product_efficient_attention_backward    1.008T      5.28%
 Block.attn                                                17.618T     92.37%
  - aten.mm                                                 0.688T      3.61%
  - aten._scaled_dot_product_efficient_attention            4.837T     25.36%
  - aten._scaled_dot_product_efficient_attention_backward  12.093T     63.40%
 Block.drop_path                                           13.485T     70.70%
  - aten.mm                                                 1.376T      7.21%
  - aten._scaled_dot_product_efficient_attention_backward  12.093T     63.40%
  - aten.convolution_backward                               0.016T      0.08%
 Block.mlp                                                  1.376T      7.21%
  - aten.mm                                                 1.376T      7.21%
DistributedDataParallel                                    19.074T    100.00%
 - aten.convolution                                         0.016T      0.08%
 - aten.mm                                                  2.112T     11.07%
 - aten._scaled_dot_product_efficient_attention             4.837T     25.36%
 - aten._scaled_dot_product_efficient_attention_backward   12.093T     63.40%
 - aten.convolution_backward                                0.016T      0.08%
 DistributedDataParallel.module                            19.074T    100.00%
  - aten.convolution                                        0.016T      0.08%
  - aten.mm                                                 2.112T     11.07%
  - aten._scaled_dot_product_efficient_attention            4.837T     25.36%
  - aten._scaled_dot_product_efficient_attention_backward  12.093T     63.40%
  - aten.convolution_backward                               0.016T      0.08%
Module                                                        FLOP    % Total
---------------------------------------------------------  -------  ---------
Block                                                       6.648T     34.85%
 - aten.mm                                                  0.803T      4.21%
 - aten._scaled_dot_product_efficient_attention             4.837T     25.36%
 - aten._scaled_dot_product_efficient_attention_backward    1.008T      5.28%
 Block.attn                                                17.618T     92.37%
  - aten.mm                                                 0.688T      3.61%
  - aten._scaled_dot_product_efficient_attention            4.837T     25.36%
  - aten._scaled_dot_product_efficient_attention_backward  12.093T     63.40%
 Block.drop_path                                           13.485T     70.70%
  - aten.mm                                                 1.376T      7.21%
  - aten._scaled_dot_product_efficient_attention_backward  12.093T     63.40%
  - aten.cModule                                                        FLOP    % Total
---------------------------------------------------------  -------  ---------
Block                                                       6.648T     34.85%
 - aten.mm                                                  0.803T      4.21%
 - aten._scaled_dot_product_efficient_attention             4.837T     25.36%
 - aten._scaled_dot_product_efficient_attention_backward    1.008T      5.28%
 Block.attn                                                17.618T     92.37%
  - aten.mm                                                 0.688T      3.61%
  - aten._scaled_dot_product_efficient_attention            4.837T     25.36%
  - aten._scaled_dot_product_efficient_attention_backward  12.093T     63.40%
 Block.drop_path                                           13.485T     70.70%
  - aten.mm                                                 1.376T      7.21%
  - aten._scaled_dot_product_efficient_attention_backward  12.093T     63.40%
  - aten.convolution_backward                               0.016T      0.08%
 Block.mlp                                                  1.376T      7.21%
  - aten.mm                                                 1.376T      7.21%
DistributedDataParallel                                    19.074T    100.00%
 - aten.convolution                                         0.016T      0.08%
 - aten.mm                                                  2.112T     11.07%
 - aten._scaled_dot_product_efficient_attention             4.837T     25.36%
 - aten._scaled_dot_product_efficient_attention_backward   12.093T     63.40%
 - aten.convolution_backward                                0.016T      0.08%
 DistributedDataParallel.module                            19.074T    100.00%
  - aten.convolution                                        0.016T      0.08%
  - aten.mm                                                 2.112T     11.07%
  - aten._scaled_dot_product_efficient_attention            4.837T     25.36%
  - aten._scaled_dotonvolution_backward                               0.016T      0.08%
 Block.mlp                                                  1.376T      7.21%
  - aten.mm                                                 1.376T      7.21%
DistributedDataParallel                                    19.074T    100.00%
 - aten.convolution                                         0.016T      0.08%
 - aten.mm                                                  2.112T     11.07%
 - aten._scaled_dot_product_efficient_attention             4.837T     25.36%
 - aten._scaled_dot_product_efficient_attention_backward   12.093T     63.40%
 - aten.convolution_backward                                0.016T      0.08%
 DistributedDataParallel.module                            19.074T    100.00%
  - aten.convolution                                        0.016T      0.08%
  - aten.mm                                                 2.112T     11.07%
  - aten._scaled_dot_product_efficient_attention            4.837T     25.36%
  - aten._scaled_dot_product_efficient_attention_backward  12.093T     63.40%
  - aten.convolution_backward                               0.016T      0.08%
2025-02-16 21:51:52,480 - root - INFO - FLOPs per training step: 19,074,067,660,800
_product_efficient_attention_backward  12.093T     63.40%
  - aten.convolution_backward                               0.016T      0.08%
Module                                                        FLOP    % Total
---------------------------------------------------------  -------  ---------
Block                                                       6.648T     34.85%
 - aten.mm                                                  0.803T      4.21%
 - aten._scaled_dot_product_efficient_attention             4.837T     25.36%
 - aten._scaled_dot_product_efficient_attention_backward    1.008T      5.28%
 Block.attn                                                17.618T     92.37%
  - aten.mm                                                 0.688T      3.61%
  - aten._scaled_dot_product_efficient_attention            4.837T     25.36%
  - aten._scaled_dot_product_efficient_attention_backward  12.093T     63.40%
 Block.drop_path                                           13.485T     70.70%
  - aten.mm                                                 1.376T      7.21%
  - aten._scaled_dot_product_efficient_attention_backward  12.093T     63.40%
  - aten.convolution_backward                               0.016T      0.08%
 Block.mlp                                                  1.376T      7.21%
  - aten.mm                                                 1.376T      7.21%
DistributedDataParallel                                    19.074T    100.00%
 - aten.convolution                                         0.016T      0.08%
 - aten.mm                                                  2.112T     11.07%
 - aten._scaled_dot_product_efficient_attention             4.837T     25.36%
 - aten._scaled_dot_product_efficient_attention_backward   12.093T     63.40%
 - aten.convolution_backward                                0.016T      0.08%
 DistributedDataParallel.module                            19.074T    100.00%
  - aten.convolution                                        0.016T      0.08%
  - aten.mm                                                 2.112T     11.07%
  - aten._scaled_dot_product_efficient_attention            4.837T     25.36%
  - aten._scaled_dot_product_efficient_attention_backward  12.093T     63.40%
  - aten.convolution_backward                               0.016T      0.08%
Module                                                        FLOP    % Total
---------------------------------------------------------  -------  ---------
Block                                                       6.648T     34.85%
 - aten.mm                                                  0.803T      4.21%
 - aten._scaled_dot_product_efficient_attention             4.837T     25.36%
 - aten._scaled_dot_product_efficient_attention_backward    1.008T      5.28%
 Block.attn                                                17.618T     92.37%
  - aten.mm                                                 0.688T      3.61%
  - aten._scaled_dot_product_efficient_attention            4.837T     25.36%
  - aten._scaled_dot_product_efficient_attention_backward  12.093T     63.40%
 Block.drop_path                                           13.485T     70.70%
  - aten.mm                                                 1.376T      7.21%
  - aten._scaled_dot_product_efficient_attention_backward  12.093T     63.40%
  - aten.convolution_backward                               0.016T      0.08%
 Block.mlp                                                  1.376T      7.21%
  - aten.mm                                                 1.376T      7.21%
DistributedDataParallel                                    19.074T    100.00%
 - aten.convolution                                         0.016T      0.08%
 - aten.mm                                                  2.112T     11.07%
 - aten._scaled_dot_product_efficient_attention             4.837T     25.36%
 - aten._scaled_dot_product_efficient_attention_backward   12.093T     63.40%
 - aten.convolution_backward                                0.016T      0.08%
 DistributedDataParallel.module                            19.074T    100.00%
  - aten.convolution                                        0.016T      0.08%
  - aten.mm                                                 2.112T     11.07%
  - aten._scaled_dot_product_efficient_attention            4.837T     25.36%
  - aten._scaled_dot_product_efficient_attention_backward  12.093T     63.40%
  - aten.convolution_backward                               0.016T      0.08%
Module                                                        FLOP    % Total
---------------------------------------------------------  -------  ---------
Block                                                       6.648T     34.85%
 - aten.mm                                                  0.803T      4.21%
 - aten._scaled_dot_product_efficient_attention             4.837T     25.36%
 - aten._scaled_dot_product_efficient_attention_backward    1.008T      5.28%
 Block.attn                                                17.618T     92.37%
  - aten.mm                                                 0.688T      3.61%
  - aten._scaled_dot_product_efficient_attention            4.837T     25.36%
  - aten._scaled_dot_product_efficient_attention_backward  12.093T     63.40%
 Block.drop_path                                           13.485T     70.70%
  - aten.mm                                                 1.376T      7.21%
  - aten._scaled_dot_product_efficient_attention_backward  12.093T     63.40%
  - aten.cModule                                                        FLOP    % Total
---------------------------------------------------------  -------  ---------
Block                                                       6.648T     34.85%
 - aten.mm                                                  0.803T      4.21%
 - aten._scaled_dot_product_efficient_attention             4.837T     25.36%
 - aten._scaled_dot_product_efficient_attention_backward    1.008T      5.28%
 Block.attn                                                17.618T     92.37%
  - aten.mm                                                 0.688T      3.61%
  - aten._scaled_dot_product_efficient_attention            4.837T     25.36%
  - aten._scaled_dot_product_efficient_attention_backward  12.093T     63.40%
 Block.drop_path                                           13.485T     70.70%
  - aten.mm                                                 1.376T      7.21%
  - aten._scaled_dot_product_efficient_attention_backward  12.093T     63.40%
  - aten.convolution_backward                               0.016T      0.08%
 Block.mlp                                                  1.376T      7.21%
  - aten.mm                                                 1.376T      7.21%
DistributedDataParallel                                    19.074T    100.00%
 - aten.convolution                                         0.016T      0.08%
 - aten.mm                                                  2.112T     11.07%
 - aten._scaled_dot_product_efficient_attention             4.837T     25.36%
 - aten._scaled_dot_product_efficient_attention_backward   12.093T     63.40%
 - aten.convolution_backward                                0.016T      0.08%
 DistributedDataParallel.module                            19.074T    100.00%
  - aten.convolution                                        0.016T      0.08%
  - aten.mm                                                 2.112T     11.07%
  - aten._scaled_dot_product_efficient_attention            4.837T     25.36%
  - aten._scaled_dotonvolution_backward                               0.016T      0.08%
 Block.mlp                                                  1.376T      7.21%
  - aten.mm                                                 1.376T      7.21%
DistributedDataParallel                                    19.074T    100.00%
 - aten.convolution                                         0.016T      0.08%
 - aten.mm                                                  2.112T     11.07%
 - aten._scaled_dot_product_efficient_attention             4.837T     25.36%
 - aten._scaled_dot_product_efficient_attention_backward   12.093T     63.40%
 - aten.convolution_backward                                0.016T      0.08%
 DistributedDataParallel.module                            19.074T    100.00%
  - aten.convolution                                        0.016T      0.08%
  - aten.mm                                                 2.112T     11.07%
  - aten._scaled_dot_product_efficient_attention            4.837T     25.36%
  - aten._scaled_dot_product_efficient_attention_backward  12.093T     63.40%
  - aten.convolution_backward                               0.016T      0.08%
_product_efficient_attention_backward  12.093T     63.40%
  - aten.convolution_backward                               0.016T      0.08%
Module                                                        FLOP    % Total
---------------------------------------------------------  -------  ---------
Block                                                       6.648T     34.85%
 - aten.mm                                                  0.803T      4.21%
 - aten._scaled_dot_product_efficient_attention             4.837T     25.36%
 - aten._scaled_dot_product_efficient_attention_backward    1.008T      5.28%
 Block.attn                                                17.618T     92.37%
  - aten.mm                                                 0.688T      3.61%
  - aten._scaled_dot_product_efficient_attention            4.837T     25.36%
  - aten._scaled_dot_product_efficient_attention_backward  12.093T     63.40%
 Block.drop_path                                           13.485T     70.70%
  - aten.mm                                                 1.376T      7.21%
  - aten._scaled_dot_product_efficient_attention_backward  12.093T     63.40%
  - aten.convolution_backward                               0.016T      0.08%
 Block.mlp                                                  1.376T      7.21%
  - aten.mm                                                 1.376T      7.21%
DistributedDataParallel                                    19.074T    100.00%
 - aten.convolution                                         0.016T      0.08%
 - aten.mm                                                  2.112T     11.07%
 - aten._scaled_dot_product_efficient_attention             4.837T     25.36%
 - aten._scaled_dot_product_efficient_attention_backward   12.093T     63.40%
 - aten.convolution_backward                                0.016T      0.08%
 DistributedDataParallel.module                            19.074T    100.00%
  - aten.convolution                                        0.016T      0.08%
  - aten.mm                                                 2.112T     11.07%
  - aten._scaled_dot_product_efficient_attention            4.837T     25.36%
  - aten._scaled_dot_product_efficient_attention_backward  12.093T     63.40%
  - aten.convolution_backward                               0.016T      0.08%
Module                                                        FLOP    % Total
---------------------------------------------------------  -------  ---------
Block                                                       6.648T     34.85%
 - aten.mm                                                  0.803T      4.21%
 - aten._scaled_dot_product_efficient_attention             4.837T     25.36%
 - aten._scaled_dot_product_efficient_attention_backward    1.008T      5.28%
 Block.attn                                                17.618T     92.37%
  - aten.mm                                                 0.688T      3.61%
  - aten._scaled_dot_product_efficient_attention            4.837T     25.36%
  - aten._scaled_dot_product_efficient_attention_backward  12.093T     63.40%
 Block.drop_path                                           13.485T     70.70%
  - aten.mm                                                 1.376T      7.21%
  - aten._scaled_dot_product_efficient_attention_backward  12.093T     63.40%
  - aten.convolution_backward                               0.016T      0.08%
 Block.mlp                                                  1.376T      7.21%
  - aten.mm                                                 1.376T      7.21%
DistributedDataParallel                                    19.074T    100.00%
 - aten.convolution                                         0.016T      0.08%
 - aten.mm                                                  2.112T     11.07%
 - aten._scaled_dot_product_efficient_attention             4.837T     25.36%
 - aten._scaled_dot_product_efficient_attention_backward   12.093T     63.40%
 - aten.convolution_backward                                0.016T      0.08%
 DistributedDataParallel.module                            19.074T    100.00%
  - aten.convolution                                        0.016T      0.08%
  - aten.mm                                                 2.112T     11.07%
  - aten._scaled_dot_product_efficient_attention            4.837T     25.36%
  - aten._scaled_dot_product_efficient_attention_backward  12.093T     63.40%
  - aten.convolution_backward                               0.016T      0.08%
Module                                                        FLOP    % Total
---------------------------------------------------------  -------  ---------
Block                                                       6.648T     34.85%
 - aten.mm                                                  0.803T      4.21%
 - aten._scaled_dot_product_efficient_attention             4.837T     25.36%
 - aten._scaled_dot_product_efficient_attention_backward    1.008T      5.28%
 Block.attn                                                17.618T     92.37%
  - aten.mm                                                 0.688T      3.61%
  - aten._scaled_dot_product_efficient_attention            4.837T     25.36%
  - aten._scaled_dot_product_efficient_attention_backward  12.093T     63.40%
 Block.drop_path                                           13.485T     70.70%
  - aten.mm                                                 1.376T      7.21%
  - aten._scaled_dot_product_efficient_attention_backward  12.093T     63.40%
  - aten.convolution_backward                               0.016T      0.08%
 Block.mlp                                                  1.376T      7.21%
  - aten.mm                                                 1.376T      7.21%
DistributedDataParallel                                    19.074T    100.00%
 - aten.convolution                                         0.016T      0.08%
 - aten.mm                                                  2.112T     11.07%
 - aten._scaled_dot_product_efficient_attention             4.837T     25.36%
 - aten._scaled_dot_product_efficient_attention_backward   12.093T     63.40%
 - aten.convolution_backward                                0.016T      0.08%
 DistributedDataParallel.module                            19.074T    100.00%
  - aten.convolution                                        0.016T      0.08%
  - aten.mm                                                 2.112T     11.07%
  - aten._scaled_dot_product_efficient_attention            4.837T     25.36%
  - aten._scaled_dot_product_efficient_attention_backward  12.093T     63.40%
  - aten.convolution_backward                               0.016T      0.08%
Module                                                        FLOP    % Total
---------------------------------------------------------  -------  ---------
Block                                                       6.648T     34.85%
 - aten.mm                                                  0.803T      4.21%
 - aten._scaled_dot_product_efficient_attention             4.837T     25.36%
 - aten._scaled_dot_product_efficient_attention_backward    1.008T      5.28%
 Block.attn                                                17.618T     92.37%
  - aten.mm                                                 0.688T      3.61%
  - aten._scaled_dot_product_efficient_attention            4.837T     25.36%
  - aten._scaled_dot_product_efficient_attention_backward  12.093T     63.40%
 Block.drop_path                                           13.485T     70.70%
  - aten.mm                                                 1.376T      7.21%
  - aten._scaled_dot_product_efficient_attention_backward  12.093T     63.40%
  - aten.convolution_backward                               0.016T      0.08%
 Block.mlp                                                  1.376T      7.21%
  - aten.mm                                                 1.376T      7.21%
DistributedDataParallel                                    19.074T    100.00%
 - aten.convolution                                         0.016T      0.08%
 - aten.mm                                                  2.112T     11.07%
 - aten._scaled_dot_product_efficient_attention             4.837T     25.36%
 - aten._scaled_dot_product_efficient_attention_backward   12.093T     63.40%
 - aten.convolution_backward                                0.016T      0.08%
 DistributedDataParallel.module                            19.074T    100.00%
  - aten.convolution                                        0.016T      0.08%
  - aten.mm                                                 2.112T     11.07%
  - aten._scaled_dot_product_efficient_attention            4.837T     25.36%
  - aten._scaled_dot_product_efficient_attention_backward  12.093T     63.40%
  - aten.convolution_backward                               0.016T      0.08%
Module                                                        FLOP    % Total
---------------------------------------------------------  -------  ---------
Block                                                       6.648T     34.85%
 - aten.mm                                                  0.803T      4.21%
 - aten._scaled_dot_product_efficient_attention             4.837T     25.36%
 - aten._scaled_dot_product_efficient_attention_backward    1.008T      5.28%
 Block.attn                                                17.618T     92.37%
  - aten.mm                                                 0.688T      3.61%
  - aten._scaled_dot_product_efficient_attention            4.837T     25.36%
  - aten._scaled_dot_product_efficient_attention_backward  12.093T     63.40%
 Block.drop_path                                           13.485T     70.70%
  - aten.mm                                                 1.376T      7.21%
  - aten._scaled_dot_product_efficient_attention_backward  12.093T     63.40%
  - aten.c2025-02-16 21:51:52,534 - root - INFO - Parameter module.pos_embed sizes across ranks: [6220800, 6220800, 6220800, 6220800, 6220800, 6220800, 6220800, 6220800, 6220800, 6220800, 6220800, 6220800, 6220800, 6220800, 6220800, 6220800]
onvolution_backward                               0.016T      0.08%
 Block.mlp                                                  1.376T      7.21%
  - aten.mm                                                 1.376T      7.21%
DistributedDataParallel                                    19.074T    100.00%
 - aten.convolution                                         0.016T      0.08%
 - aten.mm                                                  2.112T     11.07%
 - aten._scaled_dot_product_efficient_attention             4.837T     25.36%
 - aten._scaled_dot_product_efficient_attention_backward   12.093T     63.40%
 - aten.convolution_backward                                0.016T      0.08%
 DistributedDataParallel.module                            19.074T    100.00%
  - aten.convolution                                        0.016T      0.08%
  - aten.mm                                                 2.112T     11.07%
  - aten._scaled_dot_product_efficient_attention            4.837T     25.36%
  - aten._scaled_dot2025-02-16 21:51:52,535 - root - INFO - Parameter module.patch_embed.proj.weight sizes across ranks: [122880, 122880, 122880, 122880, 122880, 122880, 122880, 122880, 122880, 122880, 122880, 122880, 122880, 122880, 122880, 122880]
_product_efficient_attention_backward  12.093T     63.40%
  - aten.convolution_backward                               0.016T      0.08%
2025-02-16 21:51:52,536 - root - INFO - Parameter module.patch_embed.proj.bias sizes across ranks: [384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384]
Module                                                        FLOP    % Total
---------------------------------------------------------  -------  ---------
Block                                                       6.648T     34.85%
 - aten.mm                                                  0.803T      4.21%
 - aten._scaled_dot_product_efficient_attention             4.837T     25.36%
 - aten._scaled_dot_product_efficient_attention_backward    1.008T      5.28%
 Block.attn                                                17.618T     92.37%
  - aten.mm                                                 0.688T      3.61%
  - aten._scaled_dot_product_efficient_attention            4.837T     25.36%
  - aten._scaled_dot_product_efficient_attention_backward  12.093T     63.40%
 Block.drop_path                                           13.485T     70.70%
  - aten.mm                                                 1.376T      7.21%
  - aten._scaled_dot_product_efficient_attention_backward  12.093T     63.40%
  - aten.c2025-02-16 21:51:52,536 - root - INFO - Parameter module.blocks.0.attn.q.weight sizes across ranks: [36864, 36864, 36864, 36864, 36864, 36864, 36864, 36864, 36864, 36864, 36864, 36864, 36864, 36864, 36864, 36864]
onvolution_backward                               0.016T      0.08%
 Block.mlp                                                  1.376T      7.21%
  - aten.mm                                                 1.376T      7.21%
DistributedDataParallel                                    19.074T    100.00%
 - aten.convolution                                         0.016T      0.08%
 - aten.mm                                                  2.112T     11.07%
 - aten._scaled_dot_product_efficient_attention             4.837T     25.36%
 - aten._scaled_dot_product_efficient_attention_backward   12.093T     63.40%
 - aten.convolution_backward                                0.016T      0.08%
 DistributedDataParallel.module                            19.074T    100.00%
  - aten.convolution                                        0.016T      0.08%
  - aten.mm                                                 2.112T     11.07%
  - aten._scaled_dot_product_efficient_attention            4.837T     25.36%
  - aten._scaled_dot2025-02-16 21:51:52,537 - root - INFO - Parameter module.blocks.0.attn.q.bias sizes across ranks: [96, 96, 96, 96, 96, 96, 96, 96, 96, 96, 96, 96, 96, 96, 96, 96]
_product_efficient_attention_backward  12.093T     63.40%
  - aten.convolution_backward                               0.016T      0.08%
2025-02-16 21:51:52,538 - root - INFO - Parameter module.blocks.0.attn.k.weight sizes across ranks: [36864, 36864, 36864, 36864, 36864, 36864, 36864, 36864, 36864, 36864, 36864, 36864, 36864, 36864, 36864, 36864]
Module                                                        FLOP    % Total
---------------------------------------------------------  -------  ---------
Block                                                       6.648T     34.85%
 - aten.mm                                                  0.803T      4.21%
 - aten._scaled_dot_product_efficient_attention             4.837T     25.36%
 - aten._scaled_dot_product_efficient_attention_backward    1.008T      5.28%
 Block.attn                                                17.618T     92.37%
  - aten.mm                                                 0.688T      3.61%
  - aten._scaled_dot_product_efficient_attention            4.837T     25.36%
  - aten._scaled_dot_product_efficient_attention_backward  12.093T     63.40%
 Block.drop_path                                           13.485T     70.70%
  - aten.mm                                                 1.376T      7.21%
  - aten._scaled_dot_product_efficient_attention_backward  12.093T     63.40%
  - aten.c2025-02-16 21:51:52,539 - root - INFO - Parameter module.blocks.0.attn.k.bias sizes across ranks: [96, 96, 96, 96, 96, 96, 96, 96, 96, 96, 96, 96, 96, 96, 96, 96]
onvolution_backward                               0.016T      0.08%
 Block.mlp                                                  1.376T      7.21%
  - aten.mm                                                 1.376T      7.21%
DistributedDataParallel                                    19.074T    100.00%
 - aten.convolution                                         0.016T      0.08%
 - aten.mm                                                  2.112T     11.07%
 - aten._scaled_dot_product_efficient_attention             4.837T     25.36%
 - aten._scaled_dot_product_efficient_attention_backward   12.093T     63.40%
 - aten.convolution_backward                                0.016T      0.08%
 DistributedDataParallel.module                            19.074T    100.00%
  - aten.convolution                                        0.016T      0.08%
  - aten.mm                                                 2.112T     11.07%
  - aten._scaled_dot_product_efficient_attention            4.837T     25.36%
  - aten._scaled_dot2025-02-16 21:51:52,539 - root - INFO - Parameter module.blocks.0.attn.v.weight sizes across ranks: [36864, 36864, 36864, 36864, 36864, 36864, 36864, 36864, 36864, 36864, 36864, 36864, 36864, 36864, 36864, 36864]
_product_efficient_attention_backward  12.093T     63.40%
  - aten.convolution_backward                               0.016T      0.08%
2025-02-16 21:51:52,540 - root - INFO - Parameter module.blocks.0.attn.v.bias sizes across ranks: [96, 96, 96, 96, 96, 96, 96, 96, 96, 96, 96, 96, 96, 96, 96, 96]
2025-02-16 21:51:52,541 - root - INFO - Parameter module.blocks.0.attn.proj.weight sizes across ranks: [36864, 36864, 36864, 36864, 36864, 36864, 36864, 36864, 36864, 36864, 36864, 36864, 36864, 36864, 36864, 36864]
2025-02-16 21:51:52,542 - root - INFO - Parameter module.blocks.0.attn.proj.bias sizes across ranks: [384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384]
2025-02-16 21:51:52,543 - root - INFO - Parameter module.blocks.0.mlp.fc1.weight sizes across ranks: [147456, 147456, 147456, 147456, 147456, 147456, 147456, 147456, 147456, 147456, 147456, 147456, 147456, 147456, 147456, 147456]
2025-02-16 21:51:52,543 - root - INFO - Parameter module.blocks.0.mlp.fc1.bias sizes across ranks: [384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384]
2025-02-16 21:51:52,544 - root - INFO - Parameter module.blocks.0.mlp.fc2.weight sizes across ranks: [147456, 147456, 147456, 147456, 147456, 147456, 147456, 147456, 147456, 147456, 147456, 147456, 147456, 147456, 147456, 147456]
2025-02-16 21:51:52,545 - root - INFO - Parameter module.blocks.0.mlp.fc2.bias sizes across ranks: [384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384]
2025-02-16 21:51:52,546 - root - INFO - Parameter module.blocks.0.norm1.norm.weight sizes across ranks: [384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384]
2025-02-16 21:51:52,546 - root - INFO - Parameter module.blocks.0.norm1.norm.bias sizes across ranks: [384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384]
2025-02-16 21:51:52,547 - root - INFO - Parameter module.blocks.0.norm2.norm.weight sizes across ranks: [384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384]
2025-02-16 21:51:52,548 - root - INFO - Parameter module.blocks.0.norm2.norm.bias sizes across ranks: [384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384]
2025-02-16 21:51:52,549 - root - INFO - Parameter module.blocks.1.attn.q.weight sizes across ranks: [36864, 36864, 36864, 36864, 36864, 36864, 36864, 36864, 36864, 36864, 36864, 36864, 36864, 36864, 36864, 36864]
2025-02-16 21:51:52,549 - root - INFO - Parameter module.blocks.1.attn.q.bias sizes across ranks: [96, 96, 96, 96, 96, 96, 96, 96, 96, 96, 96, 96, 96, 96, 96, 96]
2025-02-16 21:51:52,550 - root - INFO - Parameter module.blocks.1.attn.k.weight sizes across ranks: [36864, 36864, 36864, 36864, 36864, 36864, 36864, 36864, 36864, 36864, 36864, 36864, 36864, 36864, 36864, 36864]
2025-02-16 21:51:52,551 - root - INFO - Parameter module.blocks.1.attn.k.bias sizes across ranks: [96, 96, 96, 96, 96, 96, 96, 96, 96, 96, 96, 96, 96, 96, 96, 96]
2025-02-16 21:51:52,552 - root - INFO - Parameter module.blocks.1.attn.v.weight sizes across ranks: [36864, 36864, 36864, 36864, 36864, 36864, 36864, 36864, 36864, 36864, 36864, 36864, 36864, 36864, 36864, 36864]
2025-02-16 21:51:52,553 - root - INFO - Parameter module.blocks.1.attn.v.bias sizes across ranks: [96, 96, 96, 96, 96, 96, 96, 96, 96, 96, 96, 96, 96, 96, 96, 96]
2025-02-16 21:51:52,553 - root - INFO - Parameter module.blocks.1.attn.proj.weight sizes across ranks: [36864, 36864, 36864, 36864, 36864, 36864, 36864, 36864, 36864, 36864, 36864, 36864, 36864, 36864, 36864, 36864]
2025-02-16 21:51:52,554 - root - INFO - Parameter module.blocks.1.attn.proj.bias sizes across ranks: [384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384]
2025-02-16 21:51:52,555 - root - INFO - Parameter module.blocks.1.mlp.fc1.weight sizes across ranks: [147456, 147456, 147456, 147456, 147456, 147456, 147456, 147456, 147456, 147456, 147456, 147456, 147456, 147456, 147456, 147456]
2025-02-16 21:51:52,556 - root - INFO - Parameter module.blocks.1.mlp.fc1.bias sizes across ranks: [384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384]
2025-02-16 21:51:52,556 - root - INFO - Parameter module.blocks.1.mlp.fc2.weight sizes across ranks: [147456, 147456, 147456, 147456, 147456, 147456, 147456, 147456, 147456, 147456, 147456, 147456, 147456, 147456, 147456, 147456]
2025-02-16 21:51:52,557 - root - INFO - Parameter module.blocks.1.mlp.fc2.bias sizes across ranks: [384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384]
2025-02-16 21:51:52,558 - root - INFO - Parameter module.blocks.1.norm1.norm.weight sizes across ranks: [384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384]
2025-02-16 21:51:52,559 - root - INFO - Parameter module.blocks.1.norm1.norm.bias sizes across ranks: [384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384]
2025-02-16 21:51:52,559 - root - INFO - Parameter module.blocks.1.norm2.norm.weight sizes across ranks: [384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384]
2025-02-16 21:51:52,560 - root - INFO - Parameter module.blocks.1.norm2.norm.bias sizes across ranks: [384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384]
2025-02-16 21:51:52,561 - root - INFO - Parameter module.blocks.2.attn.q.weight sizes across ranks: [36864, 36864, 36864, 36864, 36864, 36864, 36864, 36864, 36864, 36864, 36864, 36864, 36864, 36864, 36864, 36864]
2025-02-16 21:51:52,562 - root - INFO - Parameter module.blocks.2.attn.q.bias sizes across ranks: [96, 96, 96, 96, 96, 96, 96, 96, 96, 96, 96, 96, 96, 96, 96, 96]
2025-02-16 21:51:52,562 - root - INFO - Parameter module.blocks.2.attn.k.weight sizes across ranks: [36864, 36864, 36864, 36864, 36864, 36864, 36864, 36864, 36864, 36864, 36864, 36864, 36864, 36864, 36864, 36864]
2025-02-16 21:51:52,563 - root - INFO - Parameter module.blocks.2.attn.k.bias sizes across ranks: [96, 96, 96, 96, 96, 96, 96, 96, 96, 96, 96, 96, 96, 96, 96, 96]
2025-02-16 21:51:52,564 - root - INFO - Parameter module.blocks.2.attn.v.weight sizes across ranks: [36864, 36864, 36864, 36864, 36864, 36864, 36864, 36864, 36864, 36864, 36864, 36864, 36864, 36864, 36864, 36864]
2025-02-16 21:51:52,565 - root - INFO - Parameter module.blocks.2.attn.v.bias sizes across ranks: [96, 96, 96, 96, 96, 96, 96, 96, 96, 96, 96, 96, 96, 96, 96, 96]
2025-02-16 21:51:52,566 - root - INFO - Parameter module.blocks.2.attn.proj.weight sizes across ranks: [36864, 36864, 36864, 36864, 36864, 36864, 36864, 36864, 36864, 36864, 36864, 36864, 36864, 36864, 36864, 36864]
2025-02-16 21:51:52,566 - root - INFO - Parameter module.blocks.2.attn.proj.bias sizes across ranks: [384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384]
2025-02-16 21:51:52,567 - root - INFO - Parameter module.blocks.2.mlp.fc1.weight sizes across ranks: [147456, 147456, 147456, 147456, 147456, 147456, 147456, 147456, 147456, 147456, 147456, 147456, 147456, 147456, 147456, 147456]
2025-02-16 21:51:52,568 - root - INFO - Parameter module.blocks.2.mlp.fc1.bias sizes across ranks: [384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384]
2025-02-16 21:51:52,569 - root - INFO - Parameter module.blocks.2.mlp.fc2.weight sizes across ranks: [147456, 147456, 147456, 147456, 147456, 147456, 147456, 147456, 147456, 147456, 147456, 147456, 147456, 147456, 147456, 147456]
2025-02-16 21:51:52,569 - root - INFO - Parameter module.blocks.2.mlp.fc2.bias sizes across ranks: [384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384]
2025-02-16 21:51:52,570 - root - INFO - Parameter module.blocks.2.norm1.norm.weight sizes across ranks: [384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384]
2025-02-16 21:51:52,571 - root - INFO - Parameter module.blocks.2.norm1.norm.bias sizes across ranks: [384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384]
2025-02-16 21:51:52,572 - root - INFO - Parameter module.blocks.2.norm2.norm.weight sizes across ranks: [384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384]
2025-02-16 21:51:52,572 - root - INFO - Parameter module.blocks.2.norm2.norm.bias sizes across ranks: [384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384]
2025-02-16 21:51:52,573 - root - INFO - Parameter module.blocks.3.attn.q.weight sizes across ranks: [36864, 36864, 36864, 36864, 36864, 36864, 36864, 36864, 36864, 36864, 36864, 36864, 36864, 36864, 36864, 36864]
2025-02-16 21:51:52,574 - root - INFO - Parameter module.blocks.3.attn.q.bias sizes across ranks: [96, 96, 96, 96, 96, 96, 96, 96, 96, 96, 96, 96, 96, 96, 96, 96]
2025-02-16 21:51:52,575 - root - INFO - Parameter module.blocks.3.attn.k.weight sizes across ranks: [36864, 36864, 36864, 36864, 36864, 36864, 36864, 36864, 36864, 36864, 36864, 36864, 36864, 36864, 36864, 36864]
2025-02-16 21:51:52,575 - root - INFO - Parameter module.blocks.3.attn.k.bias sizes across ranks: [96, 96, 96, 96, 96, 96, 96, 96, 96, 96, 96, 96, 96, 96, 96, 96]
2025-02-16 21:51:52,576 - root - INFO - Parameter module.blocks.3.attn.v.weight sizes across ranks: [36864, 36864, 36864, 36864, 36864, 36864, 36864, 36864, 36864, 36864, 36864, 36864, 36864, 36864, 36864, 36864]
2025-02-16 21:51:52,577 - root - INFO - Parameter module.blocks.3.attn.v.bias sizes across ranks: [96, 96, 96, 96, 96, 96, 96, 96, 96, 96, 96, 96, 96, 96, 96, 96]
2025-02-16 21:51:52,578 - root - INFO - Parameter module.blocks.3.attn.proj.weight sizes across ranks: [36864, 36864, 36864, 36864, 36864, 36864, 36864, 36864, 36864, 36864, 36864, 36864, 36864, 36864, 36864, 36864]
2025-02-16 21:51:52,578 - root - INFO - Parameter module.blocks.3.attn.proj.bias sizes across ranks: [384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384]
2025-02-16 21:51:52,579 - root - INFO - Parameter module.blocks.3.mlp.fc1.weight sizes across ranks: [147456, 147456, 147456, 147456, 147456, 147456, 147456, 147456, 147456, 147456, 147456, 147456, 147456, 147456, 147456, 147456]
2025-02-16 21:51:52,580 - root - INFO - Parameter module.blocks.3.mlp.fc1.bias sizes across ranks: [384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384]
2025-02-16 21:51:52,581 - root - INFO - Parameter module.blocks.3.mlp.fc2.weight sizes across ranks: [147456, 147456, 147456, 147456, 147456, 147456, 147456, 147456, 147456, 147456, 147456, 147456, 147456, 147456, 147456, 147456]
2025-02-16 21:51:52,582 - root - INFO - Parameter module.blocks.3.mlp.fc2.bias sizes across ranks: [384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384]
2025-02-16 21:51:52,582 - root - INFO - Parameter module.blocks.3.norm1.norm.weight sizes across ranks: [384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384]
2025-02-16 21:51:52,583 - root - INFO - Parameter module.blocks.3.norm1.norm.bias sizes across ranks: [384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384]
2025-02-16 21:51:52,584 - root - INFO - Parameter module.blocks.3.norm2.norm.weight sizes across ranks: [384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384]
2025-02-16 21:51:52,585 - root - INFO - Parameter module.blocks.3.norm2.norm.bias sizes across ranks: [384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384]
2025-02-16 21:51:52,585 - root - INFO - Parameter module.blocks.4.attn.q.weight sizes across ranks: [36864, 36864, 36864, 36864, 36864, 36864, 36864, 36864, 36864, 36864, 36864, 36864, 36864, 36864, 36864, 36864]
2025-02-16 21:51:52,586 - root - INFO - Parameter module.blocks.4.attn.q.bias sizes across ranks: [96, 96, 96, 96, 96, 96, 96, 96, 96, 96, 96, 96, 96, 96, 96, 96]
2025-02-16 21:51:52,587 - root - INFO - Parameter module.blocks.4.attn.k.weight sizes across ranks: [36864, 36864, 36864, 36864, 36864, 36864, 36864, 36864, 36864, 36864, 36864, 36864, 36864, 36864, 36864, 36864]
2025-02-16 21:51:52,588 - root - INFO - Parameter module.blocks.4.attn.k.bias sizes across ranks: [96, 96, 96, 96, 96, 96, 96, 96, 96, 96, 96, 96, 96, 96, 96, 96]
2025-02-16 21:51:52,588 - root - INFO - Parameter module.blocks.4.attn.v.weight sizes across ranks: [36864, 36864, 36864, 36864, 36864, 36864, 36864, 36864, 36864, 36864, 36864, 36864, 36864, 36864, 36864, 36864]
2025-02-16 21:51:52,589 - root - INFO - Parameter module.blocks.4.attn.v.bias sizes across ranks: [96, 96, 96, 96, 96, 96, 96, 96, 96, 96, 96, 96, 96, 96, 96, 96]
2025-02-16 21:51:52,590 - root - INFO - Parameter module.blocks.4.attn.proj.weight sizes across ranks: [36864, 36864, 36864, 36864, 36864, 36864, 36864, 36864, 36864, 36864, 36864, 36864, 36864, 36864, 36864, 36864]
2025-02-16 21:51:52,591 - root - INFO - Parameter module.blocks.4.attn.proj.bias sizes across ranks: [384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384]
2025-02-16 21:51:52,591 - root - INFO - Parameter module.blocks.4.mlp.fc1.weight sizes across ranks: [147456, 147456, 147456, 147456, 147456, 147456, 147456, 147456, 147456, 147456, 147456, 147456, 147456, 147456, 147456, 147456]
2025-02-16 21:51:52,592 - root - INFO - Parameter module.blocks.4.mlp.fc1.bias sizes across ranks: [384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384]
2025-02-16 21:51:52,593 - root - INFO - Parameter module.blocks.4.mlp.fc2.weight sizes across ranks: [147456, 147456, 147456, 147456, 147456, 147456, 147456, 147456, 147456, 147456, 147456, 147456, 147456, 147456, 147456, 147456]
2025-02-16 21:51:52,594 - root - INFO - Parameter module.blocks.4.mlp.fc2.bias sizes across ranks: [384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384]
2025-02-16 21:51:52,594 - root - INFO - Parameter module.blocks.4.norm1.norm.weight sizes across ranks: [384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384]
2025-02-16 21:51:52,595 - root - INFO - Parameter module.blocks.4.norm1.norm.bias sizes across ranks: [384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384]
2025-02-16 21:51:52,596 - root - INFO - Parameter module.blocks.4.norm2.norm.weight sizes across ranks: [384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384]
2025-02-16 21:51:52,597 - root - INFO - Parameter module.blocks.4.norm2.norm.bias sizes across ranks: [384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384]
2025-02-16 21:51:52,598 - root - INFO - Parameter module.blocks.5.attn.q.weight sizes across ranks: [36864, 36864, 36864, 36864, 36864, 36864, 36864, 36864, 36864, 36864, 36864, 36864, 36864, 36864, 36864, 36864]
2025-02-16 21:51:52,598 - root - INFO - Parameter module.blocks.5.attn.q.bias sizes across ranks: [96, 96, 96, 96, 96, 96, 96, 96, 96, 96, 96, 96, 96, 96, 96, 96]
2025-02-16 21:51:52,599 - root - INFO - Parameter module.blocks.5.attn.k.weight sizes across ranks: [36864, 36864, 36864, 36864, 36864, 36864, 36864, 36864, 36864, 36864, 36864, 36864, 36864, 36864, 36864, 36864]
2025-02-16 21:51:52,600 - root - INFO - Parameter module.blocks.5.attn.k.bias sizes across ranks: [96, 96, 96, 96, 96, 96, 96, 96, 96, 96, 96, 96, 96, 96, 96, 96]
2025-02-16 21:51:52,601 - root - INFO - Parameter module.blocks.5.attn.v.weight sizes across ranks: [36864, 36864, 36864, 36864, 36864, 36864, 36864, 36864, 36864, 36864, 36864, 36864, 36864, 36864, 36864, 36864]
2025-02-16 21:51:52,601 - root - INFO - Parameter module.blocks.5.attn.v.bias sizes across ranks: [96, 96, 96, 96, 96, 96, 96, 96, 96, 96, 96, 96, 96, 96, 96, 96]
2025-02-16 21:51:52,602 - root - INFO - Parameter module.blocks.5.attn.proj.weight sizes across ranks: [36864, 36864, 36864, 36864, 36864, 36864, 36864, 36864, 36864, 36864, 36864, 36864, 36864, 36864, 36864, 36864]
2025-02-16 21:51:52,603 - root - INFO - Parameter module.blocks.5.attn.proj.bias sizes across ranks: [384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384]
2025-02-16 21:51:52,604 - root - INFO - Parameter module.blocks.5.mlp.fc1.weight sizes across ranks: [147456, 147456, 147456, 147456, 147456, 147456, 147456, 147456, 147456, 147456, 147456, 147456, 147456, 147456, 147456, 147456]
2025-02-16 21:51:52,604 - root - INFO - Parameter module.blocks.5.mlp.fc1.bias sizes across ranks: [384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384]
2025-02-16 21:51:52,605 - root - INFO - Parameter module.blocks.5.mlp.fc2.weight sizes across ranks: [147456, 147456, 147456, 147456, 147456, 147456, 147456, 147456, 147456, 147456, 147456, 147456, 147456, 147456, 147456, 147456]
2025-02-16 21:51:52,606 - root - INFO - Parameter module.blocks.5.mlp.fc2.bias sizes across ranks: [384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384]
2025-02-16 21:51:52,607 - root - INFO - Parameter module.blocks.5.norm1.norm.weight sizes across ranks: [384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384]
2025-02-16 21:51:52,607 - root - INFO - Parameter module.blocks.5.norm1.norm.bias sizes across ranks: [384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384]
2025-02-16 21:51:52,608 - root - INFO - Parameter module.blocks.5.norm2.norm.weight sizes across ranks: [384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384]
2025-02-16 21:51:52,609 - root - INFO - Parameter module.blocks.5.norm2.norm.bias sizes across ranks: [384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384]
2025-02-16 21:51:52,610 - root - INFO - Parameter module.blocks.6.attn.q.weight sizes across ranks: [36864, 36864, 36864, 36864, 36864, 36864, 36864, 36864, 36864, 36864, 36864, 36864, 36864, 36864, 36864, 36864]
2025-02-16 21:51:52,610 - root - INFO - Parameter module.blocks.6.attn.q.bias sizes across ranks: [96, 96, 96, 96, 96, 96, 96, 96, 96, 96, 96, 96, 96, 96, 96, 96]
2025-02-16 21:51:52,611 - root - INFO - Parameter module.blocks.6.attn.k.weight sizes across ranks: [36864, 36864, 36864, 36864, 36864, 36864, 36864, 36864, 36864, 36864, 36864, 36864, 36864, 36864, 36864, 36864]
2025-02-16 21:51:52,612 - root - INFO - Parameter module.blocks.6.attn.k.bias sizes across ranks: [96, 96, 96, 96, 96, 96, 96, 96, 96, 96, 96, 96, 96, 96, 96, 96]
2025-02-16 21:51:52,613 - root - INFO - Parameter module.blocks.6.attn.v.weight sizes across ranks: [36864, 36864, 36864, 36864, 36864, 36864, 36864, 36864, 36864, 36864, 36864, 36864, 36864, 36864, 36864, 36864]
2025-02-16 21:51:52,614 - root - INFO - Parameter module.blocks.6.attn.v.bias sizes across ranks: [96, 96, 96, 96, 96, 96, 96, 96, 96, 96, 96, 96, 96, 96, 96, 96]
2025-02-16 21:51:52,614 - root - INFO - Parameter module.blocks.6.attn.proj.weight sizes across ranks: [36864, 36864, 36864, 36864, 36864, 36864, 36864, 36864, 36864, 36864, 36864, 36864, 36864, 36864, 36864, 36864]
2025-02-16 21:51:52,615 - root - INFO - Parameter module.blocks.6.attn.proj.bias sizes across ranks: [384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384]
2025-02-16 21:51:52,616 - root - INFO - Parameter module.blocks.6.mlp.fc1.weight sizes across ranks: [147456, 147456, 147456, 147456, 147456, 147456, 147456, 147456, 147456, 147456, 147456, 147456, 147456, 147456, 147456, 147456]
2025-02-16 21:51:52,617 - root - INFO - Parameter module.blocks.6.mlp.fc1.bias sizes across ranks: [384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384]
2025-02-16 21:51:52,617 - root - INFO - Parameter module.blocks.6.mlp.fc2.weight sizes across ranks: [147456, 147456, 147456, 147456, 147456, 147456, 147456, 147456, 147456, 147456, 147456, 147456, 147456, 147456, 147456, 147456]
2025-02-16 21:51:52,618 - root - INFO - Parameter module.blocks.6.mlp.fc2.bias sizes across ranks: [384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384]
2025-02-16 21:51:52,619 - root - INFO - Parameter module.blocks.6.norm1.norm.weight sizes across ranks: [384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384]
2025-02-16 21:51:52,620 - root - INFO - Parameter module.blocks.6.norm1.norm.bias sizes across ranks: [384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384]
2025-02-16 21:51:52,620 - root - INFO - Parameter module.blocks.6.norm2.norm.weight sizes across ranks: [384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384]
2025-02-16 21:51:52,621 - root - INFO - Parameter module.blocks.6.norm2.norm.bias sizes across ranks: [384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384]
2025-02-16 21:51:52,622 - root - INFO - Parameter module.blocks.7.attn.q.weight sizes across ranks: [36864, 36864, 36864, 36864, 36864, 36864, 36864, 36864, 36864, 36864, 36864, 36864, 36864, 36864, 36864, 36864]
2025-02-16 21:51:52,623 - root - INFO - Parameter module.blocks.7.attn.q.bias sizes across ranks: [96, 96, 96, 96, 96, 96, 96, 96, 96, 96, 96, 96, 96, 96, 96, 96]
2025-02-16 21:51:52,623 - root - INFO - Parameter module.blocks.7.attn.k.weight sizes across ranks: [36864, 36864, 36864, 36864, 36864, 36864, 36864, 36864, 36864, 36864, 36864, 36864, 36864, 36864, 36864, 36864]
2025-02-16 21:51:52,624 - root - INFO - Parameter module.blocks.7.attn.k.bias sizes across ranks: [96, 96, 96, 96, 96, 96, 96, 96, 96, 96, 96, 96, 96, 96, 96, 96]
2025-02-16 21:51:52,625 - root - INFO - Parameter module.blocks.7.attn.v.weight sizes across ranks: [36864, 36864, 36864, 36864, 36864, 36864, 36864, 36864, 36864, 36864, 36864, 36864, 36864, 36864, 36864, 36864]
2025-02-16 21:51:52,626 - root - INFO - Parameter module.blocks.7.attn.v.bias sizes across ranks: [96, 96, 96, 96, 96, 96, 96, 96, 96, 96, 96, 96, 96, 96, 96, 96]
2025-02-16 21:51:52,627 - root - INFO - Parameter module.blocks.7.attn.proj.weight sizes across ranks: [36864, 36864, 36864, 36864, 36864, 36864, 36864, 36864, 36864, 36864, 36864, 36864, 36864, 36864, 36864, 36864]
2025-02-16 21:51:52,627 - root - INFO - Parameter module.blocks.7.attn.proj.bias sizes across ranks: [384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384]
2025-02-16 21:51:52,628 - root - INFO - Parameter module.blocks.7.mlp.fc1.weight sizes across ranks: [147456, 147456, 147456, 147456, 147456, 147456, 147456, 147456, 147456, 147456, 147456, 147456, 147456, 147456, 147456, 147456]
2025-02-16 21:51:52,629 - root - INFO - Parameter module.blocks.7.mlp.fc1.bias sizes across ranks: [384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384]
2025-02-16 21:51:52,630 - root - INFO - Parameter module.blocks.7.mlp.fc2.weight sizes across ranks: [147456, 147456, 147456, 147456, 147456, 147456, 147456, 147456, 147456, 147456, 147456, 147456, 147456, 147456, 147456, 147456]
2025-02-16 21:51:52,631 - root - INFO - Parameter module.blocks.7.mlp.fc2.bias sizes across ranks: [384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384]
2025-02-16 21:51:52,631 - root - INFO - Parameter module.blocks.7.norm1.norm.weight sizes across ranks: [384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384]
2025-02-16 21:51:52,632 - root - INFO - Parameter module.blocks.7.norm1.norm.bias sizes across ranks: [384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384]
2025-02-16 21:51:52,633 - root - INFO - Parameter module.blocks.7.norm2.norm.weight sizes across ranks: [384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384]
2025-02-16 21:51:52,634 - root - INFO - Parameter module.blocks.7.norm2.norm.bias sizes across ranks: [384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384]
2025-02-16 21:51:52,634 - root - INFO - Parameter module.blocks.8.attn.q.weight sizes across ranks: [36864, 36864, 36864, 36864, 36864, 36864, 36864, 36864, 36864, 36864, 36864, 36864, 36864, 36864, 36864, 36864]
2025-02-16 21:51:52,635 - root - INFO - Parameter module.blocks.8.attn.q.bias sizes across ranks: [96, 96, 96, 96, 96, 96, 96, 96, 96, 96, 96, 96, 96, 96, 96, 96]
2025-02-16 21:51:52,636 - root - INFO - Parameter module.blocks.8.attn.k.weight sizes across ranks: [36864, 36864, 36864, 36864, 36864, 36864, 36864, 36864, 36864, 36864, 36864, 36864, 36864, 36864, 36864, 36864]
2025-02-16 21:51:52,637 - root - INFO - Parameter module.blocks.8.attn.k.bias sizes across ranks: [96, 96, 96, 96, 96, 96, 96, 96, 96, 96, 96, 96, 96, 96, 96, 96]
2025-02-16 21:51:52,637 - root - INFO - Parameter module.blocks.8.attn.v.weight sizes across ranks: [36864, 36864, 36864, 36864, 36864, 36864, 36864, 36864, 36864, 36864, 36864, 36864, 36864, 36864, 36864, 36864]
2025-02-16 21:51:52,638 - root - INFO - Parameter module.blocks.8.attn.v.bias sizes across ranks: [96, 96, 96, 96, 96, 96, 96, 96, 96, 96, 96, 96, 96, 96, 96, 96]
2025-02-16 21:51:52,639 - root - INFO - Parameter module.blocks.8.attn.proj.weight sizes across ranks: [36864, 36864, 36864, 36864, 36864, 36864, 36864, 36864, 36864, 36864, 36864, 36864, 36864, 36864, 36864, 36864]
2025-02-16 21:51:52,640 - root - INFO - Parameter module.blocks.8.attn.proj.bias sizes across ranks: [384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384]
2025-02-16 21:51:52,641 - root - INFO - Parameter module.blocks.8.mlp.fc1.weight sizes across ranks: [147456, 147456, 147456, 147456, 147456, 147456, 147456, 147456, 147456, 147456, 147456, 147456, 147456, 147456, 147456, 147456]
2025-02-16 21:51:52,641 - root - INFO - Parameter module.blocks.8.mlp.fc1.bias sizes across ranks: [384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384]
2025-02-16 21:51:52,642 - root - INFO - Parameter module.blocks.8.mlp.fc2.weight sizes across ranks: [147456, 147456, 147456, 147456, 147456, 147456, 147456, 147456, 147456, 147456, 147456, 147456, 147456, 147456, 147456, 147456]
2025-02-16 21:51:52,643 - root - INFO - Parameter module.blocks.8.mlp.fc2.bias sizes across ranks: [384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384]
2025-02-16 21:51:52,644 - root - INFO - Parameter module.blocks.8.norm1.norm.weight sizes across ranks: [384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384]
2025-02-16 21:51:52,644 - root - INFO - Parameter module.blocks.8.norm1.norm.bias sizes across ranks: [384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384]
2025-02-16 21:51:52,645 - root - INFO - Parameter module.blocks.8.norm2.norm.weight sizes across ranks: [384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384]
2025-02-16 21:51:52,646 - root - INFO - Parameter module.blocks.8.norm2.norm.bias sizes across ranks: [384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384]
2025-02-16 21:51:52,647 - root - INFO - Parameter module.blocks.9.attn.q.weight sizes across ranks: [36864, 36864, 36864, 36864, 36864, 36864, 36864, 36864, 36864, 36864, 36864, 36864, 36864, 36864, 36864, 36864]
2025-02-16 21:51:52,647 - root - INFO - Parameter module.blocks.9.attn.q.bias sizes across ranks: [96, 96, 96, 96, 96, 96, 96, 96, 96, 96, 96, 96, 96, 96, 96, 96]
2025-02-16 21:51:52,648 - root - INFO - Parameter module.blocks.9.attn.k.weight sizes across ranks: [36864, 36864, 36864, 36864, 36864, 36864, 36864, 36864, 36864, 36864, 36864, 36864, 36864, 36864, 36864, 36864]
2025-02-16 21:51:52,649 - root - INFO - Parameter module.blocks.9.attn.k.bias sizes across ranks: [96, 96, 96, 96, 96, 96, 96, 96, 96, 96, 96, 96, 96, 96, 96, 96]
2025-02-16 21:51:52,650 - root - INFO - Parameter module.blocks.9.attn.v.weight sizes across ranks: [36864, 36864, 36864, 36864, 36864, 36864, 36864, 36864, 36864, 36864, 36864, 36864, 36864, 36864, 36864, 36864]
2025-02-16 21:51:52,651 - root - INFO - Parameter module.blocks.9.attn.v.bias sizes across ranks: [96, 96, 96, 96, 96, 96, 96, 96, 96, 96, 96, 96, 96, 96, 96, 96]
2025-02-16 21:51:52,651 - root - INFO - Parameter module.blocks.9.attn.proj.weight sizes across ranks: [36864, 36864, 36864, 36864, 36864, 36864, 36864, 36864, 36864, 36864, 36864, 36864, 36864, 36864, 36864, 36864]
2025-02-16 21:51:52,652 - root - INFO - Parameter module.blocks.9.attn.proj.bias sizes across ranks: [384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384]
2025-02-16 21:51:52,653 - root - INFO - Parameter module.blocks.9.mlp.fc1.weight sizes across ranks: [147456, 147456, 147456, 147456, 147456, 147456, 147456, 147456, 147456, 147456, 147456, 147456, 147456, 147456, 147456, 147456]
2025-02-16 21:51:52,654 - root - INFO - Parameter module.blocks.9.mlp.fc1.bias sizes across ranks: [384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384]
2025-02-16 21:51:52,654 - root - INFO - Parameter module.blocks.9.mlp.fc2.weight sizes across ranks: [147456, 147456, 147456, 147456, 147456, 147456, 147456, 147456, 147456, 147456, 147456, 147456, 147456, 147456, 147456, 147456]
2025-02-16 21:51:52,655 - root - INFO - Parameter module.blocks.9.mlp.fc2.bias sizes across ranks: [384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384]
2025-02-16 21:51:52,656 - root - INFO - Parameter module.blocks.9.norm1.norm.weight sizes across ranks: [384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384]
2025-02-16 21:51:52,657 - root - INFO - Parameter module.blocks.9.norm1.norm.bias sizes across ranks: [384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384]
2025-02-16 21:51:52,657 - root - INFO - Parameter module.blocks.9.norm2.norm.weight sizes across ranks: [384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384]
2025-02-16 21:51:52,658 - root - INFO - Parameter module.blocks.9.norm2.norm.bias sizes across ranks: [384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384]
2025-02-16 21:51:52,659 - root - INFO - Parameter module.blocks.10.attn.q.weight sizes across ranks: [36864, 36864, 36864, 36864, 36864, 36864, 36864, 36864, 36864, 36864, 36864, 36864, 36864, 36864, 36864, 36864]
2025-02-16 21:51:52,660 - root - INFO - Parameter module.blocks.10.attn.q.bias sizes across ranks: [96, 96, 96, 96, 96, 96, 96, 96, 96, 96, 96, 96, 96, 96, 96, 96]
2025-02-16 21:51:52,660 - root - INFO - Parameter module.blocks.10.attn.k.weight sizes across ranks: [36864, 36864, 36864, 36864, 36864, 36864, 36864, 36864, 36864, 36864, 36864, 36864, 36864, 36864, 36864, 36864]
2025-02-16 21:51:52,661 - root - INFO - Parameter module.blocks.10.attn.k.bias sizes across ranks: [96, 96, 96, 96, 96, 96, 96, 96, 96, 96, 96, 96, 96, 96, 96, 96]
2025-02-16 21:51:52,662 - root - INFO - Parameter module.blocks.10.attn.v.weight sizes across ranks: [36864, 36864, 36864, 36864, 36864, 36864, 36864, 36864, 36864, 36864, 36864, 36864, 36864, 36864, 36864, 36864]
2025-02-16 21:51:52,663 - root - INFO - Parameter module.blocks.10.attn.v.bias sizes across ranks: [96, 96, 96, 96, 96, 96, 96, 96, 96, 96, 96, 96, 96, 96, 96, 96]
2025-02-16 21:51:52,664 - root - INFO - Parameter module.blocks.10.attn.proj.weight sizes across ranks: [36864, 36864, 36864, 36864, 36864, 36864, 36864, 36864, 36864, 36864, 36864, 36864, 36864, 36864, 36864, 36864]
2025-02-16 21:51:52,664 - root - INFO - Parameter module.blocks.10.attn.proj.bias sizes across ranks: [384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384]
2025-02-16 21:51:52,665 - root - INFO - Parameter module.blocks.10.mlp.fc1.weight sizes across ranks: [147456, 147456, 147456, 147456, 147456, 147456, 147456, 147456, 147456, 147456, 147456, 147456, 147456, 147456, 147456, 147456]
2025-02-16 21:51:52,666 - root - INFO - Parameter module.blocks.10.mlp.fc1.bias sizes across ranks: [384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384]
2025-02-16 21:51:52,667 - root - INFO - Parameter module.blocks.10.mlp.fc2.weight sizes across ranks: [147456, 147456, 147456, 147456, 147456, 147456, 147456, 147456, 147456, 147456, 147456, 147456, 147456, 147456, 147456, 147456]
2025-02-16 21:51:52,667 - root - INFO - Parameter module.blocks.10.mlp.fc2.bias sizes across ranks: [384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384]
2025-02-16 21:51:52,668 - root - INFO - Parameter module.blocks.10.norm1.norm.weight sizes across ranks: [384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384]
2025-02-16 21:51:52,669 - root - INFO - Parameter module.blocks.10.norm1.norm.bias sizes across ranks: [384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384]
2025-02-16 21:51:52,670 - root - INFO - Parameter module.blocks.10.norm2.norm.weight sizes across ranks: [384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384]
2025-02-16 21:51:52,670 - root - INFO - Parameter module.blocks.10.norm2.norm.bias sizes across ranks: [384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384]
2025-02-16 21:51:52,671 - root - INFO - Parameter module.blocks.11.attn.q.weight sizes across ranks: [36864, 36864, 36864, 36864, 36864, 36864, 36864, 36864, 36864, 36864, 36864, 36864, 36864, 36864, 36864, 36864]
2025-02-16 21:51:52,672 - root - INFO - Parameter module.blocks.11.attn.q.bias sizes across ranks: [96, 96, 96, 96, 96, 96, 96, 96, 96, 96, 96, 96, 96, 96, 96, 96]
2025-02-16 21:51:52,673 - root - INFO - Parameter module.blocks.11.attn.k.weight sizes across ranks: [36864, 36864, 36864, 36864, 36864, 36864, 36864, 36864, 36864, 36864, 36864, 36864, 36864, 36864, 36864, 36864]
2025-02-16 21:51:52,673 - root - INFO - Parameter module.blocks.11.attn.k.bias sizes across ranks: [96, 96, 96, 96, 96, 96, 96, 96, 96, 96, 96, 96, 96, 96, 96, 96]
2025-02-16 21:51:52,674 - root - INFO - Parameter module.blocks.11.attn.v.weight sizes across ranks: [36864, 36864, 36864, 36864, 36864, 36864, 36864, 36864, 36864, 36864, 36864, 36864, 36864, 36864, 36864, 36864]
2025-02-16 21:51:52,675 - root - INFO - Parameter module.blocks.11.attn.v.bias sizes across ranks: [96, 96, 96, 96, 96, 96, 96, 96, 96, 96, 96, 96, 96, 96, 96, 96]
2025-02-16 21:51:52,676 - root - INFO - Parameter module.blocks.11.attn.proj.weight sizes across ranks: [36864, 36864, 36864, 36864, 36864, 36864, 36864, 36864, 36864, 36864, 36864, 36864, 36864, 36864, 36864, 36864]
2025-02-16 21:51:52,677 - root - INFO - Parameter module.blocks.11.attn.proj.bias sizes across ranks: [384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384]
2025-02-16 21:51:52,677 - root - INFO - Parameter module.blocks.11.mlp.fc1.weight sizes across ranks: [147456, 147456, 147456, 147456, 147456, 147456, 147456, 147456, 147456, 147456, 147456, 147456, 147456, 147456, 147456, 147456]
2025-02-16 21:51:52,678 - root - INFO - Parameter module.blocks.11.mlp.fc1.bias sizes across ranks: [384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384]
2025-02-16 21:51:52,679 - root - INFO - Parameter module.blocks.11.mlp.fc2.weight sizes across ranks: [147456, 147456, 147456, 147456, 147456, 147456, 147456, 147456, 147456, 147456, 147456, 147456, 147456, 147456, 147456, 147456]
2025-02-16 21:51:52,680 - root - INFO - Parameter module.blocks.11.mlp.fc2.bias sizes across ranks: [384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384]
2025-02-16 21:51:52,680 - root - INFO - Parameter module.blocks.11.norm1.norm.weight sizes across ranks: [384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384]
2025-02-16 21:51:52,681 - root - INFO - Parameter module.blocks.11.norm1.norm.bias sizes across ranks: [384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384]
2025-02-16 21:51:52,682 - root - INFO - Parameter module.blocks.11.norm2.norm.weight sizes across ranks: [384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384]
2025-02-16 21:51:52,683 - root - INFO - Parameter module.blocks.11.norm2.norm.bias sizes across ranks: [384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384]
2025-02-16 21:51:52,683 - root - INFO - Parameter module.norm.weight sizes across ranks: [384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384]
2025-02-16 21:51:52,684 - root - INFO - Parameter module.norm.bias sizes across ranks: [384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384]
2025-02-16 21:51:52,685 - root - INFO - Parameter module.head.weight sizes across ranks: [122880, 122880, 122880, 122880, 122880, 122880, 122880, 122880, 122880, 122880, 122880, 122880, 122880, 122880, 122880, 122880]
2025-02-16 21:51:52,700 - root - INFO - Process group tp: size=4
2025-02-16 21:51:52,700 - root - INFO - Process group tp ranks: 0
2025-02-16 21:51:52,700 - root - INFO - Process group cp: size=1
2025-02-16 21:51:52,700 - root - INFO - Process group cp ranks: 0
2025-02-16 21:51:52,700 - root - INFO - Process group dp: size=4
2025-02-16 21:51:52,700 - root - INFO - Process group dp ranks: 0
2025-02-16 21:51:52,700 - root - INFO - Process group tp-cp: size=4
2025-02-16 21:51:52,700 - root - INFO - Process group tp-cp ranks: 0
2025-02-16 21:51:52,700 - root - INFO - Block 0 attention: num_heads=8, num_heads_local=2
2025-02-16 21:51:52,700 - root - INFO - Block 0 attention tp group size: 4
2025-02-16 21:51:52,700 - root - INFO - tp-cp size: 4
2025-02-16 21:51:54,802 - root - INFO -  Memory usage after forward pass: 16.0428466796875 GB.
[rank2]:[E216 22:03:58.405763850 ProcessGroupNCCL.cpp:568] [Rank 2] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=11761, OpType=ALLREDUCE, NumelIn=24883200, NumelOut=24883200, Timeout(ms)=600000) ran for 600000 milliseconds before timing out.
[rank2]:[E216 22:03:58.407067018 ProcessGroupNCCL.cpp:1583] [PG 5 Rank 2] Exception (either an error or timeout) detected by watchdog at work: 11761, last enqueued NCCL work: 11808, last completed NCCL work: 11760.
[rank2]:[E216 22:03:58.407073119 ProcessGroupNCCL.cpp:1628] [PG 5 Rank 2] Timeout at NCCL work: 11761, last enqueued NCCL work: 11808, last completed NCCL work: 11760.
[rank2]:[E216 22:03:58.407076726 ProcessGroupNCCL.cpp:582] [Rank 2] Some NCCL operations have failed or timed out. Due to the asynchronous nature of CUDA kernels, subsequent GPU operations might run on corrupted/incomplete data.
[rank2]:[E216 22:03:58.407079471 ProcessGroupNCCL.cpp:588] [Rank 2] To avoid data inconsistency, we are taking the entire process down.
[rank2]:[E216 22:03:58.412567704 ProcessGroupNCCL.cpp:1444] [PG 5 Rank 2] Process group watchdog thread terminated with exception: [Rank 2] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=11761, OpType=ALLREDUCE, NumelIn=24883200, NumelOut=24883200, Timeout(ms)=600000) ran for 600000 milliseconds before timing out.
Exception raised from checkTimeout at /opt/pytorch/pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:570 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >) + 0x99 (0x7f8e3a0fb919 in /usr/local/lib/python3.10/dist-packages/torch/lib/libc10.so)
frame #1: c10d::ProcessGroupNCCL::WorkNCCL::checkTimeout(std::optional<std::chrono::duration<long, std::ratio<1l, 1000l> > >) + 0x1e1 (0x7f8dd3e79181 in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cuda.so)
frame #2: c10d::ProcessGroupNCCL::watchdogHandler() + 0x247 (0x7f8dd3e82657 in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cuda.so)
frame #3: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0x10f (0x7f8dd3e8415f in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cuda.so)
frame #4: <unknown function> + 0xdc253 (0x7f8e39cb0253 in /lib/x86_64-linux-gnu/libstdc++.so.6)
frame #5: <unknown function> + 0x94ac3 (0x7f8e96c58ac3 in /lib/x86_64-linux-gnu/libc.so.6)
frame #6: <unknown function> + 0x126850 (0x7f8e96cea850 in /lib/x86_64-linux-gnu/libc.so.6)

terminate called after throwing an instance of 'c10::DistBackendError'
  what():  [PG 5 Rank 2] Process group watchdog thread terminated with exception: [Rank 2] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=11761, OpType=ALLREDUCE, NumelIn=24883200, NumelOut=24883200, Timeout(ms)=600000) ran for 600000 milliseconds before timing out.
Exception raised from checkTimeout at /opt/pytorch/pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:570 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >) + 0x99 (0x7f8e3a0fb919 in /usr/local/lib/python3.10/dist-packages/torch/lib/libc10.so)
frame #1: c10d::ProcessGroupNCCL::WorkNCCL::checkTimeout(std::optional<std::chrono::duration<long, std::ratio<1l, 1000l> > >) + 0x1e1 (0x7f8dd3e79181 in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cuda.so)
frame #2: c10d::ProcessGroupNCCL::watchdogHandler() + 0x247 (0x7f8dd3e82657 in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cuda.so)
frame #3: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0x10f (0x7f8dd3e8415f in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cuda.so)
frame #4: <unknown function> + 0xdc253 (0x7f8e39cb0253 in /lib/x86_64-linux-gnu/libstdc++.so.6)
frame #5: <unknown function> + 0x94ac3 (0x7f8e96c58ac3 in /lib/x86_64-linux-gnu/libc.so.6)
frame #6: <unknown function> + 0x126850 (0x7f8e96cea850 in /lib/x86_64-linux-gnu/libc.so.6)

Exception raised from ncclCommWatchdog at /opt/pytorch/pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1448 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >) + 0x99 (0x7f8e3a0fb919 in /usr/local/lib/python3.10/dist-packages/torch/lib/libc10.so)
frame #1: <unknown function> + 0x107b51e (0x7f8dd3eab51e in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cuda.so)
frame #2: <unknown function> + 0xcc34df (0x7f8dd3af34df in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cuda.so)
frame #3: <unknown function> + 0xdc253 (0x7f8e39cb0253 in /lib/x86_64-linux-gnu/libstdc++.so.6)
frame #4: <unknown function> + 0x94ac3 (0x7f8e96c58ac3 in /lib/x86_64-linux-gnu/libc.so.6)
frame #5: <unknown function> + 0x126850 (0x7f8e96cea850 in /lib/x86_64-linux-gnu/libc.so.6)

[rank0]:[E216 22:03:58.420398943 ProcessGroupNCCL.cpp:568] [Rank 0] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=773, OpType=ALLREDUCE, NumelIn=20, NumelOut=20, Timeout(ms)=600000) ran for 600015 milliseconds before timing out.
[rank0]:[E216 22:03:58.421564474 ProcessGroupNCCL.cpp:1583] [PG 1 Rank 0] Exception (either an error or timeout) detected by watchdog at work: 773, last enqueued NCCL work: 777, last completed NCCL work: 772.
[rank0]:[E216 22:03:58.421570807 ProcessGroupNCCL.cpp:1628] [PG 1 Rank 0] Timeout at NCCL work: 773, last enqueued NCCL work: 777, last completed NCCL work: 772.
[rank0]:[E216 22:03:58.421574554 ProcessGroupNCCL.cpp:582] [Rank 0] Some NCCL operations have failed or timed out. Due to the asynchronous nature of CUDA kernels, subsequent GPU operations might run on corrupted/incomplete data.
[rank0]:[E216 22:03:58.421577750 ProcessGroupNCCL.cpp:588] [Rank 0] To avoid data inconsistency, we are taking the entire process down.
[rank0]:[E216 22:03:58.421604572 ProcessGroupNCCL.cpp:1444] [PG 1 Rank 0] Process group watchdog thread terminated with exception: [Rank 0] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=773, OpType=ALLREDUCE, NumelIn=20, NumelOut=20, Timeout(ms)=600000) ran for 600015 milliseconds before timing out.
Exception raised from checkTimeout at /opt/pytorch/pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:570 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >) + 0x99 (0x7f795e1c8919 in /usr/local/lib/python3.10/dist-packages/torch/lib/libc10.so)
frame #1: c10d::ProcessGroupNCCL::WorkNCCL::checkTimeout(std::optional<std::chrono::duration<long, std::ratio<1l, 1000l> > >) + 0x1e1 (0x7f795f279181 in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cuda.so)
frame #2: c10d::ProcessGroupNCCL::watchdogHandler() + 0x247 (0x7f795f282657 in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cuda.so)
frame #3: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0x10f (0x7f795f28415f in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cuda.so)
frame #4: <unknown function> + 0xdc253 (0x7f79c50b0253 in /lib/x86_64-linux-gnu/libstdc++.so.6)
frame #5: <unknown function> + 0x94ac3 (0x7f7a21f97ac3 in /lib/x86_64-linux-gnu/libc.so.6)
frame #6: <unknown function> + 0x126850 (0x7f7a22029850 in /lib/x86_64-linux-gnu/libc.so.6)

terminate called after throwing an instance of 'c10::DistBackendError'
  what():  [PG 1 Rank 0] Process group watchdog thread terminated with exception: [Rank 0] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=773, OpType=ALLREDUCE, NumelIn=20, NumelOut=20, Timeout(ms)=600000) ran for 600015 milliseconds before timing out.
Exception raised from checkTimeout at /opt/pytorch/pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:570 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >) + 0x99 (0x7f795e1c8919 in /usr/local/lib/python3.10/dist-packages/torch/lib/libc10.so)
frame #1: c10d::ProcessGroupNCCL::WorkNCCL::checkTimeout(std::optional<std::chrono::duration<long, std::ratio<1l, 1000l> > >) + 0x1e1 (0x7f795f279181 in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cuda.so)
frame #2: c10d::ProcessGroupNCCL::watchdogHandler() + 0x247 (0x7f795f282657 in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cuda.so)
frame #3: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0x10f (0x7f795f28415f in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cuda.so)
frame #4: <unknown function> + 0xdc253 (0x7f79c50b0253 in /lib/x86_64-linux-gnu/libstdc++.so.6)
frame #5: <unknown function> + 0x94ac3 (0x7f7a21f97ac3 in /lib/x86_64-linux-gnu/libc.so.6)
frame #6: <unknown function> + 0x126850 (0x7f7a22029850 in /lib/x86_64-linux-gnu/libc.so.6)

Exception raised from ncclCommWatchdog at /opt/pytorch/pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1448 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >) + 0x99 (0x7f795e1c8919 in /usr/local/lib/python3.10/dist-packages/torch/lib/libc10.so)
frame #1: <unknown function> + 0x107b51e (0x7f795f2ab51e in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cuda.so)
frame #2: <unknown function> + 0xcc34df (0x7f795eef34df in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cuda.so)
frame #3: <unknown function> + 0xdc253 (0x7f79c50b0253 in /lib/x86_64-linux-gnu/libstdc++.so.6)
frame #4: <unknown function> + 0x94ac3 (0x7f7a21f97ac3 in /lib/x86_64-linux-gnu/libc.so.6)
frame #5: <unknown function> + 0x126850 (0x7f7a22029850 in /lib/x86_64-linux-gnu/libc.so.6)

[rank12]:[E216 22:03:58.855286477 ProcessGroupNCCL.cpp:568] [Rank 3] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=772, OpType=ALLREDUCE, NumelIn=271488, NumelOut=271488, Timeout(ms)=600000) ran for 600030 milliseconds before timing out.
[rank12]:[E216 22:03:58.856577663 ProcessGroupNCCL.cpp:1583] [PG 1 Rank 3] Exception (either an error or timeout) detected by watchdog at work: 772, last enqueued NCCL work: 775, last completed NCCL work: 771.
[rank12]:[E216 22:03:58.856584306 ProcessGroupNCCL.cpp:1628] [PG 1 Rank 3] Timeout at NCCL work: 772, last enqueued NCCL work: 775, last completed NCCL work: 771.
[rank12]:[E216 22:03:58.856588013 ProcessGroupNCCL.cpp:582] [Rank 3] Some NCCL operations have failed or timed out. Due to the asynchronous nature of CUDA kernels, subsequent GPU operations might run on corrupted/incomplete data.
[rank12]:[E216 22:03:58.856590848 ProcessGroupNCCL.cpp:588] [Rank 3] To avoid data inconsistency, we are taking the entire process down.
[rank9]:[E216 22:03:58.417493632 ProcessGroupNCCL.cpp:568] [Rank 2] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=773, OpType=ALLREDUCE, NumelIn=11417088, NumelOut=11417088, Timeout(ms)=600000) ran for 600011 milliseconds before timing out.
[rank10]:[E216 22:03:58.417496588 ProcessGroupNCCL.cpp:568] [Rank 2] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=773, OpType=ALLREDUCE, NumelIn=11417088, NumelOut=11417088, Timeout(ms)=600000) ran for 600017 milliseconds before timing out.
[rank11]:[E216 22:03:58.417493352 ProcessGroupNCCL.cpp:568] [Rank 2] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=773, OpType=ALLREDUCE, NumelIn=11417088, NumelOut=11417088, Timeout(ms)=600000) ran for 600015 milliseconds before timing out.
[rank12]:[E216 22:03:58.862235972 ProcessGroupNCCL.cpp:1444] [PG 1 Rank 3] Process group watchdog thread terminated with exception: [Rank 3] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=772, OpType=ALLREDUCE, NumelIn=271488, NumelOut=271488, Timeout(ms)=600000) ran for 600030 milliseconds before timing out.
Exception raised from checkTimeout at /opt/pytorch/pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:570 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >) + 0x99 (0x7f8b8abc8919 in /usr/local/lib/python3.10/dist-packages/torch/lib/libc10.so)
frame #1: c10d::ProcessGroupNCCL::WorkNCCL::checkTimeout(std::optional<std::chrono::duration<long, std::ratio<1l, 1000l> > >) + 0x1e1 (0x7f8b8bc79181 in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cuda.so)
frame #2: c10d::ProcessGroupNCCL::watchdogHandler() + 0x247 (0x7f8b8bc82657 in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cuda.so)
frame #3: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0x10f (0x7f8b8bc8415f in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cuda.so)
frame #4: <unknown function> + 0xdc253 (0x7f8bf1ab0253 in /lib/x86_64-linux-gnu/libstdc++.so.6)
frame #5: <unknown function> + 0x94ac3 (0x7f8c4ea08ac3 in /lib/x86_64-linux-gnu/libc.so.6)
frame #6: <unknown function> + 0x126850 (0x7f8c4ea9a850 in /lib/x86_64-linux-gnu/libc.so.6)

terminate called after throwing an instance of 'c10::DistBackendError'
  what():  [PG 1 Rank 3] Process group watchdog thread terminated with exception: [Rank 3] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=772, OpType=ALLREDUCE, NumelIn=271488, NumelOut=271488, Timeout(ms)=600000) ran for 600030 milliseconds before timing out.
Exception raised from checkTimeout at /opt/pytorch/pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:570 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >) + 0x99 (0x7f8b8abc8919 in /usr/local/lib/python3.10/dist-packages/torch/lib/libc10.so)
frame #1: c10d::ProcessGroupNCCL::WorkNCCL::checkTimeout(std::optional<std::chrono::duration<long, std::ratio<1l, 1000l> > >) + 0x1e1 (0x7f8b8bc79181 in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cuda.so)
frame #2: c10d::ProcessGroupNCCL::watchdogHandler() + 0x247 (0x7f8b8bc82657 in /usr/local/lib/python3[rank9]:[E216 22:03:58.418664067 ProcessGroupNCCL.cpp:1583] [PG 2 Rank 2] Exception (either an error or timeout) detected by watchdog at work: 773, last enqueued NCCL work: 775, last completed NCCL work: 772.
[rank9]:[E216 22:03:58.418671342 ProcessGroupNCCL.cpp:1628] [PG 2 Rank 2] Timeout at NCCL work: 773, last enqueued NCCL work: 775, last completed NCCL work: 772.
[rank9]:[E216 22:03:58.418675289 ProcessGroupNCCL.cpp:582] [Rank 2] Some NCCL operations have failed or timed out. Due to the asynchronous nature of CUDA kernels, subsequent GPU operations might run on corrupted/incomplete data.
[rank9]:[E216 22:03:58.418678275 ProcessGroupNCCL.cpp:588] [Rank 2] To avoid data inconsistency, we are taking the entire process down.
.10/dist-packages/torch/lib/libtorch_cuda.so)
frame #3: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0x10f (0x7f8b8bc8415f in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cuda.so)
frame #4: <unknown function> + 0xdc253 (0x7f8bf1ab0253 in /lib/x86_64-linux-gnu/libstdc++.so.6)
frame #5: <unknown function> + 0x94ac3 (0x7f8c4ea08ac3 in /lib/x86_64-linux-gnu/libc.so.6)
frame #6: <unknown function> + 0x126850 (0x7f8c4ea9a850 in /lib/x86_64-linux-gnu/libc.so.6)

Exception raised from ncclCommWatchdog at /opt/pytorch/pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1448 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >) + 0x99 (0x7f8b8abc8919 in /usr/local/lib/python3.10/dist-packages/torch/lib/libc10.so)
frame #1: <unknown function> + 0x107b51e (0x7f8b8bcab51e in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cuda.so)
frame #2: <unknown function> + 0xcc34df (0x7f8b8b8f34df in /us[rank10]:[E216 22:03:58.418670710 ProcessGroupNCCL.cpp:1583] [PG 3 Rank 2] Exception (either an error or timeout) detected by watchdog at work: 773, last enqueued NCCL work: 775, last completed NCCL work: 772.
[rank10]:[E216 22:03:58.418675840 ProcessGroupNCCL.cpp:1628] [PG 3 Rank 2] Timeout at NCCL work: 773, last enqueued NCCL work: 775, last completed NCCL work: 772.
[rank10]:[E216 22:03:58.418679968 ProcessGroupNCCL.cpp:582] [Rank 2] Some NCCL operations have failed or timed out. Due to the asynchronous nature of CUDA kernels, subsequent GPU operations might run on corrupted/incomplete data.
[rank10]:[E216 22:03:58.418683094 ProcessGroupNCCL.cpp:588] [Rank 2] To avoid data inconsistency, we are taking the entire process down.
[rank10]:[E216 22:03:58.418711579 ProcessGroupNCCL.cpp:1444] [PG 3 Rank 2] Process group watchdog thread terminated with exception: [Rank 2] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=773, OpType=ALLREDUCE, NumelIn=11417088, NumelOut=11417088, Timeout(ms)=600000) rr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cuda.so)
frame #3: <unknown function> + 0xdc253 (0x7f8bf1ab0253 in /lib/x86_64-linux-gnu/libstdc++.so.6)
frame #4: <unknown function> + 0x94ac3 (0x7f8c4ea08ac3 in /lib/x86_64-linux-gnu/libc.so.6)
frame #5: <unknown function> + 0x126850 (0x7f8c4ea9a850 in /lib/x86_64-linux-gnu/libc.so.6)

an for 600017 milliseconds before timing out.
Exception raised from checkTimeout at /opt/pytorch/pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:570 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >) + 0x99 (0x7fc29b598919 in /usr/local/lib/python3.10/dist-packages/torch/lib/libc10.so)
frame #1: c10d::ProcessGroupNCCL::WorkNCCL::checkTimeout(std::optional<std::chrono::duration<long, std::ratio<1l, 1000l> > >) + 0x1e1 (0x7fc235279181 in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cuda.so)
frame #2: c10d::ProcessGroupNCCL::watchdogHandler() + 0x247 (0x7fc235282657 in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cuda.so)
frame #3: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0x10f (0x7fc23528415f in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cuda.so)
frame #4: <unknown function> + 0xdc253 (0x7fc29b0b0253 in /lib/x86_64-linux-gnu/libstdc++.so.6)
frame #5: <unknown function> + 0x94ac3 (0x7fc2f8131ac3 in /lib/x86_64-linux-gnu/libc.so.6)
frame #6: <unknown function> + 0x126850 (0x7fc2f81c3850 in /lib/x86_64-linux-gnu/libc.so.6)

[rank11]:[E216 22:03:58.418680048 ProcessGroupNCCL.cpp:1583] [PG 4 Rank 2] Exception (either an error or timeout) detected by watchdog at work: 773, last enqueued NCCL work: 775, last completed NCCL work: 772.
[rank11]:[E216 22:03:58.418685459 ProcessGroupNCCL.cpp:1628] [PG 4 Rank 2] Timeout at NCCL work: 773, last enqueued NCCL work: 775, last completed NCCL work: 772.
[rank11]:[E216 22:03:58.418689076 ProcessGroupNCCL.cpp:582] [Rank 2] Some NCCL operations have failed or timed out. Due to the asynchronous nature of CUDA kernels, subsequent GPU operations might run on corrupted/incomplete data.
[rank11]:[E216 22:03:58.418692182 ProcessGroupNCCL.cpp:588] [Rank 2] To avoid data inconsistency, we are taking the entire process down.
[rank11]:[E216 22:03:58.418719855 ProcessGroupNCCL.cpp:1444] [PG 4 Rank 2] Process group watchdog thread terminated with exception: [Rank 2] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=773, OpType=ALLREDUCE, NumelIn=11417088, NumelOut=11417088, Timeout(ms)=600000) ran for 600015 milliseconds before timing out.
Exception raised from checkTimeout at /opt/pytorch/pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:570 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >) + 0x99 (0x7f726dd98919 in /usr/local/lib/python3.10/dist-packages/torch/lib/libc10.so)
frame #1: c10d::ProcessGroupNCCL::WorkNCCL::checkTimeout(std::optional<std::chrono::duration<long, std::ratio<1l, 1000l> > >) + 0x1e1 (0x7f7207a79181 in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cuda.so)
frame #2: c10d::ProcessGroupNCCL::watchdogHandler() + 0x247 (0x7f7207a82657 in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cuda.so)
frame #3: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0x10f (0x7f7207a8415f in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cuda.so)
frame #4: <unknown function> + 0xdc253 (0x7f726d8b0253 in /lib/x86_64-linux-gnu/libstdc++.so.6)
frame #5: <unknown function> + 0x94ac3 (0x7f72ca8ecac3 in /lib/x86_64-linux-gnu/libc.so.6)
frame #6: <unknown function> + 0x126850 (0x7f72ca97e850 in /lib/x86_64-linux-gnu/libc.so.6)

terminate called after throwing an instance of 'c10::DistBackendError'
  what():  [PG 3 Rank 2] Process group watchdog thread terminated with exception: [Rank 2] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=773, OpType=ALLREDUCE, NumelIn=11417088, NumelOut=11417088, Timeout(ms)=600000) ran for 600017 milliseconds before timing out.
Exception raised from checkTimeout at /opt/pytorch/pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:570 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >) + 0x99 (0x7fc29b598919 in /usr/local/lib/python3.10/dist-packages/torch/lib/libc10.so)
frame #1: c10d::ProcessGroupNCCL::WorkNCCL::checkTimeout(std::optional<std::chrono::duration<long, std::ratio<1l, 1000l> > >) + 0x1e1 (0x7fc235279181 in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cuda.so)
frame #2: c10d::ProcessGroupNCCL::watchdogHandler() + 0x247 (0x7fc235282657 in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cuda.so)
frame #3: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0x10f (0x7fc23528415f in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cuda.so)
frame #4: <unknown function> + 0xdc253 (0x7fc29b0b0253 in /lib/x86_64-linux-gnu/libstdc++.so.6)
frame #5: <unknown function> + 0x94ac3 (0x7fc2f8131ac3 in /lib/x86_64-linux-gnu/libc.so.6)
frame #6: <unknown function> + 0x126850 (0x7fc2f81c3850 in /lib/x86_64-linux-gnu/libc.so.6)

Exception raised from ncclCommWatchdog at /opt/pytorch/pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1448 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >) + 0x99 (0x7fc29b598919 in /usr/local/lib/python3.10/dist-packages/torch/lib/libc10.so)
frame #1: <unknown function> + 0x107b51e (0x7fc2352ab51e in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cuda.so)
frame #2: <unknown function> + 0xcc34df (0x7fc234ef34df in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cuda.so)
frame #3: <unknown function> + 0xdc253 (0x7fc29b0b0253 in /lib/x86_64-linux-gnu/libstdc++.so.6)
frame #4: <unknown function> + 0x94ac3 (0x7fc2f8131ac3 in /lib/x86_64-linux-gnu/libc.so.6)
frame #5: <unknown function> + 0x126850 (0x7fc2f81c3850 in /lib/x86_64-linux-gnu/libc.so.6)

terminate called after throwing an instance of 'c10::DistBackendError'
  what():  [PG 4 Rank 2] Process group watchdog thread terminated with exception: [Rank 2] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=773, OpType=ALLREDUCE, NumelIn=11417088, NumelOut=11417088, Timeout(ms)=600000) ran for 600015 milliseconds before timing out.
Exception raised from checkTimeout at /opt/pytorch/pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:570 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >) + 0x99 (0x7f726dd98919 in /usr/local/lib/python3.10/dist-packages/torch/lib/libc10.so)
frame #1: c10d::ProcessGroupNCCL::WorkNCCL::checkTimeout(std::optional<std::chrono::duration<long, std::ratio<1l, 1000l> > >) + 0x1e1 (0x7f7207a79181 in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cuda.so)
frame #2: c10d::ProcessGroupNCCL::watchdogHandler() + 0x247 (0x7f7207a82657 in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cuda.so)
frame #3: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0x10f (0x7f7207a8415f in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cuda.so)
frame #4: <unknown function> + 0xdc253 (0x7f726d8b0253 in /lib/x86_64-linux-gnu/libstdc++.so.6)
frame #5: <unknown function> + 0x94ac3 (0x7f72ca8ecac3 in /lib/x86_64-linux-gnu/libc.so.6)
frame #6: <unknown function> + 0x126850 (0x7f72ca97e850 in /lib/x86_64-linux-gnu/libc.so.6)

Exception raised from ncclCommWatchdog at /opt/pytorch/pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1448 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >) + 0x99 (0x7f726dd98919 in /usr/local/lib/python3.10/dist-packages/torch/lib/libc10.so)
frame #1: <unknown function> + 0x107b51e (0x7f7207aab51e in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cuda.so)
frame #2: <unknown function> + 0xcc34df (0x7f72076f34df in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cuda.so)
frame #3: <unknown function> + 0xdc253 (0x7f726d8b0253 in /lib/x86_64-linux-gnu/libstdc++.so.6)
frame #4: <unknown function> + 0x94ac3 (0x7f72ca8ecac3 in /lib/x86_64-linux-gnu/libc.so.6)
frame #5: <unknown function> + 0x126850 (0x7f72ca97e850 in /lib/x86_64-linux-gnu/libc.so.6)

[rank9]:[E216 22:03:58.424597389 ProcessGroupNCCL.cpp:1444] [PG 2 Rank 2] Process group watchdog thread terminated with exception: [Rank 2] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=773, OpType=ALLREDUCE, NumelIn=11417088, NumelOut=11417088, Timeout(ms)=600000) ran for 600011 milliseconds before timing out.
Exception raised from checkTimeout at /opt/pytorch/pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:570 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >) + 0x99 (0x7fe3ebfc8919 in /usr/local/lib/python3.10/dist-packages/torch/lib/libc10.so)
frame #1: c10d::ProcessGroupNCCL::WorkNCCL::checkTimeout(std::optional<std::chrono::duration<long, std::ratio<1l, 1000l> > >) + 0x1e1 (0x7fe3ed079181 in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cuda.so)
frame #2: c10d::ProcessGroupNCCL::watchdogHandler() + 0x247 (0x7fe3ed082657 in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cuda.so)
frame #3: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0x10f (0x7fe3ed08415f in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cuda.so)
frame #4: <unknown function> + 0xdc253 (0x7fe452eb0253 in /lib/x86_64-linux-gnu/libstdc++.so.6)
frame #5: <unknown function> + 0x94ac3 (0x7fe4afe01ac3 in /lib/x86_64-linux-gnu/libc.so.6)
frame #6: <unknown function> + 0x126850 (0x7fe4afe93850 in /lib/x86_64-linux-gnu/libc.so.6)

terminate called after throwing an instance of 'c10::DistBackendError'
  what():  [PG 2 Rank 2] Process group watchdog thread terminated with exception: [Rank 2] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=773, OpType=ALLREDUCE, NumelIn=11417088, NumelOut=11417088, Timeout(ms)=600000) ran for 600011 milliseconds before timing out.
Exception raised from checkTimeout at /opt/pytorch/pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:570 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >) + 0x99 (0x7fe3ebfc8919 in /usr/local/lib/python3.10/dist-packages/torch/lib/libc10.so)
frame #1: c10d::ProcessGroupNCCL::WorkNCCL::checkTimeout(std::optional<std::chrono::duration<long, std::ratio<1l, 1000l> > >) + 0x1e1 (0x7fe3ed079181 in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cuda.so)
frame #2: c10d::ProcessGroupNCCL::watchdogHandler() + 0x247 (0x7fe3ed082657 in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cuda.so)
frame #3: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0x10f (0x7fe3ed08415f in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cuda.so)
frame #4: <unknown function> + 0xdc253 (0x7fe452eb0253 in /lib/x86_64-linux-gnu/libstdc++.so.6)
frame #5: <unknown function> + 0x94ac3 (0x7fe4afe01ac3 in /lib/x86_64-linux-gnu/libc.so.6)
frame #6: <unknown function> + 0x126850 (0x7fe4afe93850 in /lib/x86_64-linux-gnu/libc.so.6)

Exception raised from ncclCommWatchdog at /opt/pytorch/pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1448 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >) + 0x99 (0x7fe3ebfc8919 in /usr/local/lib/python3.10/dist-packages/torch/lib/libc10.so)
frame #1: <unknown function> + 0x107b51e (0x7fe3ed0ab51e in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cuda.so)
frame #2: <unknown function> + 0xcc34df (0x7fe3eccf34df in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cuda.so)
frame #3: <unknown function> + 0xdc253 (0x7fe452eb0253 in /lib/x86_64-linux-gnu/libstdc++.so.6)
frame #4: <unknown function> + 0x94ac3 (0x7fe4afe01ac3 in /lib/x86_64-linux-gnu/libc.so.6)
frame #5: <unknown function> + 0x126850 (0x7fe4afe93850 in /lib/x86_64-linux-gnu/libc.so.6)

[rank8]:[E216 22:03:58.429681417 ProcessGroupNCCL.cpp:568] [Rank 2] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=772, OpType=ALLREDUCE, NumelIn=271488, NumelOut=271488, Timeout(ms)=600000) ran for 600048 milliseconds before timing out.
[rank8]:[E216 22:03:58.430964810 ProcessGroupNCCL.cpp:1583] [PG 1 Rank 2] Exception (either an error or timeout) detected by watchdog at work: 772, last enqueued NCCL work: 775, last completed NCCL work: 771.
[rank8]:[E216 22:03:58.430970792 ProcessGroupNCCL.cpp:1628] [PG 1 Rank 2] Timeout at NCCL work: 772, last enqueued NCCL work: 775, last completed NCCL work: 771.
[rank8]:[E216 22:03:58.430974790 ProcessGroupNCCL.cpp:582] [Rank 2] Some NCCL operations have failed or timed out. Due to the asynchronous nature of CUDA kernels, subsequent GPU operations might run on corrupted/incomplete data.
[rank8]:[E216 22:03:58.430977725 ProcessGroupNCCL.cpp:588] [Rank 2] To avoid data inconsistency, we are taking the entire process down.
[rank8]:[E216 22:03:58.431006180 ProcessGroupNCCL.cpp:1444] [PG 1 Rank 2] Process group watchdog thread terminated with exception: [Rank 2] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=772, OpType=ALLREDUCE, NumelIn=271488, NumelOut=271488, Timeout(ms)=600000) ran for 600048 milliseconds before timing out.
Exception raised from checkTimeout at /opt/pytorch/pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:570 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >) + 0x99 (0x7fe1c6b98919 in /usr/local/lib/python3.10/dist-packages/torch/lib/libc10.so)
frame #1: c10d::ProcessGroupNCCL::WorkNCCL::checkTimeout(std::optional<std::chrono::duration<long, std::ratio<1l, 1000l> > >) + 0x1e1 (0x7fe160879181 in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cuda.so)
frame #2: c10d::ProcessGroupNCCL::watchdogHandler() + 0x247 (0x7fe160882657 in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cuda.so)
frame #3: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0x10f (0x7fe16088415f in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cuda.so)
frame #4: <unknown function> + 0xdc253 (0x7fe1c66b0253 in /lib/x86_64-linux-gnu/libstdc++.so.6)
frame #5: <unknown function> + 0x94ac3 (0x7fe2236e8ac3 in /lib/x86_64-linux-gnu/libc.so.6)
frame #6: <unknown function> + 0x126850 (0x7fe22377a850 in /lib/x86_64-linux-gnu/libc.so.6)

terminate called after throwing an instance of 'c10::DistBackendError'
  what():  [PG 1 Rank 2] Process group watchdog thread terminated with exception: [Rank 2] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=772, OpType=ALLREDUCE, NumelIn=271488, NumelOut=271488, Timeout(ms)=600000) ran for 600048 milliseconds before timing out.
Exception raised from checkTimeout at /opt/pytorch/pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:570 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >) + 0x99 (0x7fe1c6b98919 in /usr/local/lib/python3.10/dist-packages/torch/lib/libc10.so)
frame #1: c10d::ProcessGroupNCCL::WorkNCCL::checkTimeout(std::optional<std::chrono::duration<long, std::ratio<1l, 1000l> > >) + 0x1e1 (0x7fe160879181 in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cuda.so)
frame #2: c10d::ProcessGroupNCCL::watchdogHandler() + 0x247 (0x7fe160882657 in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cuda.so)
frame #3: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0x10f (0x7fe16088415f in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cuda.so)
frame #4: <unknown function> + 0xdc253 (0x7fe1c66b0253 in /lib/x86_64-linux-gnu/libstdc++.so.6)
frame #5: <unknown function> + 0x94ac3 (0x7fe2236e8ac3 in /lib/x86_64-linux-gnu/libc.so.6)
frame #6: <unknown function> + 0x126850 (0x7fe22377a850 in /lib/x86_64-linux-gnu/libc.so.6)

Exception raised from ncclCommWatchdog at /opt/pytorch/pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1448 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >) + 0x99 (0x7fe1c6b98919 in /usr/local/lib/python3.10/dist-packages/torch/lib/libc10.so)
frame #1: <unknown function> + 0x107b51e (0x7fe1608ab51e in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cuda.so)
frame #2: <unknown function> + 0xcc34df (0x7fe1604f34df in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cuda.so)
frame #3: <unknown function> + 0xdc253 (0x7fe1c66b0253 in /lib/x86_64-linux-gnu/libstdc++.so.6)
frame #4: <unknown function> + 0x94ac3 (0x7fe2236e8ac3 in /lib/x86_64-linux-gnu/libc.so.6)
frame #5: <unknown function> + 0x126850 (0x7fe22377a850 in /lib/x86_64-linux-gnu/libc.so.6)

[rank15]:[E216 22:03:58.891231987 ProcessGroupNCCL.cpp:568] [Rank 3] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=773, OpType=ALLREDUCE, NumelIn=11417088, NumelOut=11417088, Timeout(ms)=600000) ran for 600049 milliseconds before timing out.
[rank15]:[E216 22:03:58.892524486 ProcessGroupNCCL.cpp:1583] [PG 4 Rank 3] Exception (either an error or timeout) detected by watchdog at work: 773, last enqueued NCCL work: 775, last completed NCCL work: 772.
[rank15]:[E216 22:03:58.892530788 ProcessGroupNCCL.cpp:1628] [PG 4 Rank 3] Timeout at NCCL work: 773, last enqueued NCCL work: 775, last completed NCCL work: 772.
[rank15]:[E216 22:03:58.892534476 ProcessGroupNCCL.cpp:582] [Rank 3] Some NCCL operations have failed or timed out. Due to the asynchronous nature of CUDA kernels, subsequent GPU operations might run on corrupted/incomplete data.
[rank15]:[E216 22:03:58.892537251 ProcessGroupNCCL.cpp:588] [Rank 3] To avoid data inconsistency, we are taking the entire process down.
[rank15]:[E216 22:03:58.892564293 ProcessGroupNCCL.cpp:1444] [PG 4 Rank 3] Process group watchdog thread terminated with exception: [Rank 3] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=773, OpType=ALLREDUCE, NumelIn=11417088, NumelOut=11417088, Timeout(ms)=600000) ran for 600049 milliseconds before timing out.
Exception raised from checkTimeout at /opt/pytorch/pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:570 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >) + 0x99 (0x7f603d9c8919 in /usr/local/lib/python3.10/dist-packages/torch/lib/libc10.so)
frame #1: c10d::ProcessGroupNCCL::WorkNCCL::checkTimeout(std::optional<std::chrono::duration<long, std::ratio<1l, 1000l> > >) + 0x1e1 (0x7f603ea79181 in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cuda.so)
frame #2: c10d::ProcessGroupNCCL::watchdogHandler() + 0x247 (0x7f603ea82657 in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cuda.so)
frame #3: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0x10f (0x7f603ea8415f in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cuda.so)
frame #4: <unknown function> + 0xdc253 (0x7f60a48b0253 in /lib/x86_64-linux-gnu/libstdc++.so.6)
frame #5: <unknown function> + 0x94ac3 (0x7f610174cac3 in /lib/x86_64-linux-gnu/libc.so.6)
frame #6: <unknown function> + 0x126850 (0x7f61017de850 in /lib/x86_64-linux-gnu/libc.so.6)

terminate called after throwing an instance of 'c10::DistBackendError'
  what():  [PG 4 Rank 3] Process group watchdog thread terminated with exception: [Rank 3] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=773, OpType=ALLREDUCE, NumelIn=11417088, NumelOut=11417088, Timeout(ms)=600000) ran for 600049 milliseconds before timing out.
Exception raised from checkTimeout at /opt/pytorch/pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:570 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >) + 0x99 (0x7f603d9c8919 in /usr/local/lib/python3.10/dist-packages/torch/lib/libc10.so)
frame #1: c10d::ProcessGroupNCCL::WorkNCCL::checkTimeout(std::optional<std::chrono::duration<long, std::ratio<1l, 1000l> > >) + 0x1e1 (0x7f603ea79181 in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cuda.so)
frame #2: c10d::ProcessGroupNCCL::watchdogHandler() + 0x247 (0x7f603ea82657 in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cuda.so)
frame #3: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0x10f (0x7f603ea8415f in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cuda.so)
frame #4: <unknown function> + 0xdc253 (0x7f60a48b0253 in /lib/x86_64-linux-gnu/libstdc++.so.6)
frame #5: <unknown function> + 0x94ac3 (0x7f610174cac3 in /lib/x86_64-linux-gnu/libc.so.6)
frame #6: <unknown function> + 0x126850 (0x7f61017de850 in /lib/x86_64-linux-gnu/libc.so.6)

Exception raised from ncclCommWatchdog at /opt/pytorch/pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1448 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >) + 0x99 (0x7f603d9c8919 in /usr/local/lib/python3.10/dist-packages/torch/lib/libc10.so)
frame #1: <unknown function> + 0x107b51e (0x7f603eaab51e in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cuda.so)
frame #2: <unknown function> + 0xcc34df (0x7f603e6f34df in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cuda.so)
frame #3: <unknown function> + 0xdc253 (0x7f60a48b0253 in /lib/x86_64-linux-gnu/libstdc++.so.6)
frame #4: <unknown function> + 0x94ac3 (0x7f610174cac3 in /lib/x86_64-linux-gnu/libc.so.6)
frame #5: <unknown function> + 0x126850 (0x7f61017de850 in /lib/x86_64-linux-gnu/libc.so.6)

[rank5]:[E216 22:03:58.623996450 ProcessGroupNCCL.cpp:568] [Rank 1] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=773, OpType=ALLREDUCE, NumelIn=11417088, NumelOut=11417088, Timeout(ms)=600000) ran for 600055 milliseconds before timing out.
[rank5]:[E216 22:03:58.625173836 ProcessGroupNCCL.cpp:1583] [PG 2 Rank 1] Exception (either an error or timeout) detected by watchdog at work: 773, last enqueued NCCL work: 775, last completed NCCL work: 772.
[rank5]:[E216 22:03:58.625179407 ProcessGroupNCCL.cpp:1628] [PG 2 Rank 1] Timeout at NCCL work: 773, last enqueued NCCL work: 775, last completed NCCL work: 772.
[rank5]:[E216 22:03:58.625182903 ProcessGroupNCCL.cpp:582] [Rank 1] Some NCCL operations have failed or timed out. Due to the asynchronous nature of CUDA kernels, subsequent GPU operations might run on corrupted/incomplete data.
[rank5]:[E216 22:03:58.625185879 ProcessGroupNCCL.cpp:588] [Rank 1] To avoid data inconsistency, we are taking the entire process down.
[rank5]:[E216 22:03:58.630805021 ProcessGroupNCCL.cpp:1444] [PG 2 Rank 1] Process group watchdog thread terminated with exception: [Rank 1] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=773, OpType=ALLREDUCE, NumelIn=11417088, NumelOut=11417088, Timeout(ms)=600000) ran for 600055 milliseconds before timing out.
Exception raised from checkTimeout at /opt/pytorch/pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:570 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >) + 0x99 (0x7fcdc37c8919 in /usr/local/lib/python3.10/dist-packages/torch/lib/libc10.so)
frame #1: c10d::ProcessGroupNCCL::WorkNCCL::checkTimeout(std::optional<std::chrono::duration<long, std::ratio<1l, 1000l> > >) + 0x1e1 (0x7fcdc4879181 in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cuda.so)
frame #2: c10d::ProcessGroupNCCL::watchdogHandler() + 0x247 (0x7fcdc4882657 in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cuda.so)
frame #3: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0x10f (0x7fcdc488415f in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cuda.so)
frame #4: <unknown function> + 0xdc253 (0x7fce2a6b0253 in /lib/x86_64-linux-gnu/libstdc++.so.6)
frame #5: <unknown function> + 0x94ac3 (0x7fce87553ac3 in /lib/x86_64-linux-gnu/libc.so.6)
frame #6: <unknown function> + 0x126850 (0x7fce875e5850 in /lib/x86_64-linux-gnu/libc.so.6)

terminate called after throwing an instance of 'c10::DistBackendError'
  what():  [PG 2 Rank 1] Process group watchdog thread terminated with exception: [Rank 1] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=773, OpType=ALLREDUCE, NumelIn=11417088, NumelOut=11417088, Timeout(ms)=600000) ran for 600055 milliseconds before timing out.
Exception raised from checkTimeout at /opt/pytorch/pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:570 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >) + 0x99 (0x7fcdc37c8919 in /usr/local/lib/python3.10/dist-packages/torch/lib/libc10.so)
frame #1: c10d::ProcessGroupNCCL::WorkNCCL::checkTimeout(std::optional<std::chrono::duration<long, std::ratio<1l, 1000l> > >) + 0x1e1 (0x7fcdc4879181 in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cuda.so)
frame #2: c10d::ProcessGroupNCCL::watchdogHandler() + 0x247 (0x7fcdc4882657 in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cuda.so)
frame #3: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0x10f (0x7fcdc488415f in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cuda.so)
frame #4: <unknown function> + 0xdc253 (0x7fce2a6b0253 in /lib/x86_64-linux-gnu/libstdc++.so.6)
frame #5: <unknown function> + 0x94ac3 (0x7fce87553ac3 in /lib/x86_64-linux-gnu/libc.so.6)
frame #6: <unknown function> + 0x126850 (0x7fce875e5850 in /lib/x86_64-linux-gnu/libc.so.6)

Exception raised from ncclCommWatchdog at /opt/pytorch/pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1448 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >) + 0x99 (0x7fcdc37c8919 in /usr/local/lib/python3.10/dist-packages/torch/lib/libc10.so)
frame #1: <unknown function> + 0x107b51e (0x7fcdc48ab51e in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cuda.so)
frame #2: <unknown function> + 0xcc34df (0x7fcdc44f34df in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cuda.so)
frame #3: <unknown function> + 0xdc253 (0x7fce2a6b0253 in /lib/x86_64-linux-gnu/libstdc++.so.6)
frame #4: <unknown function> + 0x94ac3 (0x7fce87553ac3 in /lib/x86_64-linux-gnu/libc.so.6)
frame #5: <unknown function> + 0x126850 (0x7fce875e5850 in /lib/x86_64-linux-gnu/libc.so.6)

[rank4]:[E216 22:03:58.642999211 ProcessGroupNCCL.cpp:568] [Rank 1] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=772, OpType=ALLREDUCE, NumelIn=271488, NumelOut=271488, Timeout(ms)=600000) ran for 600092 milliseconds before timing out.
[rank6]:[E216 22:03:58.642998530 ProcessGroupNCCL.cpp:568] [Rank 1] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=773, OpType=ALLREDUCE, NumelIn=11417088, NumelOut=11417088, Timeout(ms)=600000) ran for 600066 milliseconds before timing out.
[rank7]:[E216 22:03:58.643028528 ProcessGroupNCCL.cpp:568] [Rank 1] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=773, OpType=ALLREDUCE, NumelIn=11417088, NumelOut=11417088, Timeout(ms)=600000) ran for 600069 milliseconds before timing out.
[rank4]:[E216 22:03:58.644209250 ProcessGroupNCCL.cpp:1583] [PG 1 Rank 1] Exception (either an error or timeout) detected by watchdog at work: 772, last enqueued NCCL work: 775, last completed NCCL work: 771.
[rank4]:[E216 22:03:58.644215683 ProcessGroupNCCL.cpp:1628] [PG 1 Rank 1] Timeout at NCCL work: 772, last enqueued NCCL work: 775, last completed NCCL work: 771.
[rank4]:[E216 22:03:58.644220081 ProcessGroupNCCL.cpp:582] [Rank 1] Some NCCL operations have failed or timed out. Due to the asynchronous nature of CUDA kernels, subsequent GPU operations might run on corrupted/incomplete data.
[rank4]:[E216 22:03:58.644223117 ProcessGroupNCCL.cpp:588] [Rank 1] To avoid data inconsistency, we are taking the entire process down.
[rank6]:[E216 22:03:58.644191817 ProcessGroupNCCL.cpp:1583] [PG 3 Rank 1] Exception (either an error or timeout) detected by watchdog at work: 773, last enqueued NCCL work: 775, last completed NCCL work: 772.
[rank6]:[E216 22:03:58.644198730 ProcessGroupNCCL.cpp:1628] [PG 3 Rank 1] Timeout at NCCL work: 773, last enqueued NCCL work: 775, last completed NCCL work: 772.
[rank6]:[E216 22:03:58.644202758 ProcessGroupNCCL.cpp:582] [Rank 1] Some NCCL operations have failed or timed out. Due to the asynchronous nature of CUDA kernels, subsequent GPU operations might run on corrupted/incomplete data.
[rank6]:[E216 22:03:58.644205633 ProcessGroupNCCL.cpp:588] [Rank 1] To avoid data inconsistency, we are taking the entire process down.
[rank7]:[E216 22:03:58.644204290 ProcessGroupNCCL.cpp:1583] [PG 4 Rank 1] Exception (either an error or timeout) detected by watchdog at work: 773, last enqueued NCCL work: 775, last completed NCCL work: 772.
[rank7]:[E216 22:03:58.644210202 ProcessGroupNCCL.cpp:1628] [PG 4 Rank 1] Timeout at NCCL work: 773, last enqueued NCCL work: 775, last completed NCCL work: 772.
[rank7]:[E216 22:03:58.644214029 ProcessGroupNCCL.cpp:582] [Rank 1] Some NCCL operations have failed or timed out. Due to the asynchronous nature of CUDA kernels, subsequent GPU operations might run on corrupted/incomplete data.
[rank7]:[E216 22:03:58.644217166 ProcessGroupNCCL.cpp:588] [Rank 1] To avoid data inconsistency, we are taking the entire process down.
[rank6]:[E216 22:03:58.644233457 ProcessGroupNCCL.cpp:1444] [PG 3 Rank 1] Process group watchdog thread terminated with exception: [Rank 1] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=773, OpType=ALLREDUCE, NumelIn=11417088, NumelOut=11417088, Timeout(ms)=600000) ran for 600066 milliseconds before timing out.
Exception raised from checkTimeout at /opt/pytorch/pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:570 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >) + 0x99 (0x7f1875efb919 in /usr/local/lib/python3.10/dist-packages/torch/lib/libc10.so)
frame #1: c10d::ProcessGroupNCCL::WorkNCCL::checkTimeout(std::optional<std::chrono::duration<long, std::ratio<1l, 1000l> > >) + 0x1e1 (0x7f180fc79181 in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cuda.so)
frame #2: c10d::ProcessGroupNCCL::watchdogHandler() + 0x247 (0x7f180fc82657 in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cuda.so)
frame #3: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0x10f (0x7f180fc8415f in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cuda.so)
frame #4: <unknown function> + 0xdc253 (0x7f1875ab0253 in /lib/x86_64-linux-gnu/libstdc++.so.6)
frame #5: <unknown function> + 0x94ac3 (0x7f18d2a5aac3 in /lib/x86_64-linux-gnu/libc.so.6)
frame #6: <unknown function> + 0x126850 (0x7f18d2aec850 in /lib/x86_64-linux-gnu/libc.so.6)

[rank4]:[E216 22:03:58.644250380 ProcessGroupNCCL.cpp:1444] [PG 1 Rank 1] Process group watchdog thread terminated with exception: [Rank 1] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=772, OpType=ALLREDUCE, NumelIn=271488, NumelOut=271488, Timeout(ms)=600000) ran for 600092 milliseconds before timing out.
Exception raised from checkTimeout at /opt/pytorch/pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:570 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >) + 0x99 (0x7f34529c8919 in /usr/local/lib/python3.10/dist-packages/torch/lib/libc10.so)
frame #1: c10d::ProcessGroupNCCL::WorkNCCL::checkTimeout(std::optional<std::chrono::duration<long, std::ratio<1l, 1000l> > >) + 0x1e1 (0x7f3453a79181 in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cuda.so)
frame #2: c10d::ProcessGroupNCCL::watchdogHandler() + 0x247 (0x7f3453a82657 in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cuda.so)
frame #3: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0x10f (0x7f3453a8415f in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cuda.so)
frame #4: <unknown function> + 0xdc253 (0x7f34b98b0253 in /lib/x86_64-linux-gnu/libstdc++.so.6)
frame #5: <unknown function> + 0x94ac3 (0x7f35167d5ac3 in /lib/x86_64-linux-gnu/libc.so.6)
frame #6: <unknown function> + 0x126850 (0x7f3516867850 in /lib/x86_64-linux-gnu/libc.so.6)

[rank1]:[E216 22:03:58.505668904 ProcessGroupNCCL.cpp:568] [Rank 1] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=11761, OpType=ALLREDUCE, NumelIn=24883200, NumelOut=24883200, Timeout(ms)=600000) ran for 600094 milliseconds before timing out.
[rank7]:[E216 22:03:58.644243546 ProcessGroupNCCL.cpp:1444] [PG 4 Rank 1] Process group watchdog thread terminated with exception: [Rank 1] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=773, OpType=ALLREDUCE, NumelIn=11417088, NumelOut=11417088, Timeout(ms)=600000) ran for 600069 milliseconds before timing out.
Exception raised from checkTimeout at /opt/pytorch/pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:570 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >) + 0x99 (0x7fed479c8919 in /usr/local/lib/python3.10/dist-packages/torch/lib/libc10.so)
frame #1: c10d::ProcessGroupNCCL::WorkNCCL::checkTimeout(std::optional<std::chrono::duration<long, std::ratio<1l, 1000l> > >) + 0x1e1 (0x7fed48a79181 in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cuda.so)
frame #2: c10d::ProcessGroupNCCL::watchdogHandler() + 0x247 (0x7fed48a82657 in /usr/local/lib/python3.10/dist-packages/[rank3]:[E216 22:03:58.505679585 ProcessGroupNCCL.cpp:568] [Rank 3] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=11761, OpType=ALLREDUCE, NumelIn=24883200, NumelOut=24883200, Timeout(ms)=600000) ran for 600099 milliseconds before timing out.
torch/lib/libtorch_cuda.so)
frame #3: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0x10f (0x7fed48a8415f in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cuda.so)
frame #4: <unknown function> + 0xdc253 (0x7fedae8b0253 in /lib/x86_64-linux-gnu/libstdc++.so.6)
frame #5: <unknown function> + 0x94ac3 (0x7fee0b7fcac3 in /lib/x86_64-linux-gnu/libc.so.6)
frame #6: <unknown function> + 0x126850 (0x7fee0b88e850 in /lib/x86_64-linux-gnu/libc.so.6)

[rank1]:[E216 22:03:58.506755934 ProcessGroupNCCL.cpp:1583] [PG 5 Rank 1] Exception (either an error or timeout) detected by watchdog at work: 11761, last enqueued NCCL work: 11808, last completed NCCL work: 11760.
[rank1]:[E216 22:03:58.506761856 ProcessGroupNCCL.cpp:1628] [PG 5 Rank 1] Timeout at NCCL work: 11761, last enqueued NCCL work: 11808, last completed NCCL work: 11760.
[rank1]:[E216 22:03:58.506765443 ProcessGroupNCCL.cpp:582] [Rank 1] Some NCCL operations have failed or timed out. Due to the asynchronous nature of CUDA kernels, subsequent GPU operations might run on corrupted/incomplete data.
[rank1]:[E216 22:03:58.506768248 ProcessGroupNCCL.cpp:588] [Rank 1] To avoid data inconsistency, we are taking the entire process down.
terminate called after throwing an instance of 'c10::DistBackendError'
  what():  [PG 3 Rank 1] Process group watchdog thread terminated with exception: [Rank 1] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=773, OpType=ALLREDUCE, NumelIn=11417088, NumelOut=11417088, Timeout(ms)=600000) ran for 600066 milliseconds before timing out.
Exception raised from checkTimeout at /opt/pytorch/pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:570 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >) + 0x99 (0x7f1875efb919 in /usr/local/lib/python3.10/dist-packages/torch/lib/libc10.so)
frame #1: c10d::ProcessGroupNCCL::WorkNCCL::checkTimeout(std::optional<std::chrono::duration<long, std::ratio<1l, 1000l> > >) + 0x1e1 (0x7f180fc79181 in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cuda.so)
frame #2: c10d::ProcessGroupNCCL::watchdogHandler() + 0x247 (0x7f180fc82657 in /usr/local/lib/pyt[rank1]:[E216 22:03:58.506794379 ProcessGroupNCCL.cpp:1444] [PG 5 Rank 1] Process group watchdog thread terminated with exception: [Rank 1] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=11761, OpType=ALLREDUCE, NumelIn=24883200, NumelOut=24883200, Timeout(ms)=600000) ran for 600094 milliseconds before timing out.
Exception raised from checkTimeout at /opt/pytorch/pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:570 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >) + 0x99 (0x7faf84798919 in /usr/local/lib/python3.10/dist-packages/torch/lib/libc10.so)
frame #1: c10d::ProcessGroupNCCL::WorkNCCL::checkTimeout(std::optional<std::chrono::duration<long, std::ratio<1l, 1000l> > >) + 0x1e1 (0x7faf1e479181 in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cuda.so)
frame #2: c10d::ProcessGroupNCCL::watchdogHandler() + 0x247 (0x7faf1e482657 in /usr/local/lib/python3.10/dist-packagehon3.10/dist-packages/torch/lib/libtorch_cuda.so)
frame #3: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0x10f (0x7f180fc8415f in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cuda.so)
frame #4: <unknown function> + 0xdc253 (0x7f1875ab0253 in /lib/x86_64-linux-gnu/libstdc++.so.6)
frame #5: <unknown function> + 0x94ac3 (0x7f18d2a5aac3 in /lib/x86_64-linux-gnu/libc.so.6)
frame #6: <unknown function> + 0x126850 (0x7f18d2aec850 in /lib/x86_64-linux-gnu/libc.so.6)

Exception raised from ncclCommWatchdog at /opt/pytorch/pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1448 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >) + 0x99 (0x7f1875efb919 in /usr/local/lib/python3.10/dist-packages/torch/lib/libc10.so)
frame #1: <unknown function> + 0x107b51e (0x7f180fcab51e in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cuda.so)
frame #2: <unknown function> + 0xcc34df (0x7f180f8f34df ins/torch/lib/libtorch_cuda.so)
frame #3: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0x10f (0x7faf1e48415f in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cuda.so)
frame #4: <unknown function> + 0xdc253 (0x7faf842b0253 in /lib/x86_64-linux-gnu/libstdc++.so.6)
frame #5: <unknown function> + 0x94ac3 (0x7fafe12ebac3 in /lib/x86_64-linux-gnu/libc.so.6)
frame #6: <unknown function> + 0x126850 (0x7fafe137d850 in /lib/x86_64-linux-gnu/libc.so.6)

 /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cuda.so)
frame #3: <unknown function> + 0xdc253 (0x7f1875ab0253 in /lib/x86_64-linux-gnu/libstdc++.so.6)
frame #4: <unknown function> + 0x94ac3 (0x7f18d2a5aac3 in /lib/x86_64-linux-gnu/libc.so.6)
frame #5: <unknown function> + 0x126850 (0x7f18d2aec850 in /lib/x86_64-linux-gnu/libc.so.6)

[rank3]:[E216 22:03:58.506799789 ProcessGroupNCCL.cpp:1583] [PG 5 Rank 3] Exception (either an error or timeout) detected by watchdog at work: 11761, last enqueued NCCL work: 11808, last completed NCCL work: 11760.
[rank3]:[E216 22:03:58.506804949 ProcessGroupNCCL.cpp:1628] [PG 5 Rank 3] Timeout at NCCL work: 11761, last enqueued NCCL work: 11808, last completed NCCL work: 11760.
[rank3]:[E216 22:03:58.506808416 ProcessGroupNCCL.cpp:582] [Rank 3] Some NCCL operations have failed or timed out. Due to the asynchronous nature of CUDA kernels, subsequent GPU operations might run on corrupted/incomplete data.
[rank3]:[E216 22:03:58.506811211 ProcessGroupNCCL.cpp:588] [Rank 3] To avoid data inconsistency, we are taking the entire process down.
terminate called after throwing an instance of 'c10::DistBackendError'
  what():  [PG 4 Rank 1] Process group watchdog thread terminated with exception: [Rank 1] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=773, OpType=ALLREDUCE, NumelIn=11417088, NumelOut=11417088, Timeout(ms)=600000) ran for 600069 milliseconds before timing out.
Exception raised from checkTimeout at /opt/pytorch/pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:570 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >) + 0x99 (0x7fed479c8919 in /usr/local/lib/python3.10/dist-packages/torch/lib/libc10.so)
frame #1: c10d::ProcessGroupNCCL::WorkNCCL::checkTimeout(std::optional<std::chrono::duration<long, std::ratio<1l, 1000l> > >) + 0x1e1 (0x7fed48a79181 in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cuda.so)
frame #2: c10d::ProcessGroupNCCL::watchdogHandler() + 0x247 (0x7fed48a82657 in /usr/local/lib/pyt[rank3]:[E216 22:03:58.506837311 ProcessGroupNCCL.cpp:1444] [PG 5 Rank 3] Process group watchdog thread terminated with exception: [Rank 3] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=11761, OpType=ALLREDUCE, NumelIn=24883200, NumelOut=24883200, Timeout(ms)=600000) ran for 600099 milliseconds before timing out.
Exception raised from checkTimeout at /opt/pytorch/pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:570 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >) + 0x99 (0x7f2d6a398919 in /usr/local/lib/python3.10/dist-packages/torch/lib/libc10.so)
frame #1: c10d::ProcessGroupNCCL::WorkNCCL::checkTimeout(std::optional<std::chrono::duration<long, std::ratio<1l, 1000l> > >) + 0x1e1 (0x7f2d04079181 in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cuda.so)
frame #2: c10d::ProcessGroupNCCL::watchdogHandler() + 0x247 (0x7f2d04082657 in /usr/local/lib/python3.10/dist-packagehon3.10/dist-packages/torch/lib/libtorch_cuda.so)
frame #3: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0x10f (0x7fed48a8415f in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cuda.so)
frame #4: <unknown function> + 0xdc253 (0x7fedae8b0253 in /lib/x86_64-linux-gnu/libstdc++.so.6)
frame #5: <unknown function> + 0x94ac3 (0x7fee0b7fcac3 in /lib/x86_64-linux-gnu/libc.so.6)
frame #6: <unknown function> + 0x126850 (0x7fee0b88e850 in /lib/x86_64-linux-gnu/libc.so.6)

Exception raised from ncclCommWatchdog at /opt/pytorch/pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1448 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >) + 0x99 (0x7fed479c8919 in /usr/local/lib/python3.10/dist-packages/torch/lib/libc10.so)
frame #1: <unknown function> + 0x107b51e (0x7fed48aab51e in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cuda.so)
frame #2: <unknown function> + 0xcc34df (0x7fed486f34df ins/torch/lib/libtorch_cuda.so)
frame #3: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0x10f (0x7f2d0408415f in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cuda.so)
frame #4: <unknown function> + 0xdc253 (0x7f2d69eb0253 in /lib/x86_64-linux-gnu/libstdc++.so.6)
frame #5: <unknown function> + 0x94ac3 (0x7f2dc6f2cac3 in /lib/x86_64-linux-gnu/libc.so.6)
frame #6: <unknown function> + 0x126850 (0x7f2dc6fbe850 in /lib/x86_64-linux-gnu/libc.so.6)

 /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cuda.so)
frame #3: <unknown function> + 0xdc253 (0x7fedae8b0253 in /lib/x86_64-linux-gnu/libstdc++.so.6)
frame #4: <unknown function> + 0x94ac3 (0x7fee0b7fcac3 in /lib/x86_64-linux-gnu/libc.so.6)
frame #5: <unknown function> + 0x126850 (0x7fee0b88e850 in /lib/x86_64-linux-gnu/libc.so.6)

terminate called after throwing an instance of 'c10::DistBackendError'
  what():  [PG 5 Rank 1] Process group watchdog thread terminated with exception: [Rank 1] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=11761, OpType=ALLREDUCE, NumelIn=24883200, NumelOut=24883200, Timeout(ms)=600000) ran for 600094 milliseconds before timing out.
Exception raised from checkTimeout at /opt/pytorch/pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:570 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >) + 0x99 (0x7faf84798919 in /usr/local/lib/python3.10/dist-packages/torch/lib/libc10.so)
frame #1: c10d::ProcessGroupNCCL::WorkNCCL::checkTimeout(std::optional<std::chrono::duration<long, std::ratio<1l, 1000l> > >) + 0x1e1 (0x7faf1e479181 in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cuda.so)
frame #2: c10d::ProcessGroupNCCL::watchdogHandler() + 0x247 (0x7faf1e482657 in /usr/local/lib/pterminate called after throwing an instance of 'c10::DistBackendError'
  what():  [PG 1 Rank 1] Process group watchdog thread terminated with exception: [Rank 1] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=772, OpType=ALLREDUCE, NumelIn=271488, NumelOut=271488, Timeout(ms)=600000) ran for 600092 milliseconds before timing out.
Exception raised from checkTimeout at /opt/pytorch/pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:570 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >) + 0x99 (0x7f34529c8919 in /usr/local/lib/python3.10/dist-packages/torch/lib/libc10.so)
frame #1: c10d::ProcessGroupNCCL::WorkNCCL::checkTimeout(std::optional<std::chrono::duration<long, std::ratio<1l, 1000l> > >) + 0x1e1 (0x7f3453a79181 in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cuda.so)
frame #2: c10d::ProcessGroupNCCL::watchdogHandler() + 0x247 (0x7f3453a82657 in /usr/local/lib/python3ython3.10/dist-packages/torch/lib/libtorch_cuda.so)
frame #3: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0x10f (0x7faf1e48415f in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cuda.so)
frame #4: <unknown function> + 0xdc253 (0x7faf842b0253 in /lib/x86_64-linux-gnu/libstdc++.so.6)
frame #5: <unknown function> + 0x94ac3 (0x7fafe12ebac3 in /lib/x86_64-linux-gnu/libc.so.6)
frame #6: <unknown function> + 0x126850 (0x7fafe137d850 in /lib/x86_64-linux-gnu/libc.so.6)

Exception raised from ncclCommWatchdog at /opt/pytorch/pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1448 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >) + 0x99 (0x7faf84798919 in /usr/local/lib/python3.10/dist-packages/torch/lib/libc10.so)
frame #1: <unknown function> + 0x107b51e (0x7faf1e4ab51e in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cuda.so)
frame #2: <unknown function> + 0xcc34df (0x7faf1e0f34df .10/dist-packages/torch/lib/libtorch_cuda.so)
frame #3: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0x10f (0x7f3453a8415f in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cuda.so)
frame #4: <unknown function> + 0xdc253 (0x7f34b98b0253 in /lib/x86_64-linux-gnu/libstdc++.so.6)
frame #5: <unknown function> + 0x94ac3 (0x7f35167d5ac3 in /lib/x86_64-linux-gnu/libc.so.6)
frame #6: <unknown function> + 0x126850 (0x7f3516867850 in /lib/x86_64-linux-gnu/libc.so.6)

Exception raised from ncclCommWatchdog at /opt/pytorch/pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1448 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >) + 0x99 (0x7f34529c8919 in /usr/local/lib/python3.10/dist-packages/torch/lib/libc10.so)
frame #1: <unknown function> + 0x107b51e (0x7f3453aab51e in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cuda.so)
frame #2: <unknown function> + 0xcc34df (0x7f34536f34df in /usin /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cuda.so)
frame #3: <unknown function> + 0xdc253 (0x7faf842b0253 in /lib/x86_64-linux-gnu/libstdc++.so.6)
frame #4: <unknown function> + 0x94ac3 (0x7fafe12ebac3 in /lib/x86_64-linux-gnu/libc.so.6)
frame #5: <unknown function> + 0x126850 (0x7fafe137d850 in /lib/x86_64-linux-gnu/libc.so.6)

r/local/lib/python3.10/dist-packages/torch/lib/libtorch_cuda.so)
frame #3: <unknown function> + 0xdc253 (0x7f34b98b0253 in /lib/x86_64-linux-gnu/libstdc++.so.6)
frame #4: <unknown function> + 0x94ac3 (0x7f35167d5ac3 in /lib/x86_64-linux-gnu/libc.so.6)
frame #5: <unknown function> + 0x126850 (0x7f3516867850 in /lib/x86_64-linux-gnu/libc.so.6)

terminate called after throwing an instance of 'c10::DistBackendError'
  what():  [PG 5 Rank 3] Process group watchdog thread terminated with exception: [Rank 3] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=11761, OpType=ALLREDUCE, NumelIn=24883200, NumelOut=24883200, Timeout(ms)=600000) ran for 600099 milliseconds before timing out.
Exception raised from checkTimeout at /opt/pytorch/pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:570 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >) + 0x99 (0x7f2d6a398919 in /usr/local/lib/python3.10/dist-packages/torch/lib/libc10.so)
frame #1: c10d::ProcessGroupNCCL::WorkNCCL::checkTimeout(std::optional<std::chrono::duration<long, std::ratio<1l, 1000l> > >) + 0x1e1 (0x7f2d04079181 in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cuda.so)
frame #2: c10d::ProcessGroupNCCL::watchdogHandler() + 0x247 (0x7f2d04082657 in /usr/local/lib/p[rank13]:[E216 22:03:58.929722338 ProcessGroupNCCL.cpp:568] [Rank 3] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=773, OpType=ALLREDUCE, NumelIn=11417088, NumelOut=11417088, Timeout(ms)=600000) ran for 600087 milliseconds before timing out.
ython3.10/dist-packages/torch/lib/libtorch_cuda.so)
frame #3: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0x10f (0x7f2d0408415f in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cuda.so)
frame #4: <unknown function> + 0xdc253 (0x7f2d69eb0253 in /lib/x86_64-linux-gnu/libstdc++.so.6)
frame #5: <unknown function> + 0x94ac3 (0x7f2dc6f2cac3 in /lib/x86_64-linux-gnu/libc.so.6)
frame #6: <unknown function> + 0x126850 (0x7f2dc6fbe850 in /lib/x86_64-linux-gnu/libc.so.6)

Exception raised from ncclCommWatchdog at /opt/pytorch/pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1448 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >) + 0x99 (0x7f2d6a398919 in /usr/local/lib/python3.10/dist-packages/torch/lib/libc10.so)
frame #1: <unknown function> + 0x107b51e (0x7f2d040ab51e in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cuda.so)
frame #2: <unknown function> + 0xcc34df (0x7f2d03cf34df in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cuda.so)
frame #3: <unknown function> + 0xdc253 (0x7f2d69eb0253 in /lib/x86_64-linux-gnu/libstdc++.so.6)
frame #4: <unknown function> + 0x94ac3 (0x7f2dc6f2cac3 in /lib/x86_64-linux-gnu/libc.so.6)
frame #5: <unknown function> + 0x126850 (0x7f2dc6fbe850 in /lib/x86_64-linux-gnu/libc.so.6)

[rank13]:[E216 22:03:58.930935854 ProcessGroupNCCL.cpp:1583] [PG 2 Rank 3] Exception (either an error or timeout) detected by watchdog at work: 773, last enqueued NCCL work: 775, last completed NCCL work: 772.
[rank13]:[E216 22:03:58.930941776 ProcessGroupNCCL.cpp:1628] [PG 2 Rank 3] Timeout at NCCL work: 773, last enqueued NCCL work: 775, last completed NCCL work: 772.
[rank13]:[E216 22:03:58.930945382 ProcessGroupNCCL.cpp:582] [Rank 3] Some NCCL operations have failed or timed out. Due to the asynchronous nature of CUDA kernels, subsequent GPU operations might run on corrupted/incomplete data.
[rank13]:[E216 22:03:58.930948218 ProcessGroupNCCL.cpp:588] [Rank 3] To avoid data inconsistency, we are taking the entire process down.
[rank13]:[E216 22:03:58.930975521 ProcessGroupNCCL.cpp:1444] [PG 2 Rank 3] Process group watchdog thread terminated with exception: [Rank 3] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=773, OpType=ALLREDUCE, NumelIn=11417088, NumelOut=11417088, Timeout(ms)=600000) ran for 600087 milliseconds before timing out.
Exception raised from checkTimeout at /opt/pytorch/pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:570 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >) + 0x99 (0x7faa72998919 in /usr/local/lib/python3.10/dist-packages/torch/lib/libc10.so)
frame #1: c10d::ProcessGroupNCCL::WorkNCCL::checkTimeout(std::optional<std::chrono::duration<long, std::ratio<1l, 1000l> > >) + 0x1e1 (0x7faa0c679181 in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cuda.so)
frame #2: c10d::ProcessGroupNCCL::watchdogHandler() + 0x247 (0x7faa0c682657 in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cuda.so)
frame #3: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0x10f (0x7faa0c68415f in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cuda.so)
frame #4: <unknown function> + 0xdc253 (0x7faa724b0253 in /lib/x86_64-linux-gnu/libstdc++.so.6)
frame #5: <unknown function> + 0x94ac3 (0x7faacf4bfac3 in /lib/x86_64-linux-gnu/libc.so.6)
frame #6: <unknown function> + 0x126850 (0x7faacf551850 in /lib/x86_64-linux-gnu/libc.so.6)

terminate called after throwing an instance of 'c10::DistBackendError'
  what():  [PG 2 Rank 3] Process group watchdog thread terminated with exception: [Rank 3] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=773, OpType=ALLREDUCE, NumelIn=11417088, NumelOut=11417088, Timeout(ms)=600000) ran for 600087 milliseconds before timing out.
Exception raised from checkTimeout at /opt/pytorch/pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:570 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >) + 0x99 (0x7faa72998919 in /usr/local/lib/python3.10/dist-packages/torch/lib/libc10.so)
frame #1: c10d::ProcessGroupNCCL::WorkNCCL::checkTimeout(std::optional<std::chrono::duration<long, std::ratio<1l, 1000l> > >) + 0x1e1 (0x7faa0c679181 in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cuda.so)
frame #2: c10d::ProcessGroupNCCL::watchdogHandler() + 0x247 (0x7faa0c682657 in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cuda.so)
frame #3: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0x10f (0x7faa0c68415f in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cuda.so)
frame #4: <unknown function> + 0xdc253 (0x7faa724b0253 in /lib/x86_64-linux-gnu/libstdc++.so.6)
frame #5: <unknown function> + 0x94ac3 (0x7faacf4bfac3 in /lib/x86_64-linux-gnu/libc.so.6)
frame #6: <unknown function> + 0x126850 (0x7faacf551850 in /lib/x86_64-linux-gnu/libc.so.6)

Exception raised from ncclCommWatchdog at /opt/pytorch/pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1448 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >) + 0x99 (0x7faa72998919 in /usr/local/lib/python3.10/dist-packages/torch/lib/libc10.so)
frame #1: <unknown function> + 0x107b51e (0x7faa0c6ab51e in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cuda.so)
frame #2: <unknown function> + 0xcc34df (0x7faa0c2f34df in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cuda.so)
frame #3: <unknown function> + 0xdc253 (0x7faa724b0253 in /lib/x86_64-linux-gnu/libstdc++.so.6)
frame #4: <unknown function> + 0x94ac3 (0x7faacf4bfac3 in /lib/x86_64-linux-gnu/libc.so.6)
frame #5: <unknown function> + 0x126850 (0x7faacf551850 in /lib/x86_64-linux-gnu/libc.so.6)

[rank14]:[E216 22:03:58.936750445 ProcessGroupNCCL.cpp:568] [Rank 3] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=773, OpType=ALLREDUCE, NumelIn=11417088, NumelOut=11417088, Timeout(ms)=600000) ran for 600085 milliseconds before timing out.
[rank14]:[E216 22:03:58.937958811 ProcessGroupNCCL.cpp:1583] [PG 3 Rank 3] Exception (either an error or timeout) detected by watchdog at work: 773, last enqueued NCCL work: 775, last completed NCCL work: 772.
[rank14]:[E216 22:03:58.937964442 ProcessGroupNCCL.cpp:1628] [PG 3 Rank 3] Timeout at NCCL work: 773, last enqueued NCCL work: 775, last completed NCCL work: 772.
[rank14]:[E216 22:03:58.937968069 ProcessGroupNCCL.cpp:582] [Rank 3] Some NCCL operations have failed or timed out. Due to the asynchronous nature of CUDA kernels, subsequent GPU operations might run on corrupted/incomplete data.
[rank14]:[E216 22:03:58.937971004 ProcessGroupNCCL.cpp:588] [Rank 3] To avoid data inconsistency, we are taking the entire process down.
[rank14]:[E216 22:03:58.937999399 ProcessGroupNCCL.cpp:1444] [PG 3 Rank 3] Process group watchdog thread terminated with exception: [Rank 3] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=773, OpType=ALLREDUCE, NumelIn=11417088, NumelOut=11417088, Timeout(ms)=600000) ran for 600085 milliseconds before timing out.
Exception raised from checkTimeout at /opt/pytorch/pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:570 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >) + 0x99 (0x7f250e998919 in /usr/local/lib/python3.10/dist-packages/torch/lib/libc10.so)
frame #1: c10d::ProcessGroupNCCL::WorkNCCL::checkTimeout(std::optional<std::chrono::duration<long, std::ratio<1l, 1000l> > >) + 0x1e1 (0x7f24a8679181 in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cuda.so)
frame #2: c10d::ProcessGroupNCCL::watchdogHandler() + 0x247 (0x7f24a8682657 in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cuda.so)
frame #3: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0x10f (0x7f24a868415f in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cuda.so)
frame #4: <unknown function> + 0xdc253 (0x7f250e4b0253 in /lib/x86_64-linux-gnu/libstdc++.so.6)
frame #5: <unknown function> + 0x94ac3 (0x7f256b49bac3 in /lib/x86_64-linux-gnu/libc.so.6)
frame #6: <unknown function> + 0x126850 (0x7f256b52d850 in /lib/x86_64-linux-gnu/libc.so.6)

terminate called after throwing an instance of 'c10::DistBackendError'
  what():  [PG 3 Rank 3] Process group watchdog thread terminated with exception: [Rank 3] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=773, OpType=ALLREDUCE, NumelIn=11417088, NumelOut=11417088, Timeout(ms)=600000) ran for 600085 milliseconds before timing out.
Exception raised from checkTimeout at /opt/pytorch/pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:570 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >) + 0x99 (0x7f250e998919 in /usr/local/lib/python3.10/dist-packages/torch/lib/libc10.so)
frame #1: c10d::ProcessGroupNCCL::WorkNCCL::checkTimeout(std::optional<std::chrono::duration<long, std::ratio<1l, 1000l> > >) + 0x1e1 (0x7f24a8679181 in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cuda.so)
frame #2: c10d::ProcessGroupNCCL::watchdogHandler() + 0x247 (0x7f24a8682657 in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cuda.so)
frame #3: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0x10f (0x7f24a868415f in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cuda.so)
frame #4: <unknown function> + 0xdc253 (0x7f250e4b0253 in /lib/x86_64-linux-gnu/libstdc++.so.6)
frame #5: <unknown function> + 0x94ac3 (0x7f256b49bac3 in /lib/x86_64-linux-gnu/libc.so.6)
frame #6: <unknown function> + 0x126850 (0x7f256b52d850 in /lib/x86_64-linux-gnu/libc.so.6)

Exception raised from ncclCommWatchdog at /opt/pytorch/pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1448 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >) + 0x99 (0x7f250e998919 in /usr/local/lib/python3.10/dist-packages/torch/lib/libc10.so)
frame #1: <unknown function> + 0x107b51e (0x7f24a86ab51e in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cuda.so)
frame #2: <unknown function> + 0xcc34df (0x7f24a82f34df in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cuda.so)
frame #3: <unknown function> + 0xdc253 (0x7f250e4b0253 in /lib/x86_64-linux-gnu/libstdc++.so.6)
frame #4: <unknown function> + 0x94ac3 (0x7f256b49bac3 in /lib/x86_64-linux-gnu/libc.so.6)
frame #5: <unknown function> + 0x126850 (0x7f256b52d850 in /lib/x86_64-linux-gnu/libc.so.6)

bash: line 3: 469511 Aborted                 python train_mp_mod.py --config=mp --tensor_parallel=4 --scale_depth=12 --scale_heads=8 --scale_dim=384 --n_train=25 --local_batch_size=4 --amp_mode=none --patch_size=4 --parallel_order=tp-dp
srun: error: nid001529: task 12: Exited with exit code 134
srun: Terminating StepId=35918163.0
slurmstepd: error: *** STEP 35918163.0 ON nid001501 CANCELLED AT 2025-02-16T22:03:59 ***
srun: error: nid001501: tasks 0-3: Terminated
srun: error: nid001529: tasks 13-15: Terminated
srun: error: nid001505: tasks 8-11: Terminated
srun: error: nid001504: tasks 4-7: Terminated
srun: Force Terminated StepId=35918163.0