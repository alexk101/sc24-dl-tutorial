diff --git a/train_mp_mod.py b/train_mp_mod.py
index 97cf975..d5f6406 100644
--- a/train_mp_mod.py
+++ b/train_mp_mod.py
@@ -37,33 +37,15 @@ from utils.gpu_utils import (
     get_gpu_info, initialize_gpu, get_profiler
 )
 
-# Import appropriate GPU monitoring tools
-if NVIDIA_AVAILABLE:
-    import pynvml
-    pynvml.nvmlInit()
-elif ROCM_AVAILABLE:
-    from rocm_smi import rocm_smi
-    rocm_smi.initialize()
 
 # Check for bfloat16 support
 BFLOAT16_AVAILABLE = False
-if NVIDIA_AVAILABLE:
+if NVIDIA_AVAILABLE or ROCM_AVAILABLE:
     BFLOAT16_AVAILABLE = torch.cuda.is_bf16_supported()
     from torch.cuda.amp import autocast, GradScaler
-    logging.info(f"NVIDIA bfloat16 support: {BFLOAT16_AVAILABLE}")
-elif ROCM_AVAILABLE:
-    # Check ROCm version for bfloat16 support (available in ROCm 5.0+)
-    try:
-        BFLOAT16_AVAILABLE = torch.bfloat16 in torch.hip.get_device_properties(0).supported_dtypes
-    except:
-        BFLOAT16_AVAILABLE = False  # Fallback if can't determine
-    from torch.hip.amp import autocast, GradScaler
-    logging.info(f"AMD bfloat16 support: {BFLOAT16_AVAILABLE}")
+    logging.info(f"bfloat16 support: {BFLOAT16_AVAILABLE}")
 else:
-    from torch.cpu.amp import autocast, GradScaler
-    # CPU bfloat16 support depends on hardware (e.g., Intel CPUs with AVX512-BF16)
-    BFLOAT16_AVAILABLE = hasattr(torch.cpu, 'is_bf16_supported') and torch.cpu.is_bf16_supported()
-    logging.info(f"CPU bfloat16 support: {BFLOAT16_AVAILABLE}")
+    raise RuntimeError("No GPU support available. This script requires either NVIDIA CUDA or AMD ROCm GPUs.")
 
 from torch.utils.flop_counter import FlopCounterMode
 
@@ -310,6 +292,31 @@ def train(params, args, local_rank, world_rank, world_size, hyperparameter_searc
     if world_rank == 0:
         logging.info(f"FLOPs per training step: {flops_per_step:,}")
 
+    # Add after model creation (around line 171)
+    if params.distributed:
+        # Print tensor sizes for each rank
+        for name, param in model.named_parameters():
+            if param.requires_grad:
+                numel = param.numel()
+                gathered_numels = [torch.tensor([numel], device=device) for _ in range(world_size)]
+                torch.distributed.all_gather(gathered_numels, torch.tensor([numel], device=device))
+                if world_rank == 0:
+                    logging.info(f"Parameter {name} sizes across ranks: {[x.item() for x in gathered_numels]}")
+        
+        # Synchronize before proceeding
+        torch.distributed.barrier()
+
+    # Add after model initialization
+    if world_rank == 0:
+        for name, group in comm.get_groups().items():
+            logging.info(f"Process group {name}: size={comm.get_size(name)}")
+            logging.info(f"Process group {name} ranks: {comm.get_ranks(name)}")
+
+        # Check specific attention block
+        block0 = model.module.blocks[0].attn
+        logging.info(f"Block 0 attention: num_heads={block0.num_heads}, num_heads_local={block0.num_heads_local}")
+        logging.info(f"Block 0 attention tp group size: {comm.get_size(block0.comm_tp_name)}")
+
     # Training loop
     for epoch in range(startEpoch, startEpoch + params.num_epochs):
         torch.cuda.synchronize()  # device sync to ensure accurate epoch timings
@@ -443,6 +450,12 @@ def train(params, args, local_rank, world_rank, world_size, hyperparameter_searc
 
             # Optional: Log time and FLOP statistics
             if world_rank == 0 and iters % params.logging_freq == 0:
+                # Get validation metrics at same frequency as FLOPS
+                val_loss, val_rmse, valid_steps = validate_model(
+                    model, val_data_loader, device, params,
+                    loss_func, world_rank, comm if params.distributed else None
+                )
+                
                 total_flops += flops_per_step
                 elapsed_time = time.time() - start_time
                 remaining_time = get_remaining_time()
@@ -453,9 +466,11 @@ def train(params, args, local_rank, world_rank, world_size, hyperparameter_searc
                 logging.info(f"Current iteration: {iters}/{params.num_iters} ({(iters/params.num_iters)*100:.1f}%)")
                 logging.info(f"Total FLOPs: {total_flops:,}")
                 logging.info(f"FLOPS/second: {flops_per_second:,.2f}")
+                logging.info(f"Validation RMSE: {val_rmse.cpu().numpy()[0]:.4f}")
                 
                 args.tboard_writer.add_scalar('Performance/total_flops', total_flops, iters)
                 args.tboard_writer.add_scalar('Performance/flops_per_second', flops_per_second, iters)
+                args.tboard_writer.add_scalar('RMSE(u10m)/valid', val_rmse.cpu().numpy()[0], iters)
 
         torch.cuda.synchronize()  # device sync to ensure accurate epoch timings
         end = time.time()
@@ -505,9 +520,6 @@ def train(params, args, local_rank, world_rank, world_size, hyperparameter_searc
     torch.cuda.synchronize()
     t2 = time.time()
     tottime = t2 - t1
-    if NVIDIA_AVAILABLE:
-        pynvml.nvmlShutdown()
-
     if hyperparameter_search:
         training_time = time.time() - training_start_time
         return best_val_rmse, peak_memory, training_time
@@ -675,6 +687,12 @@ if __name__ == "__main__":
     parser.add_argument("--time_buffer", type=int, default=60, help="buffer time in seconds before SLURM time limit")
     parser.add_argument("--time_limit", type=str, default="00:30:00", help="SLURM time limit (Not used here, but logged for later analysis)")
 
+    # Add this with the other arguments
+    parser.add_argument("--learning_rate", type=float, default=None, help="Override the default learning rate")
+
+    # Add with other scaling arguments
+    parser.add_argument("--patch_size", type=int, default=None, help="Override the default patch size")
+
     args = parser.parse_args()
     params = YParams(os.path.abspath(args.yaml_config), args.config)
     
@@ -684,6 +702,10 @@ if __name__ == "__main__":
     params.depth = args.scale_depth
     params.num_heads = args.scale_heads
     params.n_train = args.n_train
+    if args.learning_rate is not None:
+        params.lr = args.learning_rate
+    if args.patch_size is not None:
+        params.patch_size = args.patch_size
     ########
 
     # Update config with modified args
@@ -787,6 +809,8 @@ if __name__ == "__main__":
             'n_nodes': args.n_nodes,
             'time_limit': args.time_limit,
             'local_batch_size': args.local_batch_size,
+            'learning_rate': args.learning_rate,
+            'patch_size': args.patch_size,
         }
         with open(expDir/'hparams.json', "w") as f:
             json.dump(hparams, f)
